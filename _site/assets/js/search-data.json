{"0": {
    "doc": "Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms",
    "title": "Scenario",
    "content": "ACME is an AWS customer who uses the Adobe Experience Platform (AEP) to manage their customer profiles across all systems. The Chief Marketing Officer (CMO) of ACME wants to select a subset of the ACME customers contained in their AEP account and enrich this subset of users with additional data from a third-party. The CMO tasks the data engineering team to make this possible. The ACME data engineering team evaluates this request and defines the following steps to do: . Access and retrieval from AEP; data ingestion and security in AWS: . | Create an Amazon Simple Storage Service (Amazon S3) repository that can receive the export object from AEP . | Ensure the data is protected/secure and encrypted to adhere to the ACME security practices . | Get access to the AEP account to select the specific fields needed (this provides schema stability downstream), and define and execute the query . | Configure a periodic task that will execute this query and export the result set to the ACME AWS account . | . Transformation of data to target data structure/repository: . | On receipt of this data into Amazon S3, they will need to initiate a process that will transform the data from the AEP format into the format required by the AWS Clean Rooms service identified for enabling the sharing and enrichment of the data set  | . Automation: . | The data engineering team anticipates that they will do many similar tasks so they will automate this process for future re-use | . This Guidance aims to assist ACME to import their customer profile information in an ongoing manner (daily) from Adobe Experience Platform into their AWS account and thereafter, process it, normalize it, and prepare it for consumption within an AWS Clean Rooms environment using AWS native services. ",
    "url": "/connecting-data-from-adobe-experience-platform.html#scenario",
    
    "relUrl": "/connecting-data-from-adobe-experience-platform.html#scenario"
  },"1": {
    "doc": "Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms",
    "title": "Implementation Details",
    "content": "In this Guidance, customer profile data from the Adobe Experience Platform (AEP) is ingested into your Amazon Simple Storage Service (Amazon S3) bucket. The Guidance will demonstrate how to read this data, normalize it, and process it to make it compatible within AWS Clean Rooms. Reference Architecture . Figure 1: Reference Architecture for importing AEP data into AWS . ",
    "url": "/connecting-data-from-adobe-experience-platform.html#implementation-details",
    
    "relUrl": "/connecting-data-from-adobe-experience-platform.html#implementation-details"
  },"2": {
    "doc": "Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms",
    "title": "Data Ingestion",
    "content": "In order to ingest data from AEP, you will need to setup a daily export schedule from within AEP to Amazon S3. The dataset used in this Guidance is Adobe Experience Platform Customer Profile. Adobe Experience Platform (AEP) and Amazon S3 integration . In this section, you will establish a live outbound connection to your Amazon S3 storage to periodically (daily) export customer profile data from AEP for a given selected segment. 1. Create an Amazon S3 bucket for receiving data from AEP . You will create an Amazon S3 bucket that will serve as the landing zone for the incoming data files from AEP. | Sign in into AWS console and navigate to Amazon S3 bucket page (https://console.aws.amazon.com/s3/buckets) . | Choose Create bucket . | Provide a globally unique bucket name . | Choose the appropriate region . | Block public access . | Enable Bucket Versioning . | Enable SSE-S3 based bucket encryption . | Provide appropriate tag(s) . | Create bucket . | . Once the bucket is created, create three folders within the bucket. | Navigate within the bucket and click Create folder . | Provide the folder name as landing and select SSE-S3 as the server-side encryption option . | . Repeat the above two steps to create two folders named raw and archive. Figure 2: Displays UI when creating the landing folder . Figure 3: UI after all 3 folders are created . 2. AWS Access Keys and permissions . You will have to provide AEP with Access Keys (access key ID and secret key ID) to connect to your Amazon S3 storage. It is recommended for you to create a separate AWS Identity and Access Management (IAM) user within your AWS account that has limited access to allow AEP to connect to Amazon S3 [see the next section for permissions required]. While AWS does not recommend sharing of access keys with any third party, AEP currently supports connectivity via access keys and therefore a separate dedicated service user will limit the exposure of your account. Steps to create an IAM user and generate access keys . | Sign in into AWS console as a user with appropriate permissions to create IAM user and access keys . | Navigate to the AWS IAM console . | In the navigation pane, select Users → Add user . | Type the name of the user . | Select Programmatic access to generate the access keys for this user that can be entered into AEP console . | Within Permissions section, select Attach existing policies and click on Create Policy . | Within the new tab, create the policy as shown below  . | Navigate to the create user screen and refresh the policy list and select the policy created . | Proceed to create the user . | Important: Download and save the CSV file with the access key details . | . Reference Link: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console . Setup IAM Permissions for the user in AWS for AEP to connect  . For AEP to establish a connection and write data into Amazon S3, it needs the following permissions: . | s3:GetObject . | s3:GetBucketLocation . | s3:DeleteObject . | s3:ListBucket . | s3:PutObject . | s3:ListMultipartUploadParts . | . Below is the policy JSON to associate with the IAM user (please note to enter the correct S3 bucket ARN) . {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Sid\": \"VisualEditor0\",             \"Effect\": \"Allow\",             \"Action\": [                 \"s3:PutObject\",                 \"s3:GetObject\",                 \"s3:ListBucket\",                 \"s3:DeleteObject\",                 \"s3:GetBucketLocation\",                 \"s3:ListMultipartUploadParts\"             ],             \"Resource\": [                 \"arn:aws:s3:::&lt;bucket-name&gt;/*\",                 \"arn:aws:s3:::&lt;bucket-name&gt;\"             ]         }     ] } . 3. Setup AWS Key Management Service (AWS KMS) based key for encryption-decryption . You will create an AWS KMS key to generate a public-private key pair. This allows AEP to use the public key to encrypt the files while exporting them to Amazon S3. Later during the data processing, the private key will be used to decrypt the file for further processing. | Sign into the AWS console and navigate to the AWS KMS page. | Select Create a key . | Select Asymmetric as key type; Encrypt and Decrypt as key usage; RSA_2048 as key spec . | Provide the key alias name . | Select the appropriate key administrators . | Select the appropriate IAM roles and users (including the IAM user created above) to allow key usage. Note: also make sure to select the IAM role that will be responsible for decryption of the files later during the data processing stage . | Create the key . | . Once the key has been created, navigate to the key and select the Public Key tab and download the key. You will need this key for later while setting up the Amazon S3 connection within AEP. Figure 4: Highlights Public Key tab . 4. Connect to Amazon S3 as a destination within AEP . You will need Manage Destinations access control permission as part of your AEP user permissions. | Navigate to Connections → Destinations → Catalog and search for Amazon S3 . | Select Set up if you do not have an active S3 connection already created. Alternatively, if you see Activate as an option, you can click on it and choose Configure new destination in the next screen that opens up. | . Figure 5: Highlights Amazon S3 with the Activate option . | Enter the access keys of the IAM user previously created . | Enter the public key downloaded from AWS KMS in the previous section . | Fill in additional details: . | name . | description . | bucket [the name of the bucket previously created] . | folder path: landing/ . | . | . | Check the options for all the alerts . | Save the destination configuration . | . Figure 6: Highlights the options for different alerts . Reference Link: https://experienceleague.adobe.com/docs/experience-platform/destinations/catalog/cloud-storage/amazon-s3.html?lang=en . 5. Create a batch export job within AEP to upload data files to Amazon S3 . You will setup an activation of audience data (batch profile export) to Amazon S3 in this section.  . | Navigate to Connections → Destinations → Catalog and search for Amazon S3 . | Select Activate Segments . | Choose the correct Amazon S3 connection and proceed . | Select the desired segment of choice and proceed to scheduling. For this Guidance, select one segment from the list. | Create a schedule with the following options: . | export full files . | schedule frequency: daily . | select the time as 12pm GMT . | save the schedule . | . | . Figure 7: Displays UI for creating a schedule . | Select the profile attributes | . Select add new fields and choose the below listed schema fields from the profile class: . | Label | Adobe XDM Field Name | . | customerID | personID | . | first_name | person.name.firstName | . | last_name | person.name.lastName | . | address_1 | homeAddress.street1 | . | address_2 | homeAddress.street2 | . | city | homeAddress.city | . | state | homeAddress.stateProvince | . | zip | homeAddress.postalCode | . | country | homeAddress.country | . | email | personalEmail.address | . | phone | mobilePhone.number | . | loyalty_member_status | loyalty.status | . | segmentation_status | segmentMembership.status | . | Review and finish the activation flow | . Reference Link: https://experienceleague.adobe.com/docs/experience-platform/destinations/ui/activate/activate-batch-profile-destinations.html?lang=en#select-attributes . Once the export is setup, AEP will publish one or more CSV files in Amazon S3 for the selected segment. The number of file(s) uploaded in Amazon S3 will depend on the amount of data that qualifies for that segment; for every five million profile records, one file is generated for the segment. ",
    "url": "/connecting-data-from-adobe-experience-platform.html#data-ingestion",
    
    "relUrl": "/connecting-data-from-adobe-experience-platform.html#data-ingestion"
  },"3": {
    "doc": "Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms",
    "title": "Data Processing",
    "content": "Once the data arrives from AEP in your Amazon S3 bucket, a daily prompt will decrypt the file(s), normalize it, perform the Personal Identifiable Information (PII) handling of the data and prepare it to be made available for AWS Clean Rooms. 1. AWS Lambda function to decrypt the data file(s) . An AWS Lambda function is used to decrypt the file(s) using the AWS KMS APIs and write the decrypted file(s) to a separate prefix/folder within the same Amazon S3 bucket. AWS Lambda IAM role will need appropriate permissions to read/write from Amazon S3 and AWS KMS . {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Sid\": \"VisualEditor0\",             \"Effect\": \"Allow\",             \"Action\": [                 \"s3:GetObject\",                 \"s3:ListBucket\"             ],             \"Resource\": [                 \"arn:aws:s3:::[bucket name]\",                 \"arn:aws:s3:::[bucket name]/*\"             ]         }     ] } . {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Sid\": \"VisualEditor0\",             \"Effect\": \"Allow\",             \"Action\": \"kms:Decrypt\",             \"Resource\": \"arn:aws:kms:us-east-1:[accountid]:key/[KMS key id]\"         }     ] } . Figure 8: AWS Lambda function is used to decrypt files . Sample code: . def decrypt_data_key(data_key_encrypted):     \"\"\"Decrypt an encrypted data key     :param data_key_encrypted: Encrypted ciphertext data key.     :return Plaintext base64-encoded binary data key as binary string     :return None if error     \"\"\"     # Decrypt the data key     kms_client = boto3.client('kms')     try:         response = kms_client.decrypt(CiphertextBlob=data_key_encrypted)     except ClientError as e:         logging.error(e)         return None     # Return plaintext base64-encoded binary data key     return base64.b64encode((response['Plaintext']))       def decrypt_file(filename):     \"\"\"Decrypt a file encrypted by encrypt_file()     The encrypted file is read from &lt;filename&gt;.encrypted     The decrypted file is written to &lt;filename&gt;.decrypted     :param filename: File to decrypt     :return: True if file was decrypted. Otherwise, False.     \"\"\"     # Read the encrypted file into memory     try:         with open(filename + '.encrypted', 'rb') as file:             file_contents = file.read()     except IOError as e:         logging.error(e)         return False     # The first NUM_BYTES_FOR_LEN bytes contain the integer length of the     # encrypted data key.     # Add NUM_BYTES_FOR_LEN to get index of end of encrypted data key/start     # of encrypted data.     data_key_encrypted_len = int.from_bytes(file_contents[:NUM_BYTES_FOR_LEN],                                             byteorder='big' \\)                              + NUM_BYTES_FOR_LEN     data_key_encrypted = file_contents[NUM_BYTES_FOR_LEN:data_key_encrypted_len]     # Decrypt the data key before using it     data_key_plaintext = decrypt_data_key(data_key_encrypted)     if data_key_plaintext is None:         return False     # Decrypt the rest of the file     f = Fernet(data_key_plaintext)     file_contents_decrypted = f.decrypt(file_contents[data_key_encrypted_len:])     # Write the decrypted file contents     try:         with open(filename + '.decrypted', 'wb') as file_decrypted:             file_decrypted.write(file_contents_decrypted)     except IOError as e:         logging.error(e)         return False     # The same security issue described at the end of encrypt_file() exists     # here, too, i.e., the wish to wipe the data_key_plaintext value from     # memory.     return True . 2. Data Exploration using AWS Glue DataBrew . In this Guidance, you will use AWS Glue DataBrew to explore the incoming data and create the recipe to normalize and process the data. While other AWS services can be used to perform these tasks, AWS Glue DataBrew is chosen to demonstrate the ease of manipulating the data visually using the AWS Glue DataBrew service. | Sign in into the AWS console as a user with the appropriate permissions to create IAM user and access keys . | Navigate to AWS Glue DataBrew . | Select Create project . | Provide the name of the project . | Select Create new recipe and provide a name . | Choose New dataset and select Amazon S3 as your source . | Provide the path as follows: s3://[bucket name]/raw/&lt;.*&gt;.csv . | . This will ensure that all file(s) in the landing/ folder will be picked up for processing . Figure 9: AWS Glue DataBrew UI with New dataset highlighted . | Select CSV as the file type and treat first row as header options . | Provide the appropriate tags; this will help for cost calculation/monitoring . | Choose the appropriate IAM role (or create one if it does not exist) . | Your IAM user needs permission to access the AWS Glue DataBrew from AWS Console . | IAM permissions for AWS Glue DataBrew to access the data resources on your behalf . | . | Create project . | . Once the project has been created, a visual editor displaying a sample of 500 records will open up. At this stage, you can add steps for data cleansing, normalization, and PII handling. Figure 10: Visual editor displaying a sample of records . The following steps will be performed as part of data processing: . 1. Normalization - the incoming data will be parsed and the column names will be appropriately renamed. While this is an optional step, mapping the schema makes this dataset further accessible within AWS Clean Rooms for additional workflows such as identity resolution and de-duplication. Certain columns are cleansed (for example, phone number formatting is performed), data quality checks are performed to drop records that do not match the criteria, for example, drop records with empty email values. 2. PII Data Handling - the PII information in the incoming data is hashed using a secret key from AWS Secrets Manager. Reference Link: https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.CRYPTOGRAPHIC_HASH.html . Once all the steps have been completed, publish the recipe. This will create a version of the recipe. Figure 11: Data processing step with Publish and Create job highlighted . Select Create Job to run the above recipe on the entire dataset and generate an output. | Provide the job name . | Under the Job output settings create Output 1 as follows: . | Amazon S3 | Parquet file format | Snappy compression [if you do not have an Amazon S3 output location, you can create one by following the steps outlined in the section Create an Amazon S3 bucket] . | Provide the Amazon S3 output location . | Click on the settings for Output 1 . | Replace output files for each job run (this option is chosen since this is a full refresh) | . | . | . Figures 12 and 13: Job output settings . | Provide the appropriate tags . | Chose the IAM role that was previously created/chosen while creating the AWS Glue DataBrew project . | this role needs permissions to access the source Amazon S3 data and write to Amazon S3 at the output location . | Create and run job | . | . Navigate to the JOBS section in the left menu and wait until the job you just created runs successfully. *Figure 14: JOBS section in AWS Glue DataBrew UI . Once the job is successfully completed, navigate to the output Amazon S3 location to verify. As you observe in the below screenshot, two output files have been generated, one for each input source file. Figure 15: Two output files are generated . 3. AWS Lambda function for cleanup . An AWS Lambda function is used to perform cleanup activities, namely moving the file(s) from landing/ and raw/ prefix to an archive/ prefix folder within the same Amazon S3 bucket. Note: AWS Lambda IAM role will need appropriate permissions to read/write from Amazon S3 . Figure 16: Diagram of Lambda function . ",
    "url": "/connecting-data-from-adobe-experience-platform.html#data-processing",
    
    "relUrl": "/connecting-data-from-adobe-experience-platform.html#data-processing"
  },"4": {
    "doc": "Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms",
    "title": "Data Catalog",
    "content": "In order to use the AEP data in AWS Clean Rooms environment, the data needs to be registered with AWS Glue Data Catalog as a table. 1. Create AWS Glue Data Catalog database . You will create a database within AWS Glue Data Catalog . | Sign in into AWS console and navigate to the AWS Glue Data Catalog page . | Select Databases under the Data catalog section in the left menu . | Select Add database and provide a name . | . 2. Create AWS Glue Crawler . You can use a crawler to populate the AWS Glue Data Catalog with tables by crawling the Amazon S3 bucket output location. Upon completion, the crawler creates the table in your data catalog. This table can be later referenced within AWS Clean Rooms environment. | Sign in into AWS console and navigate to AWS Glue Data Catalog page . | Select Crawlers under the Data catalog section in the left menu . | Select Add crawler option . | Provide the name for the crawler and appropriate tags . | Under security configuration, choose the security configuration as recommended by your admin. If there is no existing security configuration, you can create one by selecting the Security configurations from the left panel menu. The security configuration allows you to specify encryption options for the Amazon S3 bucket that the AWS Glue service will write to along with logging encryption options to Amazon CloudWatch. | Chose Data stores and Crawl all folders as the crawler source type options; since this Guidance is for a full refresh, crawl all folders will yield the desired result as older data will be overwritten with new data. | Add a data store . | Enter S3 and provide output bucket location: | . | . *Figure 17: Adding a data store . | Choose an IAM Role or let AWS Glue create a role for you: | . Figure 18: Choose an IM role or let Aws Glue create an IAM role for you . | Choose Frequency as Run on demand. We will be automating the entire workflow using AWS Step Functions in the later section. | . | Choose the database previously created as the crawler’s output | . | Create a crawler and run it | . Once the crawler runs successfully, verify the AWS Glue Data Catalog table is created. Navigate to the Tables section from the left panel menu. Figure 19: AWS Glue Data Catalog . 3. Verification . You can further verify that the table has been successfully created in AWS Glue Data Catalog by running a query in Amazon Athena. | Navigate to Amazon Athena Query Editor in your AWS Console . | Select AwsDataCatalog as the Data source . | Select the AWS Glue Data Catalog database . | Under the Query text area, you can run a SELECT query to verify the data displays . | . Figure 20: Amazon Athena query . ",
    "url": "/connecting-data-from-adobe-experience-platform.html#data-catalog",
    
    "relUrl": "/connecting-data-from-adobe-experience-platform.html#data-catalog"
  },"5": {
    "doc": "Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms",
    "title": "Automation using AWS Step Functions and Amazon EventBridge",
    "content": "To make the entire solution automated and deployable as a unit, you will use AWS Step Functions to integrate the data processing workflow. On a daily scheduled run, the AWS Step Function will orchestrate various stages to decrypt and process the data and then initiate an AWS Glue Crawler job. Once the AWS Glue crawler job is completed, it will send a notification to the Amazon Simple Notification Service (Amazon SNS) topic letting you know that the data is available for consumption in AWS Clean Rooms. ",
    "url": "/connecting-data-from-adobe-experience-platform.html#automation-using-aws-step-functions-and-amazon-eventbridge",
    
    "relUrl": "/connecting-data-from-adobe-experience-platform.html#automation-using-aws-step-functions-and-amazon-eventbridge"
  },"6": {
    "doc": "Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms",
    "title": "1.  Create AWS Step Function",
    "content": ". | Navigate to Step Functions in your AWS Console . | Select Create state machine . | Design your workflow visually and create a standard type state machine . | In the visual editor, draw your state machine definition as shown below with the correct configuration. | . Figure 21: Visual editor configuration . | Provide appropriate tag names . | Let the step function create a new IAM role or choose an existing appropriate IAM role. Permissions needed for Step Function to execute as desired. | . Ability to publish to Amazon SNS topics - error topic and success topic: . {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Sid\": \"VisualEditor0\",             \"Effect\": \"Allow\",             \"Action\": \"sns:Publish\",             \"Resource\": [                 \"arn:aws:sns:us-east-1:[accountid]:aep-error-handling\",                 \"arn:aws:sns:us-east-1:[accountid]:aep-notification-topic\"             ]         }     ] } . Ability to invoke Lambda functions: . {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Sid\": \"VisualEditor0\",             \"Effect\": \"Allow\",             \"Action\": \"lambda:InvokeFunction\",             \"Resource\": [                 \"arn:aws:lambda:us-east-1:[accountid]:function:AEP-DecryptFiles:*\",                 \"arn:aws:lambda:us-east-1:[accountid]:function:aep-cleanup-activities:*\"             ]         },         {             \"Sid\": \"VisualEditor1\",             \"Effect\": \"Allow\",             \"Action\": \"lambda:InvokeFunction\",             \"Resource\": [                 \"arn:aws:lambda:us-east-1:[accountid]:function:AEP-DecryptFiles\",                 \"arn:aws:lambda:us-east-1:[accountid]:function:aep-cleanup-activities\"             ]         }     ] } . Ability to start a Glue DataBrew job: . {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Action\": [                 \"databrew:startJobRun\",                 \"databrew:listJobRuns\",                 \"databrew:stopJobRun\"             ],             \"Resource\": [                 \"arn:aws:databrew:us-east-1:[accountid]:job/aep-customer-profile-job\"             ]         }     ] } . Ability to start an AWS Glue Crawler and its associated APIs for status check: . {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Sid\": \"VisualEditor0\",             \"Effect\": \"Allow\",             \"Action\": [                 \"glue:GetCrawler\",                 \"glue:StartCrawler\"             ]             \"Resource\": \"arn:aws:glue:us-east-1:[accountid]:crawler/aep-customer-profile\"         }     ] } . | Save the step function definition | . Error Handling within the AWS Step Function . Each step within the Step Function will have a “catch-ALL” mechanism to trap all errors and report them. For this Guidance, you are not setting up any retry logic. Example of a catch-ALL for a given step: . \"Glue DataBrew StartJobRun\": {       \"Type\": \"Task\",       \"Resource\": \"arn:aws:states:::databrew:startJobRun.sync\",       \"Parameters\": {         \"Name\": \"aep-customer-profile-job\"       },       \"Catch\": [         {           \"ErrorEquals\": [             \"States.ALL\"           ],           \"Next\": \"Error Handling Notfication\"         }       ],       \"Next\": \"StartCrawler\" } . In the above snippet, any error state will result in a notification being sent to the error handling Amazon SNS topic that will further inform all the subscribers. Details about the State Machine . 1. AWS Lambda - Decrypt CSV files . Provide the ARN of the lambda function created previously that is responsible for decrypting the incoming files and placing them in the raw/ prefix location . Error Handling: For this Guidance, you will catch ALL errors and send a notification to an Amazon SNS topic for notification. Additionally, these errors will also be captured and logged within AWS CloudWatch Log Groups . Figures 22-24: Decrypting CSV files . 2. AWS Glue DataBrew StartJobRun . a. Provide the name of the DataBrew Job; you can find this from the AWS Console → DataBrew → Jobs . b. Select the Wait for task to complete checkbox; this will ensure that the step machine waits to move to the next step until the DataBrew job is completed successfully . Error Handling: For this Guidance, you will catch ALL errors and send a notification to an Amazon SNS topic for notification. Additionally, these errors will be captured and logged within AWS CloudWatch Log Groups. Figures 25-27: AWS Glue DataBrew StartJobRun . 3. WS Glue StartCrawler . | Provide the name of the glue crawler previously created; you can find this from AWS Console → Glue → Crawlers | . Error Handling: For this Guidance, you will catch ALL errors and send a notification to an Amazon SNS topic for notification. Additionally, these errors will also be captured and logged within AWS CloudWatch Log Groups . we will NOT check the wait for callback option; this is to avoid writing custom callback function. Instead, we will rely on the native APIs of AWS Glue Crawler to poll for status within the Step Function . Figures 28-29: AWS Glue StartCrawler . 4. AWS Glue GetCrawler . It fetches the meta information about the particular AWS Glue crawler including its current state. If the current state is RUNNING OR STOPPING, it will wait for a configured amount of time and fetch the information once again. This will continue until the AWS Glue crawler status is READY. Error Handling: For this Guidance, you will catch ALL errors and send a notification to an Amazon SNS topic for notification. Additionally, these errors will also be captured and logged within AWS CloudWatch Log Groups . Example of GetCrawler output json . {   \"Crawler\": {     \"Classifiers\": [],     \"CrawlElapsedTime\": 12421,     \"CrawlerSecurityConfiguration\": \"s3\",     \"CreationTime\": \"2022-08-19T21:47:41Z\",     \"DatabaseName\": \"amt-db\",     \"LakeFormationConfiguration\": {       \"AccountId\": \"\",       \"UseLakeFormationCredentials\": false     },     \"LastCrawl\": {       \"LogGroup\": \"/aws-glue/crawlers-role/service-role/AWSGlueServiceRole-aep-customer-profile-role-s3\",       \"LogStream\": \"aep-customer-profile\",       \"MessagePrefix\": \"2ab03631-e730-42cb-9d4b-b4a5a7f8b385\",       \"StartTime\": \"2022-08-23T02:59:58Z\",       \"Status\": \"SUCCEEDED\"     },     \"LastUpdated\": \"2022-08-19T21:47:41Z\",     \"LineageConfiguration\": {       \"CrawlerLineageSettings\": \"DISABLE\"     },     \"Name\": \"aep-customer-profile\",     \"RecrawlPolicy\": {       \"RecrawlBehavior\": \"CRAWL_EVERYTHING\"     },     \"Role\": \"service-role/AWSGlueServiceRole-aep-customer-profile-role\",     \"SchemaChangePolicy\": {       \"DeleteBehavior\": \"DEPRECATE_IN_DATABASE\",       \"UpdateBehavior\": \"UPDATE_IN_DATABASE\"     },     \"State\": \"RUNNING\",     \"TablePrefix\": \"aep\",     \"Targets\": {       \"CatalogTargets\": [],       \"DeltaTargets\": [],       \"DynamoDBTargets\": [],       \"JdbcTargets\": [],       \"MongoDBTargets\": [],       \"S3Targets\": [         {           \"Exclusions\": [],           \"Path\": \"s3://adobe-output-bucket-2022\"         }       ]     },     \"Version\": 1   } } . 5. Choice State . Figure 30: Choice state . 6. Wait state . If the GetCrawler status is RUNNING or STOPPING, the step functions waits for a configured time of 30 seconds, and checks the status once again. If the status of the crawler is in any other state, it moves to the next state function, otherwise it continues to wait and checks for the status again. 7. AWS Lambda - Cleanup activities . Provide the ARN of the Lambda function created previously that is responsible for performing cleanup activities. Error Handling: For this Guidance, you will catch ALL errors and send a notification to an Amazon SNS topic for notification. Additionally, these errors will also be captured and logged within AWS CloudWatch Log Groups . 8. Publish to Amazon SNS upon successful execution . Select the appropriate topic in the configuration panel of the step function . ",
    "url": "/connecting-data-from-adobe-experience-platform.html#1--create-aws-step-function",
    
    "relUrl": "/connecting-data-from-adobe-experience-platform.html#1--create-aws-step-function"
  },"7": {
    "doc": "Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms",
    "title": "2.  Schedule the AWS Step Function using Amazon EventBridge rule",
    "content": "You will create a rule in the Amazon EventBridge to invoke the AWS Step Function to start on a daily schedule. | Sign in into AWS console and navigate to the Amazon EventBridge page . | Select Create rule . | Provide the rule name and select schedule as the rule type . | . Figure 31: Amazon EventBridge schedule highlighted . | select schedule pattern as “A fine-grained pattern” . | Provide the cron expression as 0 0 * * ? * . | . This will run the job at midnight everyday (UTC time zone) . Figure 32: Defining AWS Step Function . | Select AWS Service and Step Functions state machine as the target and select the step function: | . Figure 33: Selecting AWS Service and Step Functions state machine . | Provide the appropriate tags . | Create the rule . | . The rule will invoke the AWS Step Function daily to perform the data preparation task on the incoming data from AEP. ",
    "url": "/connecting-data-from-adobe-experience-platform.html#2--schedule-the-aws-step-function-using-amazon-eventbridge-rule",
    
    "relUrl": "/connecting-data-from-adobe-experience-platform.html#2--schedule-the-aws-step-function-using-amazon-eventbridge-rule"
  },"8": {
    "doc": "Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms",
    "title": "Closing Out",
    "content": "In this Guidance, the focus was for ACME data engineers to be able to setup an ongoing data export from the Adobe Experience Platform and prepare it to be made available for business team members to use it in AWS Clean Rooms.  . The Guidance is based on a consistent input schema that does not change, with a full refresh of data (periodic overwrite) every time a new export is pushed. Future versions of this Guidance will address handling schema changes and incremental arrival of data. ",
    "url": "/connecting-data-from-adobe-experience-platform.html#closing-out",
    
    "relUrl": "/connecting-data-from-adobe-experience-platform.html#closing-out"
  },"9": {
    "doc": "Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms",
    "title": "Notices",
    "content": "Adobe, the Adobe logo, Acrobat, the Adobe PDF logo, Adobe Premiere, Creative Cloud, InDesign, and Photoshop are either registered trademarks or trademarks of Adobe in the United States. ",
    "url": "/connecting-data-from-adobe-experience-platform.html#notices",
    
    "relUrl": "/connecting-data-from-adobe-experience-platform.html#notices"
  },"10": {
    "doc": "Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms",
    "title": "Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms",
    "content": ". ",
    "url": "/connecting-data-from-adobe-experience-platform.html",
    
    "relUrl": "/connecting-data-from-adobe-experience-platform.html"
  },"11": {
    "doc": "Guidance for Connecting Audiences to Amazon Marketing Cloud Uploader",
    "title": "Implementation",
    "content": "AWS Connectors . AWS connectors are available to support data ingestion from external third-party sources into AWS. These connectors bring data into an Amazon Simple Storage Service (Amazon S3) bucket so AMC Uploader from AWS can access the data. Once users make the data available and have formatted it in accordance with the AMC file format requirements, AMC Uploader from AWS imports the data into AMC. Data sources may include: . | Salesforce Marketing Cloud Connector Solution . | Adobe Experience Platform Connector Solution . | Google Cloud Connector . | . Importing data from AWS Clean Rooms to AMC . AWS Clean Rooms helps customers and their partners match, analyze, and collaborate on their collective datasets—without sharing or revealing underlying data. AWS Clean Rooms supports exporting data that can be used to create datasets within AMC using a LIST analysis rule on a configured table within a collaboration. When communicating with your collaboration partner, ensure that the data they are sharing is within the same region as your AMC Instance. AWS Clean Rooms does not support cross-region data collaborations. Setting up the collaboration . | Navigate to the AWS Clean Rooms service within the AWS Management Console . | Select Create collaboration . | Input a name for the collaboration and an optional description . | Set up the Members . a. Input a member display name for yourself . b. Input a member display name and AWS Account ID for each member you will be collaborating with . | Select yourself as the member who will be performing queries and receiving results . | Optional: Enable query logging or cryptographic computing . | Select Create and join collaboration . | Each member you invited will now need to accept the invitation to collaborate within their AWS Clean Rooms console. | . Configuring a table . It’s assumed that you already have your data within an S3 Bucket within the same region as your AWS Clean Room and that you’ve already configured a Glue Table that is maintaining the schema of your data. This is a requirement before completing the next steps. | Navigate to the Configured Tables section on the sidebar within the AWS Clean Rooms Console . | Select configure new table . | Select the relevant AWS Glue Database and Table . | Select which columns will be allowed within the collaboration. You can select all columns or individually select specific columns to be shared. | Give the table a name . | Select configure new table . | . You must now configure an Analysis rule. An Analysis rule impacts how the tables can be queried and which data will be exposed . | Locate the alert to configure an analysis rule (it may be at the top of the page). Select Configure analysis rule . | Select List for the Type and use the Guided flow as the creation method . | Select Next . | Select a join control column that you have determined with your collaborators to use as a match key. You can select one or multiple columns. | Select which columns you wish to expose to your collaborators as list controls . | Select Next . | Validate the information is correct on the review page. If everything looks good, select Configure analysis rule . | . Associate your table to the collaboration . Now that you’ve created a collaboration and configured your table, you need to associate that table to the collaboration. | Navigate to the collaboration that you’ve set up . | Navigate to the Tables tab and select Associate table . | Select the table that you just created . | Input a table name and optional description. Ensure that the table name is unique to the collaboration and that you do not overlap table names with your collaborators . | For Service access, you can either have the service provision a new IAM Role for you or you can create a new one separately using the policy document given on the page . | Select Associate table . | . By associating your table to your collaboration, you can monitor the Tables tab to see when other collaborating members have associated their tables. The other collaborators will need to follow the same steps here to ensure that the proper data is being exposed and the same analysis rule is applied to the associated tables. Querying the collaboration and storing the results . Once your collaborators have successfully set up their side of the collaboration, you can start performing queries: . | Navigate to the collaboration and select the Queries tab . | You will be prompted to set up a query output location . a. Select the same S3 Bucket you used to store your first party data when deploying the AMC Uploader Solution. You should also set up a dedicated folder in that S3 Bucket for the AWS Clean Room query output. b. Ensure that you select CSV as the output format . | You can now input your SQL query into the box and select Run. The results will be outputed into the console as well as within the S3 Bucket and path you specified. | . Uploading your AWS Clean Rooms output to Amazon Marketing Cloud . Note: It is assumed that you’ve already deployed AMC Uploader Solution. This is a requirement before completing the next steps . | Navigate to the AMC Uploader Solution web interface . | Select Step 1 - Select file on the left side bar . | Select the file that was outputted from AWS Clean Rooms and select Next . | Input a name for the Dataset name and optional description . | Select CSV for the File format and Dimension for the Dataset type. | Select Next . | Map the schema to the columns that were discovered within the file. a. Ensure that if any column contains personally identfiable information (PII) data, that you specify PII as the Column Type and select the relevant PII type. This will ensure that this data is hashed prior to being uploaded to AMC. b. Ensure that any column where you wish to perform an aggregate function is defined as a Metric column type . | Select Next . | Verify the data is correct and select Submit . | . Once submitted, a new dataset will be created within AMC and an AWS Glue Job has been created to normalize and hash the data. You can monitor the Glue Job by selecting it under Phase 2: Datasets transformed. Once the Glue Job has been completed, the data will be outputed into another S3 Bucket that the AMC Uploader Solution created. AMC will then access that bucket and extract the data to be inserted into the newly created Dataset within AMC. You can monitor that process as well in the Phase 3: Datasets uploaded section by selecting on the relevant Dataset in Phase 1 . Importing data from Salesforce Marketing Cloud (SFMC) to AMC using the SFMC Connector Solution . When you deploy the SFMC Connector Solution, it will create multiple S3 buckets to handle the raw and transformed data. To upload the data into AMC, you will be setting up an S3 Bucket Replication Rule to automatically send the raw data that is imported to an S3 Bucket where all of your first party data will live. | Before deploying the solution, ensure that you select a region that matches the same region your AMC Instance is deployed in. For North America, this is us-east-1 and for EMEA/APAC, this is eu-west-1. You can find the region your instance is deployed in by visiting the instance info page within the AMC Console. | Deploy the Solution using the provided CloudFormation template. Follow the implementation guide’s deployment steps for more information. Ensure that Subscriber is the data you are importing from SFMC. | Ensure that the AppFlow that was created as part of the SFMC solution has been executed and is importing data. If not, trigger the job to run or set up the flow on a schedule to import data on a cadence you specify. | Once the solution is deployed, go to the Outputs tab of the CloudFormation template and save the value of BucketInboundData. This is where your raw data will be saved to and where we will need to replicate data from. | Select the link specified in the BucketInboundData value. This will take you directly to the created S3 Bucket. | Select on the bucket name in the breadcrumb at the top of the page, it should start with sfmc-connector-inboundbucket. | Select the Management tab and scroll down to Replication rules . | Select Create replication rule . | Enter a name such as AMCDataReplication . | Data coming into the InboundBucket will be saved with the prefix “sfmc-connector-flow/”. Enter “sfmc-connector-flow/” (without quotes) into the Prefix box. | For the Destination, select a bucket that you will use to store your first party data that will be accessed by the AMC Uploader. If you do not have a bucket, create a new one. Make sure the bucket is in the same region as your AMC Instance. Ensure that versioning is enabled in the destination bucket as it is required for replication. If versioning is not enabled on the destination bucket, you can enable it directly from the Replication set up page within the Console. | Next, select an existing IAM Role to allow the replication, or select Create new role . | The SFMC connector uses an AWS KMS key to encrypt the raw data. Check the box for Replicate objects encrypted with AWS KMS and specify the KMS key that the files are encrypted with. If you are unsure which KMS key is used, browse to one of the files in the S3 bucket that was imported from SFMC and locate the Encryption key ARN . | Leave all other options as default . | Select Save. If versioning is not enabled in the source bucket, the console may prompt you to enable versioning. You can do so by selecting the enable version control button. | Once saved, you will be on the Batch Operations Job page . | Uncheck Generate completion report . | Leave all other details default . | Select Save . | The job will run immediately and the data will be replicated to your AMC first party data S3 Bucket . | . Uploading your AWS Clean Rooms output to Amazon Marketing Cloud . Note: It is assumed that you’ve already deployed AMC Uploader Solution.This is a requirement before completing the next steps . | Navigate to the AMC Uploader Solution web interface . | Select Step 1 - Select file on the left side bar . | Select the file that was outputted from the SFMC Connector and select Next . | Input a name for the Dataset name and optional description . | Select CSV for the File format and Dimension for the Dataset type. | Select Next . | Map the schema to the columns that were discovered within the file. a. Ensure that if any column contains PII data, that you specify PII as the Column Type and select the relevant PII type. This will ensure that this data is hashed prior to being uploaded to AMC. b. Ensure that any column where you wish to perform an aggregate function is defined as a Metric column type . | Select Next . | Verify the data is correct and select Submit . | . Once submitted, a new dataset will be created within AMC and an AWS Glue Job has been created to normalize and hash the data. You can monitor the Glue Job by selecting it under Phase 2: Datasets transformed. Once the Glue Job is complete, the data will be outputed into another S3 Bucket that the AMC Uploader Solution created. AMC will then access that bucket and extract the data to be inserted into the newly created Dataset within AMC. You can monitor that process as well in the Phase 3: Datasets uploaded section by selecting on the relevant Dataset in Phase 1. ",
    "url": "/connecting-audiences-to-amazon-marketing-cloud-uploader.html#implementation",
    
    "relUrl": "/connecting-audiences-to-amazon-marketing-cloud-uploader.html#implementation"
  },"12": {
    "doc": "Guidance for Connecting Audiences to Amazon Marketing Cloud Uploader",
    "title": "Formatting data for AMC Uploader from AWS",
    "content": "When the data is imported into an Amazon S3 bucket, it must be in one of the accepted formats in the AMC data upload file format requirements. Additionally, the data must be in a single file. AMC Uploader from AWS will take care of AMC formatting requirements, including hashing specific columns, flagging data as specific PII, and segmenting data into separate files by time windows. AMC supports two types of datasets: (1) Dimension and (2) Fact. Users must ensure the data meets the relative requirements. See the AMC Fact vs. Dimension Datasets section below for more information about the differences between these two dataset types. ",
    "url": "/connecting-audiences-to-amazon-marketing-cloud-uploader.html#formatting-data-for-amc-uploader-from-aws",
    
    "relUrl": "/connecting-audiences-to-amazon-marketing-cloud-uploader.html#formatting-data-for-amc-uploader-from-aws"
  },"13": {
    "doc": "Guidance for Connecting Audiences to Amazon Marketing Cloud Uploader",
    "title": "AMC file format requirements",
    "content": "AMC Uploader from AWS requirements . The AMC Uploader from AWS requires data to be in a single file in either CSV or JSON format. As of 2023, the AMC Uploader from AWS does not support multiple files in partition format. CSV file requirements . CSV files must be UTF-8 encoded and comma delimited. In Microsoft Excel, save the file in a “CSV UTF- 8 (comma-delimited)” format. When CSV files are uploaded, AMC will automatically convert data to the corresponding column type. For example, if “12423.56” is contained in the CSV file and is mapped to a DECIMAL type column, AMC will coerce the string value contained in the CSV file to the appropriate column type. JSON file requirements . JSON files must contain one object per row of data. JSON arrays should not be used. Table 1 is an example of the accepted JSON format: . {\"name\": \"Product A\", \"sku\": 11352987, \"quantity\": 2, \"pur_time\": \"2021-06-23T19:53:58Z\"} {\"name\": \"Product B\", \"sku\": 18467234, \"quantity\": 2, \"pur_time\": \"2021-06-24T19:53:58Z\"} {\"name\": \"Product C\", \"sku\": 27264393, \"quantity\": 2, \"pur_time\": \"2021-06-25T19:53:58Z\"} {\"name\": \"Product A\", \"sku\": 48572094, \"quantity\": 2, \"pur_time\": \"2021-06-25T19:53:58Z\"} {\"name\": \"Product B\", \"sku\": 18278476, \"quantity\": 1, \"pur_time\": \"2021-06-26T13:33:58Z\"} . Table 1- Example of the accepted JSON format . ",
    "url": "/connecting-audiences-to-amazon-marketing-cloud-uploader.html#amc-file-format-requirements",
    
    "relUrl": "/connecting-audiences-to-amazon-marketing-cloud-uploader.html#amc-file-format-requirements"
  },"14": {
    "doc": "Guidance for Connecting Audiences to Amazon Marketing Cloud Uploader",
    "title": "AMC Data Types, Timestamp, and Date Formats",
    "content": "Dataset columns can be defined with the data types listed in Table 2. Carefully review the accepted formats for TIMESTAMP and DATE columns. If values in CSV / JSON data do not meet the accepted format, the upload may fail. Ensure all values in CSV / JSON data confirm the specified data type and format before uploading. Where possible, string values will be coerced to the corresponding numerical type and vice-versa, but no guarantees are made on the casting process. | Data Type | Format | Example | . | STRING | UTF-8 encoded character data | My string data | . | DECIMAL | Numerical with two floating point level precision | 123.45 | . | INTEGER (int 32-bit) | 32-bit numerical, no floating points | 12345 | . | LONG (int 64-bit) | 32-bit numerical, no floating points | 1233454565875646 | . | TIMESTAMP | yyyy-MM-ddThh:mm:ssZ | 2021-08-02T08:00:00Z | . | DATE | yyyy-MM-dd | 8/2/2021 | . Table 2 - Data Types and Formats . ",
    "url": "/connecting-audiences-to-amazon-marketing-cloud-uploader.html#amc-data-types-timestamp-and-date-formats",
    
    "relUrl": "/connecting-audiences-to-amazon-marketing-cloud-uploader.html#amc-data-types-timestamp-and-date-formats"
  },"15": {
    "doc": "Guidance for Connecting Audiences to Amazon Marketing Cloud Uploader",
    "title": "AMC fact vs dimension datasets",
    "content": "Before data can be uploaded, a table (also referred to as a dataset) must be created to store that data. As mentioned above, AMC supports two types of datasets: fact and dimension. The implications for each dataset type is detailed in Table 3: . | Dataset Type | Usage | Requires Timestamp Column | Requires Partition Type (Period) | . | Fact | Time series data | Yes | Yes | . | Dimension | Static data | No | No | . Table 3 - Differences between Fact and Dimension Datasets . Fact datasets . Fact datasets should be used for time-series data, meaning data where each row is associated with a corresponding date or timestamp. When defining a fact dataset, it is mandatory to designate one column as the main event time. Because fact datasets are used to store time-series data, the data files must be partitioned by a unit of time before uploading to AMC. The partition type (also referred to as a period) should be specified on the dataset, and the options are per-minute, per-hour, per-day, and per-week. The partition type informs how often the data can be queried. For example, per-week partitioned data cannot be queried at the daily level, and per-day partitioned data cannot be queried at the hourly level. When data is uploaded to a fact dataset, each upload is performed according to a specific period of time. This is how AMC determines which data to include when performing queries. The AMC Uploader from AWS separates the partitions. Dimension datasets . Dimension datasets can be used to upload static data or any information that is not time-bound. Examples include customer relationship management (CRM) audience lists, campaign metadata, mapping tables, and product metadata, such as a table mapping ASINs to external product names or sensitive cost-of-goods-sold data. When data is uploaded to a dimension dataset, AMC will always use the most recent file uploaded, which is known as the full-replace method of updating data. Dimension datasets do not require a main event time column or uploaded files to be partitioned. ",
    "url": "/connecting-audiences-to-amazon-marketing-cloud-uploader.html#amc-fact-vs-dimension-datasets",
    
    "relUrl": "/connecting-audiences-to-amazon-marketing-cloud-uploader.html#amc-fact-vs-dimension-datasets"
  },"16": {
    "doc": "Guidance for Connecting Audiences to Amazon Marketing Cloud Uploader",
    "title": "Guidance for Connecting Audiences to Amazon Marketing Cloud Uploader",
    "content": ". This guide will help Amazon Marketing Cloud (AMC) users retrieve data from AWS Clean Rooms as well as third-party data sources, such as Salesforce Marketing Cloud, Adobe Experience Platform, and Google Cloud Platform. The guide also explains how to import that data into AMC using Amazon Marketing Cloud (AMC) Uploader from AWS. ",
    "url": "/connecting-audiences-to-amazon-marketing-cloud-uploader.html",
    
    "relUrl": "/connecting-audiences-to-amazon-marketing-cloud-uploader.html"
  },"17": {
    "doc": "Guidance for Building Queries in AWS Clean Rooms",
    "title": "Scenario",
    "content": "Customers (advertisers, publishers, data providers) use AWS Data Clean Rooms to collaborate on private data without exposing secure information between parties. A typical use case for AWS Data Clean Rooms is when two parties are collecting different pieces of data about overlapping end-users or customers, and the two parties want to generate insights from the overlap of these two datasets, without exposing the sensitive or Personally Identifiable Information (PII) attributes within their first party data. With AWS Data Clean Rooms, each data provider can control what data the consumer is able to view, query, and aggregate. This Guidance demonstrates how a customer could prepare data for an AWS Data Clean Rooms collaboration, and set up an analytics and insights stack on AWS to investigate the results of queries exported from the collaboration on AWS. The reference architecture demonstrates how AWS Glue can be used to crawl data stored on Amazon Simple Storage Service (Amazon S3) to create data that can be accessed through the AWS Data Clean Rooms interface. Once data is joined and analyzed within the AWS Data Clean Rooms, query results can then be stored on Amazon S3 and accessed from Amazon Athena for further analysis and visualization. ",
    "url": "/building-queries-clean-rooms.html#scenario",
    
    "relUrl": "/building-queries-clean-rooms.html#scenario"
  },"18": {
    "doc": "Guidance for Building Queries in AWS Clean Rooms",
    "title": "Data Prep",
    "content": "To utilize data within AWS Data Clean Rooms, it must first be added to an AWS Glue Data Catalog. This can be done manually or with the AWS Glue Crawler. Preparing data and adding it to an AWS Glue Data Catalog must be done by each data provider contributing data to an AWS Data Clean Rooms collaboration. Once data has been added to an AWS Glue Data Catalog, it can then be referenced within an AWS Data Clean Rooms collaboration and made available to the collaborators with custom analysis controls for each table. ",
    "url": "/building-queries-clean-rooms.html#data-prep",
    
    "relUrl": "/building-queries-clean-rooms.html#data-prep"
  },"19": {
    "doc": "Guidance for Building Queries in AWS Clean Rooms",
    "title": "Querying Data within the AWS Data Clean Rooms",
    "content": "Once at least two datasets have been made available to the AWS Data Clean Rooms collaboration, SQL Queries can be performed to join the datasets, run aggregation analysis, and more. The results of each query are automatically stored in a configurable Amazon S3 location. At the end of this Guidance, we will share an example scenario with sample data and queries that can be run within AWS Data Clean Rooms. ",
    "url": "/building-queries-clean-rooms.html#querying-data-within-the-aws-data-clean-rooms",
    
    "relUrl": "/building-queries-clean-rooms.html#querying-data-within-the-aws-data-clean-rooms"
  },"20": {
    "doc": "Guidance for Building Queries in AWS Clean Rooms",
    "title": "Use of Amazon Athena",
    "content": "Amazon Athena allows you to query data stored in Amazon S3 with standard SQL syntax. With Athena, you can query the data where it sits, save your queries, and create views, which can then be used by Amazon QuickSight for visualization purposes. In the reference architecture diagram, we can see how a user can utilize Amazon Athena to further query and analyze the results of an AWS Data Clean Rooms collaboration output. In much the same way that we prepared the data for use in the cleanroom, AWS Glue and Amazon Athena can work together to provide users easy access to a friendly SQL based user interface. Figure 1 - Diagram for building queries in AWS Clean Rooms . ",
    "url": "/building-queries-clean-rooms.html#use-of-amazon-athena",
    
    "relUrl": "/building-queries-clean-rooms.html#use-of-amazon-athena"
  },"21": {
    "doc": "Guidance for Building Queries in AWS Clean Rooms",
    "title": "Use of Amazon QuickSight",
    "content": "Amazon QuickSight is a business intelligence tool that can help you easily create complex visualizations of the query results from an AWS Data Clean Rooms collaboration. You can use QuickSight to build dashboards that can be shared among your team, embedded on web pages, and set to automatically update whenever there is new data available. The first time you navigate to Amazon QuickSight, you will need to select a plan and set up your organization name. You will also need to make some choices in terms of what Amazon QuickSight has access to. Select Athena and the Amazon S3 bucket where your AWS Data Clean Rooms query results are stored. Once you have QuickSight enabled, you can connect Amazon Athena as a data source. Once connected to your data source, you can use QuickSight to build an analysis of your dataset. ",
    "url": "/building-queries-clean-rooms.html#use-of-amazon-quicksight",
    
    "relUrl": "/building-queries-clean-rooms.html#use-of-amazon-quicksight"
  },"22": {
    "doc": "Guidance for Building Queries in AWS Clean Rooms",
    "title": "Example Scenario and Queries",
    "content": "In this example, we will consider two entities who want to create a collaboration between their potentially overlapping datasets. Entity A is an eCommerce company. This company has first party data pertaining to its customers in the form of email addresses, names, and physical addresses. This data has been collected over time through a variety of transactions, including website conversions. Entity B is an advertiser. This company places ads on behalf of their clients and captures impression data, including email addresses, campaign IDs, and more. Entity A has contracted with Entity B to run ad campaigns for their upcoming sale. Entity A would like to know details about the success of their campaigns with Entity B, but neither entity wants to compromise the privacy of their users. So, they use AWS Data Clean Rooms. Entity A has data about their customers that looks something like the following: . fields = [ \"email_address\", \"date_created\", \"state\", \"city\", \"country\", \"zip\", \"first_name\", \"last_name\", \"status\", \"birth_country\" ] . Entity A also has data about conversions on their website that looks like the following: . fields = [ \"email_address\", \"date\", \"creative_id\", \"event_type\", \"version\", \"price\", \"currency\", \"transaction_id\" ] . Entity B has impression data from users who viewed ads placed by Entity A. This data looks like the following: . fields = [ \"email_address\", \"date\", \"creative_id\" ] . Once both entities have created a collaboration in AWS Data Clean Rooms and shared their tables, Entity A, the Data Consumer, can run queries like the following: . 1. Find the overlap between Entity A’s conversions data and Entity B’s impressions data: . SELECT COUNT(DISTINCT c.email_address) FROM conversions c INNER JOIN impressions i ON i.email_address = c.email_address . 2. Overlap Analysis, segmented by Creative ID: . SELECT COUNT(DISTINCT c.email_address) as counts, c.creative_id FROM conversions c INNER JOIN impressions i ON i.email_address = c.email_address GROUP BY c.creative_id ORDER BY counts DESC . 3. Overlap Analysis, segmented by Price: . SELECT COUNT(DISTINCT c.email_address) as counts, c.price FROM conversions c INNER JOIN impressions i ON c.email_address = i.email_address GROUP BY c.price ORDER BY c.price DESC . 4. Audience Analysis: . SELECT COUNT(DISTINCT c.email_address) as counts, crm.c_birth_country FROM impressions i INNER JOIN conversions c ON c.email_address = i.email_address INNER JOIN customers crm ON c.email_address = crm.c_email_address GROUP BY crm.c_birth_country . ",
    "url": "/building-queries-clean-rooms.html#example-scenario-and-queries",
    
    "relUrl": "/building-queries-clean-rooms.html#example-scenario-and-queries"
  },"23": {
    "doc": "Guidance for Building Queries in AWS Clean Rooms",
    "title": "Guidance for Building Queries in AWS Clean Rooms",
    "content": ". This Guidance aims to help users getting started with AWS Clean Rooms collaboration. The reference architecture first illustrates how users can set up an AWS Data Clean Rooms data collaboration and prepare data. Then it illustrates how to use Amazon Athena and Amazon QuickSight for further analysis and visualization of the results produced as output of that cleanroom collaboration. ",
    "url": "/building-queries-clean-rooms.html",
    
    "relUrl": "/building-queries-clean-rooms.html"
  },"24": {
    "doc": "CHANGELOG",
    "title": "CHANGELOG",
    "content": "All notable user-facing changes to this project are documented in this file. The project underwent a major maintenance shift in March 2022. ",
    "url": "/CHANGELOG/",
    
    "relUrl": "/CHANGELOG/"
  },"25": {
    "doc": "CHANGELOG",
    "title": "HEAD",
    "content": "This website is built from the HEAD of the main branch of the theme repository. This website includes docs for some new features that are not available in v0.5.0! . Code changes to main that are not in the latest release: . | Fixed: liquid variable leakage in navigation components by @pdmosses in #1243 | . Docs changes in main that are not in the latest release: . | N/A | . ",
    "url": "/CHANGELOG/#head",
    
    "relUrl": "/CHANGELOG/#head"
  },"26": {
    "doc": "CHANGELOG",
    "title": "Release v0.5.1",
    "content": "Hi all, this is a very small minor patch release that has two small behavioral bugfixes: fixing a regression introduced in v0.5.0 on Safari versions &lt;16.4 (broken media query), and the copy code button providing incorrect feedback in insecure browser contexts. This should be a smooth upgrade with no breaking changes. As always, we’d love your feedback. Open an issue or start a discussion for bug reports, feature requests, and any other feedback. Thanks for continuing to use Just the Docs! . Using Release v0.5.1 . Users who have not pinned the theme version will be automatically upgraded to v0.5.1 the next time they build their site. To use this release explicitly as a remote theme: . remote_theme: just-the-docs/just-the-docs@v0.5.1 . To use this version explicitly as a gem-based theme, pin the version in your Gemfile and re-run bundle install or bundle update just-the-docs: . gem \"just-the-docs\", \"0.5.1\" . To use and pin a previous version of the theme, replace the 0.5.1 with the desired release tag. Bugfixes . | Fixed: disable copy code button in insecure contexts @rmoff in #1226 | Fixed: context-based media feature not supported by Safari &lt;16.4 by @mattxwang in #1240 | . Documentation . | Added: document copy code button requiring secure context by @rmoff in #1225 | Fixed: typo (“them” → “theme”) in MIGRATION.md by @waldyrious in #1219 | Fixed: font-weight typo (Utilities &gt; Typography) by @mattxwang in #1229 | Fixed: just the docs typo in migration guide by @mattxwang in #1230 | . New Contributors . | @rmoff made their first contribution in #1225 | . ",
    "url": "/CHANGELOG/#release-v051",
    
    "relUrl": "/CHANGELOG/#release-v051"
  },"27": {
    "doc": "CHANGELOG",
    "title": "Release v0.5.0",
    "content": "Hope your April is going well! This new release of Just the Docs is relatively minor. It has one breaking change: we’ve reverted the import order of setup.scss to be before color schemes. In addition, we include two requested fixes: color contrast issues with ::selection and using Just the Docs with mermaid versions &gt;=10. We’ve marked this as a minor version bump due to the breaking change. In the next section, we briefly outline what migration steps should be. Users who did not migrate to v0.4.2 or who do not have a custom setup.scss are guaranteed no breaking changes. As always, we’d love your feedback. Open an issue or start a discussion for bug reports, feature requests, and any other feedback. Thanks for continuing to use Just the Docs! . Migrating to v0.5.0 . Migration: users with a custom setup.scss cannot rely on variables or functions defined in color_scheme. This reverts to the behaviour in v0.4.1. Users should instead move those variables or functions to the color_scheme files themselves. For more, refer to the migration guide. Using Release v0.5.0 . Users who have not pinned the theme version will be automatically upgraded to v0.5.0 the next time they build their site. To use this release explicitly as a remote theme: . remote_theme: just-the-docs/just-the-docs@v0.5.0 . To use this version explicitly as a gem-based theme, pin the version in your Gemfile and re-run bundle install or bundle update just-the-docs: . gem \"just-the-docs\", \"0.5.0\" . To use and pin a previous version of the theme, replace the 0.5.0 with the desired release tag. Bugfixes . | Reverted (breaking): “Fix import order for setup.scss (#1184)” by @mattxwang in #1209 | Fixed: color contrast issues with ::selection (reverting to browser defaults) @mattxwang in #1208 | Fixed: mermaid v10, bundle all mermaid code in component by @mattxwang in #1190 | Removed: unused images (just-the-docs.png, search.svg) by @mattxwang in #1107 | Removed: CODE_OF_CONDUCT, docker-compose, and Dockerfile files from site by @mattxwang in #1187 | . Full Changelog: https://github.com/just-the-docs/just-the-docs/compare/v0.4.2…v0.5.0 . ",
    "url": "/CHANGELOG/#release-v050",
    
    "relUrl": "/CHANGELOG/#release-v050"
  },"28": {
    "doc": "CHANGELOG",
    "title": "Release v0.4.2",
    "content": "Hello! We’re back again with another small release. Like v0.4.1, this release is a semver patch: it only includes bugfixes, and is fully backwards-compatible. The big highlight of this theme is fixing our light scheme code highlighting contrast issues; this was one of our most-requested features! This change is fully backwards-compatible; users can opt-in to our old highlighting theme by using legacy_light instead of light. As always, we’d love your feedback. Open an issue or start a discussion for bug reports, feature requests, and any other feedback. Thanks for continuing to use Just the Docs! . Using Release v0.4.2 . Users who have not pinned the theme version will be automatically upgraded to v0.4.2 the next time they build their site. To use this release explicitly as a remote theme: . remote_theme: just-the-docs/just-the-docs@v0.4.2 . To use this RC explicitly as a gem-based theme, pin the version in your Gemfile and re-run bundle install or bundle update just-the-docs: . gem \"just-the-docs\", \"0.4.2\" . To use and pin a previous version of the theme, replace the 0.4.2 with the desired release tag. Bugfixes . | Fixed: light scheme code highlighting contrast issues; updated to use Atom’s One Light colors, consolidate theme variables by @mattxwang in #1166 | Fixed: duplicate import of color_schemes by @mattxwang in #1173 | Fixed: import order for setup.scss by @mattxwang in #1184 | Removed: unused dark syntax themes by @mattxwang in #1192 | . Documentation . | Added: docs for using mermaid with AsciiDoc by @flyx in #1182 | . Full Changelog: https://github.com/just-the-docs/just-the-docs/compare/v0.4.1…v0.4.2 . ",
    "url": "/CHANGELOG/#release-v042",
    
    "relUrl": "/CHANGELOG/#release-v042"
  },"29": {
    "doc": "CHANGELOG",
    "title": "Release v0.4.1",
    "content": "Hello! We hope you’ve been enjoying the new v0.4.0; we appreciate all the feedback we’ve gotten already! As promised, future releases will be small with simple steps to upgrade. This is one of them! v0.4.1 is a semver patch: it only includes bugfixes, and is fully backwards-compatible. As always, we’d love your feedback. Open an issue or start a discussion for bug reports, feature requests, and any other feedback. Thanks for continuing to use Just the Docs! . Using Release v0.4.1 . Users who have not pinned the theme version will be automatically upgraded to v0.4.1 the next time they build their site. To use this release explicitly as a remote theme: . remote_theme: just-the-docs/just-the-docs@v0.4.1 . To use this RC explicitly as a gem-based theme, pin the version in your Gemfile and re-run bundle install or bundle update just-the-docs: . gem \"just-the-docs\", \"0.4.1\" . To use and pin a previous version of the theme, replace the 0.4.1 with the desired release tag. Bugfixes . | Fixed: allow later versions of bundler by @mattxwang in #1165 | Fixed: AsciiDoc code block styling by @flyx in #1168 | Fixed: main content negative margin for viewports in [$md, $nav-width + $content-width] by @Dima-369 in #1177 | Removed: unused OneDarkJekyll files by @mattxwang in #1167 | . Documentation . | Fixed: re-add jekyll-github-metadata to docs site by @mattxwang in #1108 | . New Contributors . | @flyx made their first contribution in #1168 | @Dima-369 made their first contribution in #1177 | . Full Changelog: https://github.com/just-the-docs/just-the-docs/compare/v0.4.0…v0.4.1 . ",
    "url": "/CHANGELOG/#release-v041",
    
    "relUrl": "/CHANGELOG/#release-v041"
  },"30": {
    "doc": "CHANGELOG",
    "title": "Release v0.4.0",
    "content": "We’re so excited to release Just the Docs v0.4.0. This release has been almost a year in the making - after our new maintenance team has taken over the project, we’ve added two years of backlogged features and bugfixes to modernize the theme. This CHANGELOG will summarize some of the key changes, discuss migrations strategies, and outline broad future plans for this theme. Brief Overview - Highlighted Changes . v0.4.0 contains many new features and bugfixes. We enumerate all of them in further sections in this changelog; however, we’d like to call out some of the most-requested changes: . | better support for dark theme: dark highlighting, search input color | callouts, a new design component to highlight content | configuring mermaid.js, a markdown-native diagram visualization library | copy code button for code snippets | external navigation links | major improvements to nav generation efficiency and robustness | minor improvements to built-in accessibility (SVG icons, nav titles, skip to main content) | modularized site components (advanced feature) | new custom includes: table of contents heading, navigation panel footer, search placeholder, lunr search indices | bugfixes involving WEBrick and Ruby 3, Liquid processing in CSS comments, nested task lists, relative URLs, scroll navigation, corrupted search data from rake, breadcrumbs, and more! | more documentation for custom includes, this changelog, and the migration guide | . After usage instructions and the roadmap, we enumerate all changes from v0.3.3. Using Release v0.4.0 . Unlike pre-releases, v0.4.0 is a new semver minor release for the theme. That means that users who have not pinned the theme version will be automatically upgraded to v0.4.0 the next time they build their site. To use this release explicitly as a remote theme: . remote_theme: just-the-docs/just-the-docs@v0.4.0 . To use this RC explicitly as a gem-based theme, pin the version in your Gemfile and re-run bundle install or bundle update just-the-docs: . gem \"just-the-docs\", \"0.4.0\" . If you would prefer to not upgrade, you can enforce that explicitly: . | pin your gem version in your Gemfile, like so gem \"just-the-docs\", \"0.3.3\" . | freeze the remote_theme, like so remote_theme: just-the-docs/just-the-docs@v0.3.3 . | . Migration Guide and Strategies . We’ve developed a new migration guide for users to migrate from version v0.3.3 to v0.4.0. It outlines major changes in project maintenance (e.g. new repository link, team) as well as breaking changes that may break your site (and potential solutions). We suggest that all users refer to the guide before manually upgrading their site. For the vast majority of users, we do not anticipate that this will be a breaking change. The major touch points are surrounding new includes, navigation (ordering, pages, and collections), the favicon, and a shift to relative URLs. However, users who heavily customize the theme (primarily by overriding includes) will likely have to make minor changes. Given the length of features added in this release, users may want to incrementally upgrade through the pre-releases. To follow this approach, read this changelog from v0.4.0.rc1 to v0.4.0.rc5; this breaks down the release into small chunks, each of which should be easier to upgrade. v0.4.0.rc5 is identical to this release. For support with migrating to v0.4.0, open an issue or start a discussion and let us know! . Roadmap (What’s Next?) . Moving forward, we plan to release more frequently with smaller, bite-sized changes. This should make it easier for users to upgrade in the future! . Broadly, many features are still on the radar. We anticipate the rest of v0.4.x to be bugfixes surrounding this new release. For version v0.5, our roadmap includes: . | a theme toggle (light/dark mode), with automatic theme switching based on browser preferences | better GDPR compliance for analytics | multi-level/recursive navigation (unlimited hierarchy of child pages) | . In future versions, we also plan on: . | adding better dark theme defaults | adding better internationalization support | exploring offline PDF generation | improving accessibility within the theme | improving search functionality | refactoring and improving the robustness of our codebase | . Have ideas for what’s next, or want to get involved? Open an issue or start a discussion and let us know! We’re looking for more contributors and maintainers to help us develop the theme. New Features . | Added: Combination by @pdmosses in #578 . | Added: dark highlighting in #463 | Added: pages and collections in #448 | Added: callouts in #466 | Fixed: breadcrumb behaviour … by @AdityaTiwari2102 in #477 | Fixed: prevent rake command corrupting search data in #495 (also listed below) | Fixed: nested lists in #496 | Fixed: set color for search input in #498 (also listed below) | Fixed: sites with no child pages (no PR) | Fixed: TOC/breadcrumbs for multiple collections in #494 | Added: collection configuration option nav_fold (no PR) | Fixed: indentation and color for folded collection navigation (no PR) | Fixed: scroll navigation to show the link to the current page in #639 | Fixed: Replace all uses of absolute_url by relative_url, by @svrooij in #544 | . | Added: custom favicon _includes by @burner1024 in #364 | Added: set color for search input by @pdmosses in #498 | Added: search placeholder configuration by @mattxwang in #613 | Added: ‘child_nav_order’ front matter to be able to sort navigation pages in reverse by @jmertic in #726 | Added: nav_footer_custom include by @nathanjessen in #474 | Added: style fixes for jekyll-asciidoc by @alyssais in #829 | Added: mermaid.js support by @nascosto in #857 | Added: support for external navigation links by @SPGoding in #876 | Added: refactor mermaid config to use mermaid_config.js include, only require mermaid.version in _config.yml by @mattxwang in #909 | Added: accessible titles to nested page nav toggle by @JPrevost in #950 | Added: better title styling for AsciiDoc examples by @alyssais in #944 | Added: docs for custom search placeholder by @mattxwang in #939 | Added: provide ability to skip to main content by @JPrevost in #949 | Added: styling for &lt;blockquote&gt; by @mattxwang in #965 | Added: custom include for TOC heading by @pdmosses in #980 | Added: experimental nav optimization for simple cases by @pdmosses in #992 | Added: support multiple Google Analytics tracking IDs, document UA -&gt; GA4 switch by @MichelleBlanchette in #1029 | Added: copy code button to code snippets by @simonebortolin in #945 | Added: restore simple configuration of favicon.ico via site.static_files by @pdmosses in #1095 | Added: modularize site components by @mattxwang in #1058 | Added: includes for custom lunr Liquid and JS code by @diablodale in #1068 | Added: new _sass/custom/setup.scss for variable definition by @mattxwang in #1135 | Added: configuration key to load a local version of mermaid by @fabrik42 in #1153 | . Bugfixes . | Fixed: prepend site.collections_dir if exists by @alexsegura in #519 | Fixed: nested task lists (#517) by @pdmosses in #855 | Fixed: suppress Liquid processing in CSS comments by @pdmosses in #686 | Fixed: prevent rake command from corrupting search data by @pdmosses in #495 | Fixed: anchor heading links should be visible on focus by @jacobhq in #846 | Fixed: add overflow-x: auto to figure.highlight by @iridazzle in #727 | Fixed: add overflow-wrap: word-break to body by @iridazzle in #889 | Fixed: vertical alignment for consecutive labels by @Eisverygoodletter in #893 | Fixed: allow links to wrap by @pdmosses in #905 | Fixed: nav scroll feature and absolute/relative URLs by @pdmosses in #898 | Fixed: exclude vendor/ in Jekyll config by @manuelhenke in #941 | Fixed: improve build time of navigation panel by @pdmosses in #956 | Fixed: spacing issue when search is disabled by @henryiii in #960 | Fixed: active grandchild link class by @pdmosses in #962 | Fixed: HTML validation issues (W3C validator) by @mattxwang in #964 | Fixed: link styling now uses text-decoration values by @mattxwang in #967 | Fixed: cleaning up Jekyll excludes by @pdmosses in #985 | Fixed: docs, narrow styling for code highlighting with line numbers by @pdmosses in #974 | Fixed: default syntax highlighting in custom color schemes @pdmosses in #986 | Fixed: incorrect disambiguation in generated TOCs by @pdmosses in #999 | Fixed: duplicated external links in collections by @pdmosses in #1001 | Fixed: import order of custom.scss; puts at end by @deseo in #1010 | Fixed: top-level active link styling by @pdmosses in #1015 | Fixed: external links for sites with no pages by @pdmosses in #1021 | Fixed: duplicate title if jekyll-seo-tag not in users’s plugins by @Tom-Brouwer in #1040 | Fixed: removes (duplicate) favicon.html, shifts content to head_custom.html by @mattxwang in #1027 | Fixed: add reversed, deprecate desc for nav child_nav_order by @jmertic in #1061 | Fixed: child.child_nav_order to node.child_nav_order by @mattxwang in #1065 | Fixed: remove all uses of / as SASS division by @mattxwang in #1074 . | note: this was originally merged as #1074 with a bug; it was reverted in #1076, and then reimplemented in #1077 | . | Fixed: skip nav collection generation when site has no pages by @pdmosses in #1092 | Fixed: standardize SCSS with declaration-block-no-redundant-longhand-properties by @simonebortolin in #1102 | Fixed: incorrect padding property value pair in labels.scss by @SConaway in #1104 | Fixed: various bugs with copy code button by @simonebortolin in #1096 | Fixed: replace inline styling for &lt;svg&gt; icons by @captn3m0 in #1110 | Fixed: incorrect padding property value pair in search.scss by @kevinlin1 in #1123 | Fixed: minor spacing and comment nits by @EricFromCanada in #1128 | Fixed: exclude images from being bundled with gem by @m-r-mccormick in #1142 | Fixed: dark theme code block background, line number colors by @m-r-mccormick in #1124 | Fixed: copy code button interaction with kramdown line numbers by @mattxwang in #1143 | . Maintenance . | Added: VScode devcontainer by @max06 in #783 | Added: webrick to Gemfile by @mattxwang in #799 | Added: ‘This site is powered by Netlify.’ to the footer by @mattxwang in #797 | Updated: new repo path by @pmarsceill in #775 | Updated: rename master -&gt; main by @pmarsceill in #776 | Updated: README by @pmarsceill in #777 | Updated: Code of Conduct to Contributor Covenant v2.1 by @mattxwang in #790 | Updated: CI files, Ruby &amp; Node Versions by @mattxwang in #820 | Updated: Stylelint to v14, extend SCSS plugins, remove primer-* configs, resolve issues by @mattxwang in #821 | Deleted: unused script directory by @mattxwang in #937 | Vendor: update jekyll-anchor-headings, lunr.js by @mattxwang in #1071 | . Documentation . | Added: docs on how to break an ol by @pdmosses in #856 | Added: docs for custom includes by @nathanjessen in #806 | Added: document caveat about variable dependencies by @waldyrious in #555 | Added: docs on how to use custom_head to add a custom favicon by @UnclassedPenguin in #814 | Added: docs load mermaid.js by default by @mattxwang in #935 | Added: warning about mandatory _-prefix for collections by @max06 in #1091 | Added: migration guide by @pdmosses in #1059 | Added: label new features introduced in v0.4 by @mattxwang in #1138 | Fixed: ol on index.md by @pmarsceill in #778 | Fixed: image link in Markdown kitchen sink by @JeffGuKang in #221 | Fixed: images in Markdown kitchen sink by @dougaitken in #782 | Fixed: clearer label of link to Jekyll quickstart by @waldyrious in #549 | Fixed: remove extra spaces in component docs by @MichelleBlanchette in #554 | Fixed: double “your” typo in index.md by @sehilyi in #499 | Fixed: “you” -&gt; “your” typo in index.md by @nathanjessen in #473 | Fixed: spacing in toc example by @henryiii in #835 | Fixed: typo in README on _config.yml by @ivanskodje in #891 | Fixed: missing code fence in navigation structure docs by @mattxwang in #906 | Fixed: table of contents on search docs by @robinpokorny in #940 | Fixed: broken docs link (custom footer) by @olgarithms in #951 | Fixed: clarify version docs by @pdmosses in #955 | Fixed: typo in changelog links @koppor in #1000 | Fixed: two bugs in “Customization” (custom favicon, new annotation) by @mattxwang in #1090 | Fixed: “View Typography Utilities” link by @agabrys in #1130 | Fixed: broken relative page links by @mattxwang in #1106 | Fixed: clarify steps to add custom lunr index code by @diablodale in #1139 | Updated: homepage (focus: new features, conciseness, deduplication) by @pdmosses in #1018 | Updated: README (focus: new features, conciseness, deduplication) by @pdmosses in #1019 | Updated: README demo video by @codewithfan in #1097 | . New Contributors . | @AdityaTiwari2102 made their first contribution in #477 | @svrooij made their first contribution in #544 | @alexsegura made their first contribution in #519 | @burner1024 made their first contribution in #364 | @JeffGuKang made their first contribution in #221 | @dougaitken made their first contribution in #782 | @max06 made their first contribution in #783 | @sehilyi made their first contribution in #499 | @nathanjessen made their first contribution in #473 | @waldyrious made their first contribution in #549 | @MichelleBlanchette made their first contribution in #554 | @henryiii made their first contribution in #835 | @jmertic made their first contribution in #726 | @jacobhq made their first contribution in #846 | @UnclassedPenguin made their first contribution in #814 | @alyssais made their first contribution in #829 | @nascosto made their first contribution in #857 | @SPGoding made their first contribution in #876 | @iridazzle made their first contribution in #727 | @ivanskodje made their first contribution in #891 | @Eisverygoodletter made their first contribution in #893 | @robinpokorny made their first contribution in #940 | @olgarithms made their first contribution in #951 | @manuelhenke made their first contribution in #941 | @JPrevost made their first contribution in #950 | @koppor made their first contribution in #1000 | @deseo made their first contribution in #1010 | @Tom-Brouwer made their first contribution in #1040 | @simonebortolin made their first contribution in #945 | @SConaway made their first contribution in #1104 | @captn3m0 made their first contribution in #1110 | @kevinlin1 made their first contribution in #1123 | @codewithfan made their first contribution in #1097 | @agabrys made their first contribution in #1130 | @diablodale made their first contribution in #1068 | @m-r-mccormick made their first contribution in #1142 | @fabrik42 made their first contribution in #1153 | . ",
    "url": "/CHANGELOG/#release-v040",
    
    "relUrl": "/CHANGELOG/#release-v040"
  },"31": {
    "doc": "CHANGELOG",
    "title": "Pre-release v0.4.0.rc5",
    "content": "Hi everyone, we’re so excited to finally release v0.4.0! For posterity’s sake, we’re going to release v0.4.0.rc5 and then immediately re-release it as v0.4.0; this should make it more clear what changes were introduced in the lead up to the minor release. This RC does not introduce any major user-facing features. It adds more customizability for custom SCSS variables (fixing a bug with callout introduction order), lunr indexing, and loading mermaid locally. In addition, it fixes bugs introduced in .rc4: incorrect CSS, inconsistencies with code block backgrounds in dark theme, and the copy code button. It also adds a migration guide for users coming from v0.3.3. Trying out pre-release v0.4.0.rc5 . Simlar to the prior release, v0.4.0.rc5 is a release candidate for the theme (i.e., a pre-release) with release v0.4.0 following immediately after. While we don’t anticipate many users using this RC, it is still possible to opt-in. To use this RC explicitly as a remote theme: . remote_theme: just-the-docs/just-the-docs@v0.4.0.rc5 . To use this RC explicitly as a gem-based theme, pin the version in your Gemfile and re-run bundle install or bundle update just-the-docs: . gem \"just-the-docs\", \"0.4.0.rc5\" . By default, users will not be upgraded to 0.4.0.rc5. To enforce that explicitly, either: . | pin your gem version in your Gemfile, like so gem \"just-the-docs\", \"0.3.3\" . | freeze the remote_theme, like so remote_theme: just-the-docs/just-the-docs@v0.3.3 . | . New Features . | Added: includes for custom lunr Liquid and JS code by @diablodale in #1068 | Added: new _sass/custom/setup.scss for variable definition by @mattxwang in #1135 | Added: configuration key to load a local version of mermaid by @fabrik42 in #1153 | . Bugfixes and Maintenance . | Fixed: incorrect padding property value pair in search.scss by @kevinlin1 in #1123 | Fixed: minor spacing and comment nits by @EricFromCanada in #1128 | Fixed: exclude images from being bundled with gem by @m-r-mccormick in #1142 | Fixed: dark theme code block background, line number colors by @m-r-mccormick in #1124 | Fixed: copy code button interaction with kramdown line numbers by @mattxwang in #1143 | . Docs . | Docs: add a migration guide by @pdmosses in #1059 | Docs: update README demo video by @codewithfan in #1097 | Docs: update “View Typography Utilities” link by @agabrys in #1130 | Docs: fix broken relative page links by @mattxwang in #1106 | Docs: clarify steps to add custom lunr index code by @diablodale in #1139 | Docs: label new features introduced in v0.4 by @mattxwang in #1138 | . New Contributors . | @kevinlin1 made their first contribution in #1123 | @codewithfan made their first contribution in #1097 | @agabrys made their first contribution in #1130 | @diablodale made their first contribution in #1068 | @m-r-mccormick made their first contribution in #1142 | @fabrik42 made their first contribution in #1153 | . ",
    "url": "/CHANGELOG/#pre-release-v040rc5",
    
    "relUrl": "/CHANGELOG/#pre-release-v040rc5"
  },"32": {
    "doc": "CHANGELOG",
    "title": "Pre-release v0.4.0.rc4",
    "content": "Happy new year! We’re celebrating with another pre-release, with features that should help theme users better adapt to changes moving forward. We aim to re-release this as v0.4.0, with only few changes. Notable new additions include: . | modular site components, which split up the site into smaller reusable components; advanced theme users can then remix layouts quickly without duplication | a “copy code” button to code blocks | fixing bugs in generated TOCs and navigation from previous prereleases | various cleanups of CSS and HTML markup | . The roadmap to v0.4.0 is small. We are only looking to: . | finish a migration guide, so users can easily upgrade from v0.3.3 to v0.4.0 | fix one last bug relating to callouts and custom colors | fix any new bugs introduced by this pre-release | . Have any questions, thoughts, or concerns? We’d love to hear from you! Please open an issue or start a discussion and let us know! . Trying out pre-release v0.4.0.rc4 . Simlar to the prior release, v0.4.0.rc4 is a release candidate for the theme (i.e., a pre-release) with release v0.4.0 coming soon. We want your help in testing the changes! As of now, the gem on RubyGems and the repository are updated to v0.4.0.rc4. To use this RC explicitly as a remote theme: . remote_theme: just-the-docs/just-the-docs@v0.4.0.rc4 . To use this RC explicitly as a gem-based theme, pin the version in your Gemfile and re-run bundle install or bundle update just-the-docs: . gem \"just-the-docs\", \"0.4.0.rc4\" . By default, users will not be upgraded to 0.4.0.rc4. To enforce that explicitly, either: . | pin your gem version in your Gemfile, like so gem \"just-the-docs\", \"0.3.3\" . | freeze the remote_theme, like so remote_theme: just-the-docs/just-the-docs@v0.3.3 . | . New Features . | Added: support multiple Google Analytics tracking IDs, document UA -&gt; GA4 switch by @MichelleBlanchette in #1029 | Added: copy code button to code snippets by @simonebortolin in #945 | Added: restore simple configuration of favicon.ico via site.static_files by @pdmosses in #1095 | Added: modularize site components by @mattxwang in #1058 | . Bugfixes and Maintenance . | Fixed: incorrect disambiguation in generated TOCs by @pdmosses in #999 | Fixed: duplicated external links in collections by @pdmosses in #1001 | Fixed: import order of custom.scss; puts at end by @deseo in #1010 | Fixed: top-level active link styling by @pdmosses in #1015 | Fixed: external links for sites with no pages by @pdmosses in #1021 | Fixed: duplicate title if jekyll-seo-tag not in users’s plugins by @Tom-Brouwer in #1040 | Fixed: removes (duplicate) favicon.html, shifts content to head_custom.html by @mattxwang in #1027 | Fixed: add reversed, deprecate desc for nav child_nav_order by @jmertic in #1061 | Fixed: child.child_nav_order to node.child_nav_order by @mattxwang in #1065 | Fixed: remove all uses of / as SASS division by @mattxwang in #1074 . | note: this was originally merged as #1074 with a bug; it was reverted in #1076, and then reimplemented in #1077 | . | Fixed: skip nav collection generation when site has no pages by @pdmosses in #1092 | Fixed: standardize SCSS with declaration-block-no-redundant-longhand-properties by @simonebortolin in #1102 | Fixed: incorrect padding property value pair in labels.scss by @SConaway in #1104 | Fixed: various bugs with copy code button by @simonebortolin in #1096 | Fixed: replace inline styling for &lt;svg&gt; icons by @captn3m0 in #1110 | Vendor: update jekyll-anchor-headings, lunr.js by @mattxwang in #1071 | . Docs . | Docs: fix typo in changelog links @koppor in #1000 | Docs: update homepage (focus: new features, conciseness, deduplication) by @pdmosses in #1018 | Docs: update README (focus: new features, conciseness, deduplication) by @pdmosses in #1019 | Docs: fix two bugs in “Customization” (custom favicon, new annotation) by @mattxwang in #1090 | Docs: Add warning about mandatory _-prefix for collections by @max06 in #1091 | Docs: remove Google Analytics on main site by @mattxwang in #1113 | . New Contributors . | @koppor made their first contribution in #1000 | @deseo made their first contribution in #1010 | @Tom-Brouwer made their first contribution in #1040 | @simonebortolin made their first contribution in #945 | @SConaway made their first contribution in #1104 | @captn3m0 made their first contribution in #1110 | . Full Changelog: https://github.com/just-the-docs/just-the-docs/compare/v0.4.0.rc3…v0.4.0.rc4 . ",
    "url": "/CHANGELOG/#pre-release-v040rc4",
    
    "relUrl": "/CHANGELOG/#pre-release-v040rc4"
  },"33": {
    "doc": "CHANGELOG",
    "title": "Pre-release v0.4.0.rc3",
    "content": "Hi there! This is (actually) hopefully the last prerelease before v0.4.0; in particular, if we find that this prerelease is stable, we’ll re-release it as v0.4.0. In general, this is a more mature pre-release; there are few new features. However, we’ll highlight @pdmosses’s work in #992 to better optimize nav generation for large sites (ex 100+ pages). We don’t expect this to affect most users; however, it is technically a breaking change, and we suggest testing your site before upgrading to this prerelease. We want your feedback! Please open an issue or start a discussion and let us know! . As soon as we get stable test results from major downstream users, we’ll push out a v0.4.0 ASAP - closing out almost 2 years of backlogged work! . Trying out pre-release v0.4.0.rc3 . Simlar to the prior release, v0.4.0.rc3 is a release candidate for the theme (i.e., a pre-release) with release v0.4.0 coming soon. We want your help in testing the changes! As of now, the gem on RubyGems and the repository are updated to v0.4.0.rc3. To use this RC explicitly as a remote theme: . remote_theme: just-the-docs/just-the-docs@v0.4.0.rc3 . To use this RC explicitly as a gem-based theme, pin the version in your Gemfile and re-run bundle install or bundle update just-the-docs: . gem \"just-the-docs\", \"0.4.0.rc3\" . By default, users will not be upgraded to 0.4.0.rc3. To enforce that explicitly, either: . | pin your gem version in your Gemfile, like so gem \"just-the-docs\", \"0.3.3\" . | freeze the remote_theme, like so remote_theme: just-the-docs/just-the-docs@v0.3.3 . | . Features . Broadly, this prerelease is feature-light! . | Added: styling for &lt;blockquote&gt; by @mattxwang in #965 | Added: custom include for TOC heading by @pdmosses in #980 | . Bugfixes and Experimental Features . Note: experimental nav optimization may be unstable. Please give us feedback! . | Added: experimental nav optimization for simple cases by @pdmosses in #992 | Fixed: spacing issue when search is disabled by @henryiii in #960 | Fixed: active grandchild link class by @pdmosses in #962 | Fixed: HTML validation issues (W3C validator) by @mattxwang in #964 | Fixed: link styling now uses text-decoration values by @mattxwang in #967 | Fixed: cleaning up Jekyll excludes by @pdmosses in #985 | Fixed: docs, narrow styling for code highlighting with line numbers by @pdmosses in #974 | Fixed: default syntax highlighting in custom color schemes @pdmosses in #986 | . Full Changelog: https://github.com/just-the-docs/just-the-docs/compare/v0.4.0.rc2…v0.4.0.rc3 . ",
    "url": "/CHANGELOG/#pre-release-v040rc3",
    
    "relUrl": "/CHANGELOG/#pre-release-v040rc3"
  },"34": {
    "doc": "CHANGELOG",
    "title": "Pre-release v0.4.0.rc2",
    "content": "This website includes docs for some new features that are not available in v0.4.0.rc1 and v0.3.3! . Hey there! This is likely the last pre-release before releasing v0.4.0, which we plan on doing soon (i.e. before the end of the month) - very exciting! Some new additions to highlight: . | significant improvement on build time of navigation panel by @pdmosses . | this is big: for a community member with over 300 pages, we shortened the build time from 3 minutes to 30 seconds! | . | improved accessibility features led by @JPrevost | more docs! | . The intention of this release candidate is to gather even more feedback on a potential v0.4.0. As it stands, we have not encountered any breaking changes with early adopters of v0.4.0.rc1. If you encounter any - for either of our pre-releases - please let us know! . Trying out pre-release v0.4.0.rc2 . Simlar to the prior release, v0.4.0.rc2 is a release candidate for the theme (i.e., a pre-release) with release v0.4.0 coming soon. We want your help in testing the changes! As of now, the gem on RubyGems and the repository are updated to v0.4.0.rc2. To use this RC explicitly as a remote theme: . remote_theme: just-the-docs/just-the-docs@v0.4.0.rc2 . To use this RC explicitly as a gem-based theme, pin the version in your Gemfile and re-run bundle install or bundle update just-the-docs: . gem \"just-the-docs\", \"0.4.0.rc2\" . By default, users will not be upgraded to 0.4.0.rc2. To enforce that explicitly, either: . | pin your gem version in your Gemfile, like so gem \"just-the-docs\", \"0.3.3\" . | freeze the remote_theme, like so remote_theme: just-the-docs/just-the-docs@v0.3.3 . | . Features . | Added: accessible titles to nested page nav toggle by @JPrevost in #950 | Added: better title styling for AsciiDoc examples by @alyssais in #944 | Added: docs for custom search placeholder by @mattxwang in #939 | Added: provide ability to skip to main content by @JPrevost in #949 | Fixed: exclude vendor/ in Jekyll config by @manuelhenke in #941 | Fixed: improve build time of navigation panel by @pdmosses in #956 | . Documentation and Maintenance . | Added: docs load mermaid.js by default by @mattxwang in #935 | Fixed: table of contents on search docs by @robinpokorny in #940 | Fixed: broken docs link (custom footer) by @olgarithms in #951 | Fixed: clarify version docs by @pdmosses in #955 | Deleted: unused script directory by @mattxwang in #937 | . New Contributors . | @robinpokorny made their first contribution in #940 | @olgarithms made their first contribution in #951 | @manuelhenke made their first contribution in #941 | @JPrevost made their first contribution in #950 | . ",
    "url": "/CHANGELOG/#pre-release-v040rc2",
    
    "relUrl": "/CHANGELOG/#pre-release-v040rc2"
  },"35": {
    "doc": "CHANGELOG",
    "title": "Pre-release v0.4.0.rc1",
    "content": "We’re back! . Hi all! The Just the Docs team is excited to have our first pre-release in over two years! It is jam-packed with features and bugfixes that have been requested by the community since 2020. They include: . | The new callouts component | Allowing pages and collections to coexist on the navigation pane | New styling: dark syntax highlighting, support for jekyll-asciidoc, word-wrapping instead of overflow for various elements | More customization: external nav links, custom nav footers, favicon includes, search color and placeholder configuration, mermaid.js support, and nav sorting | Over 20 bugfixes! Big ones include fixing the rake command, using relative_url, and search input color | More documentation, especially on using custom includes | Updating core dependencies to stable Ruby versions | A WIP template repository that allows you to setup your own repository using Just the Docs and GitHub Pages with one click - give it a shot! More documentation, etc. is on the way! | . We want your feedback! Are these changes helpful? Are our docs easy to understand? Should new features like mermaid be opt-in or opt-out? Please open an issue or start a discussion and let us know! . Trying out pre-release v0.4.0.rc1 . Due to the massive scope of these changes, we’re making v0.4.0.rc1 avaialble as a release candidate for the theme (i.e., a pre-release) with release v0.4.0 coming soon. We want your help in testing the changes! As of now, the gem on RubyGems and the repository are updated to v0.4.0.rc1. To use this RC explicitly as a remote theme: . remote_theme: just-the-docs/just-the-docs@v0.4.0.rc1 . To use this RC explicitly as a gem-based theme, pin the version in your Gemfile and re-run bundle install or bundle update just-the-docs: . gem \"just-the-docs\", \"0.4.0.rc1\" . Staying on v0.3.3 . If you’re not ready to make the switch, that’s alright! If your version of just-the-docs is pinned to v0.3.3 (i.e. by a Gemfile.lock or in remote_theme, then there’s nothing you need to do. If you have not pinned your theme version, you should either: . | pin your gem version in your Gemfile, like so gem \"just-the-docs\", \"0.3.3\" . | freeze the remote_theme, like so remote_theme: just-the-docs/just-the-docs@v0.3.3 . | . Use of branches for closed PRs (e.g., #466, #578) is now deprecated, as those branches have been (directly or indirectly) merged, and they may be deleted after the pre-release of v0.4.0.rc1. Maintenance . Internally, our maintainer team has expanded: Patrick Marsceill, the original maintainer, has stepped down from an active role after almost 4 years! We’re very thankful for the work that he’s done to create and maintain one of the most popular Jekyll themes. Please join us in giving him thanks! . The new core team currently consists of @mattxwang, @pdmosses, @skullface, @dougaitken, and @max06. Over the past six months, we’ve been triaging and merging in PRs, as well as contributing our own fixes. We’ll continue to address open issues, merge in PRs from the community, and plan out the future of Just the Docs. If you’d like to contribute, now is a great time! . Roadmap . In the short-term, we’re committed to tidying up everything for a v0.4.0 release. This involves fixing bugs reported from the community in this pre-release, as well as continually merging in minor PRs. We’re also scoping out medium and long-term projects, and want to keep you in the loop. These include: . | upgrading to Jekyll 4, and stopping support for Jekyll 3 | versioned docs - issue #728 | improved accessibility - issues #566, #870 | internationalization (i18n) - issue #59 | recursive/multi-level navigation - PR #462 | toggleable dark mode - issue #234 | . as well as DX improvements like better regression tests, CI, and tooling. If you’re interested in any of these, please join us on GitHub - any contribution (raising an issue, writing docs, or submitting a PR) is welcome! . Features . | Added: Combination by @pdmosses in #578 . | Added: dark highlighting in #463 | Added: pages and collections in #448 | Added: callouts in #466 | Fixed: breadcrumb behaviour … by @AdityaTiwari2102 in #477 | Fixed: prevent rake command corrupting search data in #495 (also listed below) | Fixed: nested lists in #496 | Fixed: set color for search input in #498 (also listed below) | Fixed: sites with no child pages (no PR) | Fixed: TOC/breadcrumbs for multiple collections in #494 | Added: collection configuration option nav_fold (no PR) | Fixed: indentation and color for folded collection navigation (no PR) | Fixed: scroll navigation to show the link to the current page in #639 | Fixed: Replace all uses of absolute_url by relative_url, by @svrooij in #544 | . | Added: custom favicon _includes by @burner1024 in #364 | Added: set color for search input by @pdmosses in #498 | Added: search placeholder configuration by @mattxwang in #613 | Added: ‘child_nav_order’ front matter to be able to sort navigation pages in reverse by @jmertic in #726 | Added: nav_footer_custom include by @nathanjessen in #474 | Added: style fixes for jekyll-asciidoc by @alyssais in #829 | Added: mermaid.js support by @nascosto in #857 | Added: support for external navigation links by @SPGoding in #876 | Added: refactor mermaid config to use mermaid_config.js include, only require mermaid.version in _config.yml by @mattxwang in #909 | Fixed: prepend site.collections_dir if exists by @alexsegura in #519 | Fixed: nested task lists (#517) by @pdmosses in #855 | Fixed: suppress Liquid processing in CSS comments by @pdmosses in #686 | Fixed: prevent rake command from corrupting search data by @pdmosses in #495 | Fixed: anchor heading links should be visible on focus by @jacobhq in #846 | Fixed: add overflow-x: auto to figure.highlight by @iridazzle in #727 | Fixed: add overflow-wrap: word-break to body by @iridazzle in #889 | Fixed: vertical alignment for consecutive labels by @Eisverygoodletter in #893 | Fixed: allow links to wrap by @pdmosses in #905 | Fixed: nav scroll feature and absolute/relative URLs by @pdmosses in #898 | . Documentation . | Added: docs on how to break an ol by @pdmosses in #856 | Added: docs for custom includes by @nathanjessen in #806 | Added: document caveat about variable dependencies by @waldyrious in #555 | Added: docs on how to use custom_head to add a custom favicon by @UnclassedPenguin in #814 | Fixed: ol on index.md by @pmarsceill in #778 | Fixed: image link in Markdown kitchen sink by @JeffGuKang in #221 | Fixed: images in Markdown kitchen sink by @dougaitken in #782 | Fixed: clearer label of link to Jekyll quickstart by @waldyrious in #549 | Fixed: remove extra spaces in component docs by @MichelleBlanchette in #554 | Fixed: double “your” typo in index.md by @sehilyi in #499 | Fixed: “you” -&gt; “your” typo in index.md by @nathanjessen in #473 | Fixed: spacing in toc example by @henryiii in #835 | Fixed: typo in README on _config.yml by @ivanskodje in #891 | Fixed: missing code fence in navigation structure docs by @mattxwang in #906 | . Maintenance . | Added: VScode devcontainer by @max06 in #783 | Added: webrick to Gemfile by @mattxwang in #799 | Added: ‘This site is powered by Netlify.’ to the footer by @mattxwang in #797 | Updated: new repo path by @pmarsceill in #775 | Updated: rename master -&gt; main by @pmarsceill in #776 | Updated: README by @pmarsceill in #777 | Updated: Code of Conduct to Contributor Covenant v2.1 by @mattxwang in #790 | Updated: CI files, Ruby &amp; Node Versions by @mattxwang in #820 | Updated: Stylelint to v14, extend SCSS plugins, remove primer-* configs, resolve issues by @mattxwang in #821 | . Dependencies . | Upgrade to GitHub-native Dependabot by @dependabot-preview in #627 | [Security] Bump y18n from 3.2.1 to 3.2.2 by @dependabot-preview in #606 | [Security] Bump hosted-git-info from 2.7.1 to 2.8.9 by @dependabot-preview in #641 | [Security] Bump lodash from 4.17.19 to 4.17.21 by @dependabot-preview in #640 | [Security] Bump ini from 1.3.5 to 1.3.8 by @dependabot-preview in #511 | Bump path-parse from 1.0.6 to 1.0.7 by @dependabot in #699 | Bump ajv from 6.10.0 to 6.12.6 by @dependabot in #766 | Bump prettier from 2.1.2 to 2.5.1 by @dependabot in #787 | Bump prettier from 2.5.1 to 2.6.2 by @dependabot in #809 | Bump prettier from 2.6.2 to 2.7.1 by @dependabot in #864 | . New Contributors . | @AdityaTiwari2102 made their first contribution in #477 | @svrooij made their first contribution in #544 | @alexsegura made their first contribution in #519 | @burner1024 made their first contribution in #364 | @JeffGuKang made their first contribution in #221 | @dougaitken made their first contribution in #782 | @max06 made their first contribution in #783 | @sehilyi made their first contribution in #499 | @nathanjessen made their first contribution in #473 | @waldyrious made their first contribution in #549 | @MichelleBlanchette made their first contribution in #554 | @henryiii made their first contribution in #835 | @jmertic made their first contribution in #726 | @jacobhq made their first contribution in #846 | @UnclassedPenguin made their first contribution in #814 | @alyssais made their first contribution in #829 | @nascosto made their first contribution in #857 | @SPGoding made their first contribution in #876 | @iridazzle made their first contribution in #727 | @ivanskodje made their first contribution in #891 | @Eisverygoodletter made their first contribution in #893 | . Full Changelog: https://github.com/just-the-docs/just-the-docs/compare/v0.3.3…v0.4.0.rc1 . ",
    "url": "/CHANGELOG/#pre-release-v040rc1",
    
    "relUrl": "/CHANGELOG/#pre-release-v040rc1"
  },"36": {
    "doc": "CHANGELOG",
    "title": "v0.3.3",
    "content": "🚀 Features . | Add custom header and footer include files @CodeSandwich (#334) | . 🐛 Bug Fixes . | Limit the effect of nav_exclude to the main navigation @pdmosses (#443) | Update normalize.scss @pdmosses (#444) | Update code.scss @pdmosses (#445) | Fix list alignment @pdmosses (#446) | . 🧰 Maintenance . | Bump stylelint-config-primer from 9.0.0 to 9.2.1 @dependabot-preview (#451) | Bump stylelint from 13.6.1 to 13.7.2 @dependabot-preview (#440) | Bump @primer/css from 15.1.0 to 15.2.0 @dependabot-preview (#436) | Bump prettier from 2.1.1 to 2.1.2 @dependabot-preview (#429) | . ",
    "url": "/CHANGELOG/#v033",
    
    "relUrl": "/CHANGELOG/#v033"
  },"37": {
    "doc": "CHANGELOG",
    "title": "v0.3.2",
    "content": "Changes . | Safe page sorting @pdmosses (#411) | v0.3.2 @pmarsceill (#388) | . 🚀 Features . | make font-sizes sass variables so they can be changed @pdebruic (#361) | run the site locally inside docker container @fogfish (#398) | Feature/doc collections @SgtSilvio (#379) | Adjust dl layout @pdmosses (#401) | . 🐛 Bug Fixes . | Add site.gh_edit_source to “Edit this page on GitHub” link @mrfleap (#418) | Inhibit text-transform for code in h4 @pdmosses (#404) | Fix native font stack precedence issue on Windows systems. @hvianna (#331) | Support for the linenos option on highlighted code @pdmosses (#375) | Update anchor_headings.html @pdmosses (#399) | Fix https @marksie1988 (#359) | . 🧰 Maintenance . | Bump prettier from 2.0.5 to 2.1.1 @dependabot-preview (#427) | Bump prettier from 2.0.5 to 2.1.1 @dependabot-preview (#419) | [Security] Bump lodash from 4.17.15 to 4.17.19 @dependabot-preview (#389) | Bump @primer/css from 14.4.0 to 15.1.0 @dependabot-preview (#402) | Bump lodash from 4.17.15 to 4.17.19 @dependabot (#384) | Bump @primer/css from 14.4.0 to 15.0.0 @dependabot-preview (#371) | . ",
    "url": "/CHANGELOG/#v032",
    
    "relUrl": "/CHANGELOG/#v032"
  },"38": {
    "doc": "CHANGELOG",
    "title": "v0.3.1",
    "content": "Changes . 🐛 Bug Fixes . | Improve accessibility by adding label to Anchor links. @mscoutermarsh (#376) | . 🧰 Maintenance . | Remove collapsible TOC on nav doc @pmarsceill (#368) | Pdmosses collapsible toc @pmarsceill (#367) | . ",
    "url": "/CHANGELOG/#v031",
    
    "relUrl": "/CHANGELOG/#v031"
  },"39": {
    "doc": "CHANGELOG",
    "title": "v0.3.0",
    "content": "Changes . | v0.2.9 @pmarsceill (#306) | . 🚀 Features . | Add print styles @pmarsceill (#362) | Navigation improvements and search sections @SgtSilvio (#352) | . 🐛 Bug Fixes . | Remove constraint with jekyll 4.1.0 @PierrickMartos (#348) | . 🧰 Maintenance . | Bump version numbers @pmarsceill (#360) | Bump stylelint from 13.3.3 to 13.6.1 @dependabot-preview (#343) | Bump stylelint-config-prettier from 8.0.1 to 8.0.2 @dependabot-preview (#349) | . ",
    "url": "/CHANGELOG/#v030",
    
    "relUrl": "/CHANGELOG/#v030"
  },"40": {
    "doc": "CHANGELOG",
    "title": "v0.2.9",
    "content": "Bug fixes . | Horizontal Alignment #103 @pmarsceill | Code snippet in headers do not inherit font size #140 @pmarsceill | Fix duplicated title and description tags #294 @iefserge | Update nav.html for handling nav_exclude #282 @blawqchain | Fix duplicate entries in nav.html and default.html #239 @KasparEtter | Don’t show pages with no title (e.g. redirects in nav) https://github.com/pmarsceill/just-the-docs/pull/295/commits/672de29f2e332a9350af7237e4fb6693c848989e @SgtSilvio | [SEARCH RAKE] Fix search generator #319 @RoiArthurB | . Enhancements . | Improvement/custom themes #186 @SgtSilvio | feat: adds “edit this page” and “page last modified” to footer #217 @malsf21 | feat: adds option to open aux links in new tab #229 @malsf21 | Default nav order #236 @pdmosses | Enable IP anonymization in Google Analytics (GDPR) #250 @r-brown | . closes #240 #308 #266 #140 #103 . ",
    "url": "/CHANGELOG/#v029",
    
    "relUrl": "/CHANGELOG/#v029"
  },"41": {
    "doc": "CHANGELOG",
    "title": "v0.2.8",
    "content": "Bugfixes . | bugfix in search.rake #218 @tiaitsch85 | . Dependency and security updates: . | Update jekyll requirement from ~&gt; 3.8.5 to &gt;= 3.8.5, &lt; 4.1.0 #197 @dependabot-preview | Update rake requirement from ~&gt; 12.3.1 to &gt;= 12.3.1, &lt; 13.1.0 #227 @dependabot-preview | Bump stylelint-config-primer from 8.0.0 to 9.0.0 #247 @dependabot-preview | Update bundler requirement from ~&gt; 2.0.1 to ~&gt; 2.1.4 #268 @dependabot-preview | Bump @primer/css from 12.7.0 to 14.3.0 #296 @dependabot-preview | . Operations . | Update CI to test multiple versions of Jekyll | Update CI to check the rake command that builds the search file | . fixes #291 #256 #293 #177 . ",
    "url": "/CHANGELOG/#v028",
    
    "relUrl": "/CHANGELOG/#v028"
  },"42": {
    "doc": "CHANGELOG",
    "title": "v0.2.7",
    "content": "Bugs fixed . | Anchor headings are now displayed on hover, not only on heading hover | Deduplicated anchor heading svg | If last page of site.html_pages was excluded from search, search json breaks | Config variable should be blanklines not blank_lines for html compression | list-style-none does not hide bullets on ul | . Enhancements . | Summary for child pages appears in generated TOC | Site logo configuration supported replacing title text with image | Allow custom CSS overrides (new scss partial at the end of the cascade) separate from variable overrides. | Configuration around search strings added to allow search for hyphenated words | . Maintenance . | Update docs to suggest using index.md as section page filename | Bump @primer/css from 12.6.0 to 12.7.0 | Bump mixin-deep from 1.3.1 to 1.3.2 | Bump stylelint-config-primer from 7.0.1 to 8.0.0 | . PR included . | #98 by @stefanoborini Introduces the possibility for a summary in the table of contents | #141 by @ghabs Fix trailing comma bug in search-data.json | #153 by @jacobherrington Change button copy on theme preview | #181 by @m3nu Recommend using index.md as parent page for sections | #183 by @SgtSilvio Improve heading anchors | #187 by @SgtSilvio Improvement/site logo | #200 Bump mixin-deep from 1.3.1 to 1.3.2 | #203 by @pdmosses Search config | #205 by @pdmosses Fix blank_lines var to blanklines in config.yml | #206 by @iamcarrico Allow for custom overrides by the user | #208 Bump @primer/css from 12.6.0 to 12.7.0 | #213 Bump mixin-deep from 1.3.1 to 1.3.2 | #214 Bump stylelint-config-primer from 7.0.1 to 8.0.0 | #215 Bump @primer/css from 12.6.0 to 12.7.0 | . ",
    "url": "/CHANGELOG/#v027",
    
    "relUrl": "/CHANGELOG/#v027"
  },"43": {
    "doc": "CHANGELOG",
    "title": "v0.2.6",
    "content": "Bugs fixed . | Google Analytics tag has been updated #162 | ~BaseURL has been modified #109~ Reverted – seems the existing implementation worked | Titles can now wrap fixes #106 | . Enhancements . | Search now displays content preview #135 | Custom footer content added #179 | Now using GitHub Actions for CI #170 | . Maintenance . | lunrjs upgraded #135 | Nav generation is optimized #159 | Stylelint upgrade #143 | Stylelint config primer upgrade #149 | Lodash upgrade #160 | . PR included . ~#109 by @daviddarnes - Fix baseurl link~ Reverted #135 by @SgtSilvio - Upgrades lunr.js, improves search UI, adds heading anchors #152 by @yavorg - Improves syntax highlighting for js readablity #159 by @julienduchesne - Optimizes nav generation #162 by @nergmada - Modifies the google analytics code to match the new tags used by GA . ",
    "url": "/CHANGELOG/#v026",
    
    "relUrl": "/CHANGELOG/#v026"
  },"44": {
    "doc": "CHANGELOG",
    "title": "v0.2.5",
    "content": "Bugs fixed . | Duplicate title tag when Jekyll SEO Plugin gem is used #125 #126 | . Enhancements . | Favicon support added #118 | . Maintenance . | Bump stylelint-config-primer from 6.0.0 to 7.0.0 #123 | Bump @primer/css from 12.2.3 to 12.3.1 #129 | Add workflow to publish to GPR | Fix workflow to publish to Ruby Gems | . ",
    "url": "/CHANGELOG/#v025",
    
    "relUrl": "/CHANGELOG/#v025"
  },"45": {
    "doc": "CHANGELOG",
    "title": "v0.2.4",
    "content": "Bugs . | #102 Remove unnecessary console.log() @JoeNyland | #97 Import custom Sass variable overrides before default variables are defined @montchr and @ptvandi | . Additions . | #117 Add links to docs for setting up GH pages locally @gnarea | #95 Add SEO and ‘lang’ param for _config @gebeto | . ",
    "url": "/CHANGELOG/#v024",
    
    "relUrl": "/CHANGELOG/#v024"
  },"46": {
    "doc": "CHANGELOG",
    "title": "v0.2.3",
    "content": "Enhancements . | Adds ability to use Google Analytics tracking by @pmarsceill | . Bug fixes . | Fixes 404 error for “/assets/js//search-data.json” by @stephenedmondson | Fixes #80 Single quotes in the string were unescaped and ruby attempted variable substitution of amp within it (which failed) by @novelistparty | Fixes bug that would only show 2 or more search results (not one) by @ilivewithian | Fixes a typo on the layout example by @woernfl | Fixes #78 Page scroll position too far down on load by @pmarsceill | Fixds ability to nest ul in ol without breaking style or counters | . Dependency updates . | Bumps stylelint dependency from 9.9.0 to 9.10.1 | . ",
    "url": "/CHANGELOG/#v023",
    
    "relUrl": "/CHANGELOG/#v023"
  },"47": {
    "doc": "CHANGELOG",
    "title": "v0.2.2",
    "content": ". | Bumps stylelint-config-primer to 3.0.1 #44 | Bumps bundler req to 2.0.1 #61 | Adds custom 404 page | Excludes package-lock.json from jekyll build #47 | Fixes keyboard scrolling / focus #48 | Adds ARIA roles to navigation elements | Adds support for optional page description metadata (if present in yaml front matter) | Addresses some issues with search in #46 | Option to hide TOC on parent pages if turned off in page’s YAML front matter #30 | Option to suppress an item from being indexed by search if present in page’s YAML front matter #32 | . ",
    "url": "/CHANGELOG/#v022",
    
    "relUrl": "/CHANGELOG/#v022"
  },"48": {
    "doc": "CHANGELOG",
    "title": "v0.2.1",
    "content": "This update fixes security vulnerablities in the lodash sub-dependency and bumps other dev dependencies to their latest version. ",
    "url": "/CHANGELOG/#v021",
    
    "relUrl": "/CHANGELOG/#v021"
  },"49": {
    "doc": "CHANGELOG",
    "title": "v0.2.0",
    "content": "Adds: . | Dark mode via color_scheme parameter | Ability to exclude a page from the main nav with nav_exclude parameter closes #21 | Ability for create children of children pages (3 nav levels) closes #25 | . Changes: . | Permalink structure for tiered navigation has been updated | Some colors have been updated for consistency / accessibility | . ",
    "url": "/CHANGELOG/#v020",
    
    "relUrl": "/CHANGELOG/#v020"
  },"50": {
    "doc": "CHANGELOG",
    "title": "v0.1.6",
    "content": "Added . | Support for task list styles #19 | Configuration docs | Configuration option to enable / disable search | Normalize.scss dependency pulled into project #16 # | . Fixed . | Layout bug in navigation #17 | . ",
    "url": "/CHANGELOG/#v016",
    
    "relUrl": "/CHANGELOG/#v016"
  },"51": {
    "doc": "CHANGELOG",
    "title": "v0.1.5",
    "content": "Major changes: . | Fixed bug where the rake task would fail when the assets/js directory didn’t exist | . ",
    "url": "/CHANGELOG/#v015",
    
    "relUrl": "/CHANGELOG/#v015"
  },"52": {
    "doc": "CHANGELOG",
    "title": "v0.1.4",
    "content": "Major changes: . | Adds Rake as a runtime dependency | Definition list styled | Sidebar and support cleaned up for smaller screen support | Updated some stale docs | . ",
    "url": "/CHANGELOG/#v014",
    
    "relUrl": "/CHANGELOG/#v014"
  },"53": {
    "doc": "CHANGELOG",
    "title": "v0.1.3",
    "content": "Major changes: . | Fix path problems, typos, and general clean-up for OSS. | . ",
    "url": "/CHANGELOG/#v013",
    
    "relUrl": "/CHANGELOG/#v013"
  },"54": {
    "doc": "CHANGELOG",
    "title": "v0.1.2",
    "content": "Fix paths when deployed to gh-pages . ",
    "url": "/CHANGELOG/#v012",
    
    "relUrl": "/CHANGELOG/#v012"
  },"55": {
    "doc": "CHANGELOG",
    "title": "v0.1.1",
    "content": "Major updates: . | Adds search to mobile nav | Pulls footer to bottom of the page on mobile (not hidden in nav) | . Minor updates: . | Cleans up h1 typography spacing | . ",
    "url": "/CHANGELOG/#v011",
    
    "relUrl": "/CHANGELOG/#v011"
  },"56": {
    "doc": "Guidance for Connecting Data to AWS Clean Rooms",
    "title": "Reference Architecture for Data Connector",
    "content": "This reference architecture provides an efficient way to ingest and package data for AWS Clean Rooms data collaboration. Figure 1 - Diagram of AWS Clean Rooms data connectors . ",
    "url": "/connecting-data-clean-rooms.html#reference-architecture-for-data-connector",
    
    "relUrl": "/connecting-data-clean-rooms.html#reference-architecture-for-data-connector"
  },"57": {
    "doc": "Guidance for Connecting Data to AWS Clean Rooms",
    "title": "Implementation Examples for common sources of advertising and marketing data",
    "content": "Salesforce Marketing Cloud . Salesforce Marketing Cloud provides a native integration to Amazon Simple Storage Service (Amazon S3). Data stored in custom objects can be exported to Amazon S3 using Automation Studio. Adobe Experience Platform . Adobe Experience Platform provides a native integration to Amazon S3 service. Data stored in custom objects can be exported to Amazon S3 using Automation Studio. Refer to this guidance for provisioning data stored in Adobe Experience Platform for AWS Clean Rooms data collaboration. Google Cloud Platform . Data stored Google Big Query can be ingested in to Amazon S3 using AWS Glue service. Refer to this guidance for provisioning data stored in Google Analytics for AWS Clean Rooms data collaboration. ",
    "url": "/connecting-data-clean-rooms.html#implementation-examples-for-common-sources-of-advertising-and-marketing-data",
    
    "relUrl": "/connecting-data-clean-rooms.html#implementation-examples-for-common-sources-of-advertising-and-marketing-data"
  },"58": {
    "doc": "Guidance for Connecting Data to AWS Clean Rooms",
    "title": "Further Strategies to consider for data ingestion",
    "content": "The processing steps in the data connector can be broadly divided into two phases: . | Data ingestion . | Data preparation/transformation . | . The data ingestion strategy varies widely given the broad list of data sources and their integration capabilities. These data sources are divided into 5 categories based on their deployment architecture, including: . | Third Party SaaS solutions: Salesforce Marketing Cloud, Adobe Experience Platform exposing an HTTP API . | Third Party SaaS solutions: Salesforce Marketing Cloud, Adobe Experience Platform having capabilities to export data in to Amazon S3 directly . | Relational Data stores hosted in public cloud: Google Cloud Platform (Google file storage, Google Big Query) . | Data made available in file format on an SFTP server . | Data made available in a public cloud object storage (Azure, Google) . | . This reference architecture captures the AWS services to consider for each category of the data sources. Figure 3 - Diagram of AWS services for consideration when building with this Guidance . Once data is ingested into Amazon S3, a common pattern displayed in the aforementioned reference architecture can be used to transform and package data for AWS Clean Rooms data collaboration. ",
    "url": "/connecting-data-clean-rooms.html#further-strategies-to-consider-for-data-ingestion",
    
    "relUrl": "/connecting-data-clean-rooms.html#further-strategies-to-consider-for-data-ingestion"
  },"59": {
    "doc": "Guidance for Connecting Data to AWS Clean Rooms",
    "title": "Guidance for Connecting Data to AWS Clean Rooms",
    "content": ". This Guidance helps advertisers, publishers, and data providers in the advertising and marketing industry provision data for an AWS Clean Rooms collaboration. Data Connectors capture best practices for connecting to specific types of data sources, landing data in AWS, transforming, and making data available for a data collaboration. ",
    "url": "/connecting-data-clean-rooms.html",
    
    "relUrl": "/connecting-data-clean-rooms.html"
  },"60": {
    "doc": "Guidance for Developing Applications Using Generative AI with Amazon CodeWhisperer",
    "title": "Starting CodeWhisperer",
    "content": "Use the following documentation to get CodeWhisperer configured for your development environment: . | Setting Up - steps needed before using CodeWhisperer for the first time. | Getting Started - how to set up CodeWhisperer with each of the supported integrated development environments (IDEs). | . ",
    "url": "/developing-applications-using-generative-ai-with-amazon-codewhisperer.html#starting-codewhisperer",
    
    "relUrl": "/developing-applications-using-generative-ai-with-amazon-codewhisperer.html#starting-codewhisperer"
  },"61": {
    "doc": "Guidance for Developing Applications Using Generative AI with Amazon CodeWhisperer",
    "title": "Use Case: Automate unit test generation (Python)",
    "content": "CodeWhisperer can offload writing repetitive unit test code. Based on natural language comments, CodeWhisperer automatically recommends unit test code that matches your implementation code. In the snippets below, you will see how CodeWhisperer can assist developers with automatic generation of unit tests to improve code coverage. | Open an empty directory in your IDE, such as Visual Studio Code. For this use case, we used Python in Visual Studio Code. | . CodeWhisperer uses artificial intelligence (AI) to provide code recommendations that are non-deterministic. The code suggestions that CodeWhisperer generates in your development session may vary. | (Optional) In the terminal, create a new Python virtual environment: | . python3 -m venv .venv source .venv/bin/activate . | Install basic testing libraries: | . pip install pytest pytest-cov . | Create a new file named calculator.py . | Insert the following comment at the beginning of the file to start building a simple calculator class, and then select Enter: . | . # example Python class for a simple calculator . CodeWhisperer will then start making suggestions to generate new code. | To accept these suggestions, select Tab.  | . Figure 6 - Building a simple calculator class . If CodeWhisperer does not automatically make a suggestion, you can manually trigger CodeWhisperer with Alt + C for Windows/Linux, or Option + C for macOS. Additional suggestions can be viewed by selecting the Right arrow key. To see previous suggestions, select the Left arrow key. To reject a recommendation, select ESC or the backspace/delete key. Continue building the calculator class by selecting the Enter key and accepting CodeWhisperer suggestions (automatically or manually). CodeWhisperer will suggest basic functions for the calculator class, such as add(), subtract(), multiply(), and divide(). It can also suggest more advanced functions such as square(), cube(), and square_root(). # example Python class for a simple calculator class Calculator: # add two numbers def add(self, a, b): return a + b # subtract two numbers def subtract(self, a, b): return a - b # multiply two numbers def multiply(self, a, b): return a * b # divide two numbers def divide(self, a, b): return a / b # square a number def square(self, a): return a * a # cube a number def cube(self, a): return a * a * a # square root a number def square_root(self, a): return a ** 0.5 # cube root a number def cube_root(self, a): return a ** (1/3) . Automate the generating of unit tests . Now, let’s run some tests to examine code coverage. Enter: . pytest ---cov=. And you will likely see: . Figure 7 - Displays ‘no tests ran’ . That’s right, we don’t have any tests, nor do we have any code coverage! Let’s use CodeWhisperer to help us automatically generate unit tests, and improve our code coverage.   . | Create a new file named test_calculator . | Insert the following code and comment at the beginning of the file to start building unit tests for the Calculator class, and then select Enter: . | . import pytest from calculator import Calculator # fixture for calculator . Figure 8 - Building unit tests for the Calculator class . | Continue building the calculator class by pressing the Enter key and accepting CodeWhisperer suggestions (automatically or manually). CodeWhisperer will suggest unit tests with the previously implemented class as context. | . import pytestfrom calculator import Calculator # fixture for calculator @pytest.fixture def calculator(): return Calculator() # unit test for multiply() def test_multiply(calculator): assert calculator.multiply(2, 3) == 6 # unit test for divide() def test_divide(calculator): assert calculator.divide(6, 3) == 2 with pytest.raises(ZeroDivisionError): calculator.divide(6, 0) # unit test for add() def test_add(calculator): assert calculator.add(2, 3) == 5 assert calculator.add(2, -3) == -1 assert calculator.add(0, 0) == 0 # unit test for subtract() def test_subtract(calculator): assert calculator.subtract(2, 3) == -1 assert calculator.subtract(2, -3) == 5 assert calculator.subtract(0, 0) == 0 # unit test for square() def test_square(calculator): assert calculator.square(2) == 4 assert calculator.square(0) == 0 with pytest.raises(TypeError): calculator.square(\"a\") # unit test for cube() def test_cube(calculator): assert calculator.cube(2) == 8 assert calculator.cube(0) == 0 with pytest.raises(TypeError): calculator.cube(\"a\")* . Let’s try running our newly implemented unit tests to examine code coverage. Enter: . pytest -cov=. Figure 9 - CodeWhisperer automatically generating the unit test methods . As you can see, CodeWhisperer was able to automatically generate the unit test methods (including the appropriate assertion values), increasing code coverage and reducing implementation time. ",
    "url": "/developing-applications-using-generative-ai-with-amazon-codewhisperer.html#use-case-automate-unit-test-generation-python",
    
    "relUrl": "/developing-applications-using-generative-ai-with-amazon-codewhisperer.html#use-case-automate-unit-test-generation-python"
  },"62": {
    "doc": "Guidance for Developing Applications Using Generative AI with Amazon CodeWhisperer",
    "title": "Use Case: Build applications using AWS services",
    "content": "Builders can speed up the development process for their applications with code recommendations for AWS APIs across the most popular services, including Amazon Elastic Compute (Amazon EC2), AWS Lambda, and Amazon Simple Storage Service (Amazon S3). CodeWhisperer can analyze and suggest custom AWS resources tailored to the context you provide. | Open an empty directory in your IDE, such as Visual Studio Code or JetBrains. For this use case, we used Python in JetBrains PyCharm IDE. | . CodeWhisperer uses AI to provide code recommendations that are non-deterministic. The code suggestions that CodeWhisperer generates in your development session may vary. | (Optional) In the terminal, create a new Python virtual environment: | . python3 -m venv .venv source .venv/bin/activate . | Install basic software development kit (SDK) libraries: | . pip install boto3 . | Open a new or existing Python file, and try a few of the examples below. | . Examples . Generating custom IAM policies . # create an IAM policy with read and write access to S3 . Figure 10 - Creating an IAM policy with read and write access to Amazon S3 . Paginating Results from SDK . # retrieve and iterate through paginated IAM users in account . Figure 11 - Retrieving and iterating through paginated IAM users . Creating encryption-enabled resources . # create bucket with server-side encryption enabled . Figure 12 - Creating bucket with server-side encryption . Creating database schemas . # create DynamoDB table for users using email as primary key and date created as sort key . Figure 13 - Creating Amazon DynamoDB table . ",
    "url": "/developing-applications-using-generative-ai-with-amazon-codewhisperer.html#use-case-build-applications-using-aws-services",
    
    "relUrl": "/developing-applications-using-generative-ai-with-amazon-codewhisperer.html#use-case-build-applications-using-aws-services"
  },"63": {
    "doc": "Guidance for Developing Applications Using Generative AI with Amazon CodeWhisperer",
    "title": "Guidance for Developing Applications Using Generative AI with Amazon CodeWhisperer",
    "content": ". This Guidance aims to familiarize users with a machine learning-powered code generator to assist and improve their development productivity. It demonstrates how CodeWhisperer can generate code for various development-related uses such as unit testing, and creating and integrating with AWS resources. ",
    "url": "/developing-applications-using-generative-ai-with-amazon-codewhisperer.html",
    
    "relUrl": "/developing-applications-using-generative-ai-with-amazon-codewhisperer.html"
  },"64": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "Introduction",
    "content": "This user guide is for anyone interested in efficiently deploying Amazon Elastic Kubernetes Service (EKS) that complements open-source documentation for EKS Blueprints, available in the AWS Solutions Library open-source GitHub repository. It introduces common concepts, specific details for configuration of EKS for selected use cases, and step-by-step instructions on how to deploy that EKS Blueprint using HashiCorp Terraform Open Source Infrastructure as Code (IaC) automation software. By following this guide, you will be able to: . | Familiarize yourself with the concepts of EKS Blueprints . | Learn ways to customize those Blueprints through parameters . | Deploy an Amazon EKS cluster with a new Amazon Virtual Private Cloud (Amazon VPC) . | Deploy an Amazon EKS cluster with operational software and Argo CD, a GitOps tool used for deployment of both add-on components and applications, into an Amazon EKS cluster . | Deploy an Amazon EKS cluster with Apache Spark and other related add-ons for data analytics . | If needed: properly clean-up and destroy an Amazon EKS cluster deployment provisioned using an EKS Blueprint . | . Finally, there are tips on support and troubleshooting at the end of this guide. ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html#introduction",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html#introduction"
  },"65": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "What are EKS Blueprints?",
    "content": "EKS Blueprints help you compose complete EKS clusters that are fully bootstrapped with operational software that is needed to deploy and operate application workloads. With EKS Blueprints, you describe the configuration of a desired state of your EKS environment, such as control plane, compute plane (worker nodes), and Kubernetes add-ons, as an Infrastructure as a Code (IaC) blueprint. Once an EKS Blueprint is configured, you can use it to stamp out consistent EKS environments across multiple AWS accounts and Regions using continuous deployment automation. You can utilize EKS Blueprints to easily bootstrap an EKS cluster with Amazon EKS add-ons as well as a wide range of popular open-source add-ons, including Prometheus, Karpenter, Nginx, Traefik, AWS Load Balancer Controller, Fluent Bit, KEDA, Argo CD (GitOps), Apache Spark and more. EKS Blueprints also help to implement relevant security controls needed to operate workloads from multiple teams in the same cluster. ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html#what-are-eks-blueprints",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html#what-are-eks-blueprints"
  },"66": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "Core Concepts",
    "content": "Below is a high-level overview of the Core Concepts that are incorporated into EKS Blueprints. It is assumed the reader is familiar with Git, Docker, Kubernetes, and AWS. | Concept | Description | . | Cluster | An Amazon EKS Cluster and associated worker groups. | . | Add-ons | Operational software that provides key functionality to support your Kubernetes applications. | . | Teams | A logical grouping of AWS Identity and Access Management (IAM) identities that have access to Kubernetes resources. | . | Pipeline | Continuous Delivery pipelines for deploying clusters and add-ons | . | Application | An application that runs within an EKS Cluster. | . Cluster . A cluster is an Amazon EKS cluster. EKS Blueprints provide customized compute options you can apply to your clusters. The framework currently supports Amazon Elastic Compute Cloud (Amazon EC2) and AWS Fargate instances for compute plane nodes. It also supports managed and self-managed Node groups. To specify the type of compute you want to use for your cluster, you can utilize the managed_node_groups,self_managed_nodegroups or fargate_profiles Terraform variables. See our Node Groups documentation for more detail. Add-ons . Add-ons allow you to configure operational tools you want to deploy into your EKS clusters. When you configure add-ons for a cluster, those add-ons will be provisioned at deploy time by the Terraform Helm provider. Add-ons can deploy both Kubernetes specific resources and AWS resources needed to support add-on functionality. For example, the metrics-server add-on only deploys the Kubernetes manifests that are needed to run the Kubernetes Metrics Server. By contrast, the aws-load-balancer-controller add-on deploys both Kubernetes YAML, in addition to creating resources through AWS APIs that are needed to support the AWS Load Balancer Controller functionality. EKS Blueprints allow you to manage cluster add-ons directly through Terraform (by using the Terraform Helm provider) or through GitOps processes with Argo CD. See Add-ons documentation section for detailed information. Teams . Teams allow you to configure the logical grouping of users that have access to your EKS clusters, in addition to the access permissions they are granted. EKS Blueprints currently supports two types of teams: application-team and platform-team. application-team members are granted access to specific namespaces, platform-team members are granted administrative access to clusters. See our Teams documentation page for detailed information. Application . Applications represent the actual workloads that run within a Kubernetes cluster. The framework leverages a GitOps approach for deploying applications onto clusters. See Applications documentation for detailed information. ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html#core-concepts",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html#core-concepts"
  },"67": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "Node Groups",
    "content": "The framework uses dedicated submodules for creating AWS Managed Node Groups, Self-managed Node groups and Fargate profiles. These modules provide flexibility to add or remove managed/self-managed compute node groups or Fargate profiles by simply adding or removing a map of values to input config. See example. The aws-auth ConfigMap handled by this module allows nodes to join your cluster, and you can also use this ConfigMap to add role-based access control (RBAC) to AWS Identity and Access Management (IAM) users and roles. Each node group can have a dedicated IAM role, Launch template, and Security Group to improve the security. Additional IAM Roles, Users and Accounts . Access to EKS clusters using AWS IAM entities is enabled by the AWS IAM Authenticator for Kubernetes, which runs on the Amazon EKS control plane. The authenticator gets its configuration information from the aws-auth ConfigMap. The following config grants additional AWS IAM users or roles an ability to interact with your cluster. However, the best practice is to leverage soft-multitenancy with the help of the Teams module. The Teams feature helps to manage users with dedicated Kubernetes (also known as K8s) namespaces, RBAC, IAM roles, and registers them with aws-auth to provide access to the EKS Cluster. The example below demonstrates adding IAM Roles, IAM Users, and Accounts using the EKS Blueprints module. An example of a source code file relevant to this Guidance where those changes can be made can be found here. module \"eks_blueprints\" { source = \"github.com/aws-solutions-library-samples/guidance-for-automated-provisioning-of-amazon-elastic-kubernetes-service-using-terraform\" # EKS CLUSTER PARAMETERS cluster_version = \"1.24\" # EKS K8s Version vpc_id = \"&lt;vpcid&gt;\" # Enter VPC ID private_subnet_ids = [\"&lt;subnet-a&gt;\", \"&lt;subnet-b&gt;\", \"&lt;subnet-c&gt;\"] # Enter Private Subnet IDs # List of map_roles map_roles = [ { rolearn = \"arn:aws:iam::&lt;aws-account-id&gt;:role/&lt;role-name&gt;\" # The ARN of the IAM role username = \"ops-role\" # The user name within Kubernetes to map to the IAM role groups = [\"system:masters\"] # A list of groups within Kubernetes to which the role is mapped; Checkout K8s Role and RoleBindings } ] # List of map_users map_users = [ { userarn = \"arn:aws:iam::&lt;aws-account-id&gt;:user/&lt;username&gt;\" # The ARN of the IAM user to add. username = \"opsuser\" # The user name within Kubernetes to map to the IAM role groups = [\"system:masters\"] # A list of groups within Kubernetes to which the role is mapped; Checkout K8s Role and Rolebindings } ] map_accounts = [\"123456789\", \"9876543321\"] # List of AWS account ids } . Managed Node Groups . The example below demonstrates the minimum configuration required to deploy a managed node group. An example of the source code file relevant to this Guidance where those changes can be made can be found here. # EKS MANAGED NODE GROUPS managed_node_groups = { mg_4 = { node_group_name = \"managed-ondemand\" instance_types = [\"m4.large\"] min_size = 3 max_size = 3 desired_size = 3 subnet_ids = [] # Mandatory Public or Private Subnet IDs } } . The example below demonstrates advanced configuration options for a managed node group with launch templates. An example of the source code file relevant to this Guidance where those changes can be made can be found here. managed_node_groups = { mg_m4 = { # 1&gt; Node Group configuration node_group_name = \"managed-ondemand\" create_launch_template = true # false will use the default launch template launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or windows or bottlerocket public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates; pre_userdata = &lt;&lt;-EOT yum install -y amazon-ssm-agent systemctl enable amazon-ssm-agent &amp;&amp; systemctl start amazon-ssm-agent EOT # 2&gt; Node Group scaling configuration desired_size = 3 max_size = 3 min_size = 3 max_unavailable = 1 # or percentage = 20 # 3&gt; Node Group compute configuration ami_type = \"AL2_x86_64\" # Amazon Linux 2(AL2_x86_64), AL2_x86_64_GPU, AL2_ARM_64, BOTTLEROCKET_x86_64, BOTTLEROCKET_ARM_64 capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m4.large\"] # List of instances used only for SPOT type disk_size = 50 # 4&gt; Node Group network configuration subnet_ids = [] # Mandatory - # Define private/public subnets list with comma separated [\"subnet1\",\"subnet2\",\"subnet3\"] # optionally, configure a taint on the compute node group: k8s_taints = [{key= \"purpose\", value=\"execution\", \"effect\"=\"NO_SCHEDULE\"}] k8s_labels = { Environment = \"preprod\" Zone = \"dev\" WorkerType = \"ON_DEMAND\" } additional_tags = { ExtraTag = \"m4-on-demand\" Name = \"m4-on-demand\" subnet_type = \"private\" } } } . For additional EKS Node Group configuration options, please see the following documentation on GitHub. Check the following references for additional details: . | Amazon EBS and NVMe on Linux instances . | AWS NVMe drivers for Windows instances . | EC2 Instance Update – M5 Instances with Local NVMe Storage (M5d) . | . ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html#node-groups",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html#node-groups"
  },"68": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "IAM Policies and Roles",
    "content": "The IAM policy below illustrates the minimum set of permissions needed to run EKS Blueprints, mainly focused on the list of allowed IAM actions: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"aps:CreateAlertManagerDefinition\", \"aps:CreateWorkspace\", \"aps:DeleteAlertManagerDefinition\", \"aps:DeleteWorkspace\", \"aps:DescribeAlertManagerDefinition\", \"aps:DescribeWorkspace\", \"aps:ListTagsForResource\", \"autoscaling:CreateAutoScalingGroup\", \"autoscaling:CreateOrUpdateTags\", \"autoscaling:DeleteAutoScalingGroup\", \"autoscaling:DeleteLifecycleHook\", \"autoscaling:DeleteTags\", \"autoscaling:DescribeAutoScalingGroups\", \"autoscaling:DescribeLifecycleHooks\", \"autoscaling:DescribeTags\", \"autoscaling:PutLifecycleHook\", \"autoscaling:SetInstanceProtection\", \"autoscaling:UpdateAutoScalingGroup\", \"ec2:AllocateAddress\", \"ec2:AssociateRouteTable\", \"ec2:AttachInternetGateway\", \"ec2:AuthorizeSecurityGroupEgress\", \"ec2:AuthorizeSecurityGroupIngress\", \"ec2:CreateEgressOnlyInternetGateway\", \"ec2:CreateInternetGateway\", \"ec2:CreateLaunchTemplate\", \"ec2:CreateNatGateway\", \"ec2:CreateNetworkAclEntry\", \"ec2:CreateRoute\", \"ec2:CreateRouteTable\", \"ec2:CreateSecurityGroup\", \"ec2:CreateSubnet\", \"ec2:CreateTags\", \"ec2:CreateVpc\", \"ec2:DeleteEgressOnlyInternetGateway\", \"ec2:DeleteInternetGateway\", \"ec2:DeleteLaunchTemplate\", \"ec2:DeleteNatGateway\", \"ec2:DeleteNetworkAclEntry\", \"ec2:DeleteRoute\", \"ec2:DeleteRouteTable\", \"ec2:DeleteSecurityGroup\", \"ec2:DeleteSubnet\", \"ec2:DeleteTags\", \"ec2:DeleteVpc\", \"ec2:DescribeAccountAttributes\", \"ec2:DescribeAddresses\", \"ec2:DescribeAvailabilityZones\", \"ec2:DescribeEgressOnlyInternetGateways\", \"ec2:DescribeImages\", \"ec2:DescribeInternetGateways\", \"ec2:DescribeLaunchTemplateVersions\", \"ec2:DescribeLaunchTemplates\", \"ec2:DescribeNatGateways\", \"ec2:DescribeNetworkAcls\", \"ec2:DescribeNetworkInterfaces\", \"ec2:DescribeRouteTables\", \"ec2:DescribeSecurityGroups\", \"ec2:DescribeSecurityGroupRules\", \"ec2:DescribeSubnets\", \"ec2:DescribeTags\", \"ec2:DescribeVpcAttribute\", \"ec2:DescribeVpcClassicLink\", \"ec2:DescribeVpcClassicLinkDnsSupport\", \"ec2:DescribeVpcs\", \"ec2:DetachInternetGateway\", \"ec2:DisassociateRouteTable\", \"ec2:ModifySubnetAttribute\", \"ec2:ModifyVpcAttribute\", \"ec2:ReleaseAddress\", \"ec2:RevokeSecurityGroupEgress\", \"ec2:RevokeSecurityGroupIngress\", \"eks:CreateAddon\", \"eks:CreateCluster\", \"eks:CreateFargateProfile\", \"eks:CreateNodegroup\", \"eks:DeleteAddon\", \"eks:DeleteCluster\", \"eks:DeleteFargateProfile\", \"eks:DeleteNodegroup\", \"eks:DescribeAddon\", \"eks:DescribeAddonVersions\", \"eks:DescribeCluster\", \"eks:DescribeFargateProfile\", \"eks:DescribeNodegroup\", \"eks:TagResource\", \"elasticfilesystem:CreateFileSystem\", \"elasticfilesystem:CreateMountTarget\", \"elasticfilesystem:DeleteFileSystem\", \"elasticfilesystem:DeleteMountTarget\", \"elasticfilesystem:DescribeFileSystems\", \"elasticfilesystem:DescribeLifecycleConfiguration\", \"elasticfilesystem:DescribeMountTargetSecurityGroups\", \"elasticfilesystem:DescribeMountTargets\", \"emr-containers:CreateVirtualCluster\", \"emr-containers:DeleteVirtualCluster\", \"emr-containers:DescribeVirtualCluster\", \"events:DeleteRule\", \"events:DescribeRule\", \"events:ListTagsForResource\", \"events:ListTargetsByRule\", \"events:PutRule\", \"events:PutTargets\", \"events:RemoveTargets\", \"iam:AddRoleToInstanceProfile\", \"iam:AttachRolePolicy\", \"iam:CreateInstanceProfile\", \"iam:CreateOpenIDConnectProvider\", \"iam:CreatePolicy\", \"iam:CreateRole\", \"iam:CreateServiceLinkedRole\", \"iam:DeleteInstanceProfile\", \"iam:DeleteOpenIDConnectProvider\", \"iam:DeletePolicy\", \"iam:DeleteRole\", \"iam:DetachRolePolicy\", \"iam:GetInstanceProfile\", \"iam:GetOpenIDConnectProvider\", \"iam:GetPolicy\", \"iam:GetPolicyVersion\", \"iam:GetRole\", \"iam:ListAttachedRolePolicies\", \"iam:ListInstanceProfilesForRole\", \"iam:ListPolicyVersions\", \"iam:ListRolePolicies\", \"iam:PassRole\", \"iam:RemoveRoleFromInstanceProfile\", \"iam:TagOpenIDConnectProvider\", \"iam:TagInstanceProfile\", \"iam:TagPolicy\", \"iam:TagRole\", \"iam:UpdateAssumeRolePolicy\", \"kms:CreateAlias\", \"kms:CreateKey\", \"kms:DeleteAlias\", \"kms:DescribeKey\", \"kms:EnableKeyRotation\", \"kms:GetKeyPolicy\", \"kms:GetKeyRotationStatus\", \"kms:ListAliases\", \"kms:ListResourceTags\", \"kms:PutKeyPolicy\", \"kms:ScheduleKeyDeletion\", \"kms:TagResource\", \"logs:CreateLogGroup\", \"logs:DeleteLogGroup\", \"logs:DescribeLogGroups\", \"logs:ListTagsLogGroup\", \"logs:PutRetentionPolicy\", \"s3:CreateBucket\", \"s3:DeleteBucket\", \"s3:DeleteBucketOwnershipControls\", \"s3:DeleteBucketPolicy\", \"s3:DeleteObject\", \"s3:GetAccelerateConfiguration\", \"s3:GetBucketAcl\", \"s3:GetBucketCORS\", \"s3:GetBucketLogging\", \"s3:GetBucketObjectLockConfiguration\", \"s3:GetBucketOwnershipControls\", \"s3:GetBucketPolicy\", \"s3:GetBucketPublicAccessBlock\", \"s3:GetBucketRequestPayment\", \"s3:GetBucketTagging\", \"s3:GetBucketVersioning\", \"s3:GetBucketWebsite\", \"s3:GetEncryptionConfiguration\", \"s3:GetLifecycleConfiguration\", \"s3:GetObject\", \"s3:GetObjectTagging\", \"s3:GetObjectVersion\", \"s3:GetReplicationConfiguration\", \"s3:ListAllMyBuckets\", \"s3:ListBucket\", \"s3:PutBucketAcl\", \"s3:PutBucketOwnershipControls\", \"s3:PutBucketPolicy\", \"s3:PutBucketPublicAccessBlock\", \"s3:PutBucketTagging\", \"s3:PutBucketVersioning\", \"s3:PutEncryptionConfiguration\", \"s3:PutObject\", \"secretsmanager:CreateSecret\", \"secretsmanager:DeleteSecret\", \"secretsmanager:DescribeSecret\", \"secretsmanager:GetResourcePolicy\", \"secretsmanager:GetSecretValue\", \"secretsmanager:PutSecretValue\", \"sqs:CreateQueue\", \"sqs:DeleteQueue\", \"sqs:GetQueueAttributes\", \"sqs:ListQueueTags\", \"sqs:SetQueueAttributes\", \"sqs:TagQueue\", \"sts:GetCallerIdentity\" ], \"Resource\": \"*\" } ] } . The source code can be also found here. ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html#iam-policies-and-roles",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html#iam-policies-and-roles"
  },"69": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "Teams",
    "content": "Introduction . EKS Blueprints provide support for onboarding and managing user teams and configuring their cluster access. They currently support two Team types: application_teams and platform_teams; application_teams represent teams managing workloads running in cluster namespaces and platform_teams represent platform administrators who have admin access (master’s group) to clusters. You can reference the aws-eks-teams module to create your own team implementations. Application Team . To create an application_team for your clusters, you will need to supply the following: . | A team name with the options to pass map of labels . | Map of K8s resource quotas . | Existing IAM entities (user/roles) . | A directory where you may optionally place any policy definitions and generic manifests for the team. | . These manifests will be applied by EKS Blueprints and will be outside of the team control. When the manifests are applied, namespaces are not checked. Therefore, you are responsible for namespace settings specified in IaC yaml files. Normally, resource kubernetes_manifest can only be used (terraform plan/apply...) after the cluster has been created and the cluster API can be accessed. To overcome this limitation, you can add/enable manifests_dir after you applied and created the EKS cluster first. Application Team Example . Below is a source code example for application_team. A source code example where those changes can be made for this Guidance can be found here. # EKS Application Teams application_teams = { # First Team team-blue = { \"labels\" = { \"appName\" = \"example1\", \"projectName\" = \"example1\", \"environment\" = \"example1\", \"domain\" = \"example1\", \"uuid\" = \"example1\", } \"quota\" = { \"requests.cpu\" = \"1000m\", \"requests.memory\" = \"4Gi\", \"limits.cpu\" = \"2000m\", \"limits.memory\" = \"8Gi\", \"pods\" = \"10\", \"secrets\" = \"10\", \"services\" = \"10\" } manifests_dir = \"./manifests\" # Below are examples of IAM users and roles users = [ \"arn:aws:iam::123456789012:user/blue-team-user\", \"arn:aws:iam::123456789012:role/blue-team-sso-iam-role\" ] } # Second Team team-red = { \"labels\" = { \"appName\" = \"example2\", \"projectName\" = \"example2\", } \"quota\" = { \"requests.cpu\" = \"2000m\", \"requests.memory\" = \"8Gi\", \"limits.cpu\" = \"4000m\", \"limits.memory\" = \"16Gi\", \"pods\" = \"20\", \"secrets\" = \"20\", \"services\" = \"20\" } manifests_dir = \"./manifests2\" users = [ \"arn:aws:iam::123456789012:role/other-sso-iam-role\" ] } } . EKS Blueprints will perform the following for every provided team option: . | Create a namespace . | Register Kubernetes resource quotas . | Register IAM users for cross-account access . | Create a shared role for cluster access. Alternatively, an existing role can be supplied. | Register provided users/roles in the aws-auth configmap for kubectl and console access to the cluster and namespace. | (Optionally) read all additional manifests (e.g., network policies, OPA policies, others) stored in a provided directory, and apply them. | . Platform Team . To create a Platform Team for your cluster, use a platform_teams configuration. You will need to supply a team name and all users/roles. Platform Team Example . Below is a source code example for Platform Team. An example of source code where those changes can be made for this Guidance can be found here: . platform_teams = { admin-team-name-example = { users = [ \"arn:aws:iam::123456789012:user/admin-user\", \"arn:aws:iam::123456789012:role/org-admin-role\"] } } . Platform Team performs the following: . | Registers IAM users for admin level access to the EKS cluster (kubectl and console). | Registers an existing role (or creates a new role) for cluster access with trust relationship with the provided/created role. | . Cluster Access (kubectl) . The Terraform script execution output will contain the IAM roles for every application (application_teams_iam_role_arn) or platform team (platform_teams_iam_role_arn). To update K8s cluster kubeconfig file contents, run the following command: . aws eks update-kubeconfig --name ${eks_cluster_id} --region ${AWS_REGION} --role-arn ${TEAM_ROLE_ARN} . Make sure to replace the ${eks_cluster_id},${AWS_REGION}and ${TEAM_ROLE_ARN} with the actual values. ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html#teams-1",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html#teams-1"
  },"70": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "Kubernetes Add-ons Modules",
    "content": "The kubernetes-add-ons modules within EKS Blueprints allows the user to configure the add-ons they would like deployed into their EKS clusters via simple true/false flags in the Terraform code. The framework currently provides support for add-ons contained in the ‘kubernetes-addons’ source code repository folder. Add-on Management . The framework provides two approaches to managing add-on configuration for EKS clusters. They are: . | Via Terraform by leveraging the Terraform Helm provider. | Via GitOps process with ArgoCD (used in this Guidance) . | . Terraform Helm Provider . The default method for managing an add-on configuration is through Terraform. By default, each individual add-on module will perform the following: . | Create any AWS resources needed to support add-on functionality. | Deploy a Helm chart into the user’s EKS cluster by leveraging the Terraform Helm provider. | . In order to deploy an add-on with default configuration, the user enables it through Terraform property variables as shown below. An example of source code where those changes can be made for this Guidance can be found here. module \"eks_blueprints_kubernetes_addons\" { source =\"github.com/aws-solutions-library-samples/guidance-for-automated-provisioning-of-amazon-elastic-kubernetes-service-using-terraform/modules/kubernetes-addons\" cluster_id = &lt;EKS-CLUSTER-ID&gt; # EKS Addons enable_amazon_eks_aws_ebs_csi_driver = true enable_amazon_eks_coredns = true enable_amazon_eks_kube_proxy = true enable_amazon_eks_vpc_cni = true # K8s Add-ons enable_argocd = true enable_aws_for_fluentbit = true enable_aws_load_balancer_controller = true enable_cluster_autoscaler = true enable_metrics_server = true } . To customize the behavior of the Helm charts that are ultimately deployed, the user can supply a custom Helm configuration. The following demonstrates how to use the configuration, including a dedicated values.yaml parameter customization file, to provide values for the Helm template. An example of a source code where these changes can be made for this Guidance can be found here. enable_metrics_server = true metrics_server_helm_config = { name = \"metrics-server\" repository = \"https://kubernetes-sigs.github.io/metrics-server/\" chart = \"metrics-server\" version = \"3.8.1\" namespace = \"kube-system\" timeout = \"1200\" # (Optional) Example to pass values.yaml from your local repo values = [templatefile(\"${path.module}/values.yaml\", { operating_system = \"linux\" })] } . Each add-on module is configured to fetch Helm Charts from public Helm repositories and Docker images from Docker Hub/Public Amazon Elastic Container Registry (Amazon ECR) repositories. This requires an outbound Internet connection from EKS Clusters. Core EKS Add-ons . Amazon EKS add-ons provide installation and management of a curated set of features for EKS clusters. All EKS add-ons include the latest security patches, bug fixes, and are validated to work with EKS. Amazon EKS add-ons allow you to consistently ensure that your EKS clusters are secure and stable and reduce the amount of work that you need to do in order to install, configure, and update add-ons. EKS currently provides support for the following managed add-ons. | Name | Description | . | Amazon VPC CNI | Native VPC networking for Kubernetes pods. | . | CoreDNS | A flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. | . | kube-proxy | Enables network communication to your pods. | . | Amazon EBS CSI | Manage the Amazon EBS CSI driver as an Amazon EKS add-on. | . EKS managed add-ons can be enabled and configured as shown in the following source code: . # EKS Addons enable_amazon_eks_vpc_cni = true # enable VPC CNI, default is false #Optional amazon_eks_vpc_cni_config = { addon_name = \"vpc-cni\" addon_version = \"v1.10.1-eksbuild.1\" service_account = \"aws-node\" resolve_conflicts = \"OVERWRITE\" namespace = \"kube-system\" additional_iam_policies = [] service_account_role_arn = \"\" tags = {} } . enable_amazon_eks_coredns = true # enable EKS CoreDNS, default is false #Optional amazon_eks_coredns_config = { addon_name = \"coredns\" addon_version = \"v1.8.4-eksbuild.1\" service_account = \"coredns\" resolve_conflicts = \"OVERWRITE\" namespace = \"kube-system\" service_account_role_arn = \"\" additional_iam_policies = [] tags = {} } enable_amazon_eks_kube_proxy = true # enable EKS Kube_proxy, default is false #Optional amazon_eks_kube_proxy_config = { addon_name = \"kube-proxy\" addon_version = \"v1.21.2-eksbuild.2\" service_account = \"kube-proxy\" resolve_conflicts = \"OVERWRITE\" namespace = \"kube-system\" additional_iam_policies = [] service_account_role_arn = \"\" tags = {} } enable_amazon_eks_aws_ebs_csi_driver = true # enable EBS CSI driver, default is false #Optional amazon_eks_aws_ebs_csi_driver_config = { addon_name = \"aws-ebs-csi-driver\" addon_version = \"v1.4.0-eksbuild.preview\" service_account = \"ebs-csi-controller-sa\" resolve_conflicts = \"OVERWRITE\" namespace = \"kube-system\" additional_iam_policies = [] service_account_role_arn = \"\" tags = {} } . An example of the source code where similar changes can be made for this Guidance can be found here. ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html#kubernetes-add-ons-modules",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html#kubernetes-add-ons-modules"
  },"71": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "Updating Managed Add-ons",
    "content": "EKS will not modify any of your Kubernetes add-ons when you update a cluster to a newer Kubernetes version. As a result, it is important to upgrade EKS add-ons each time you upgrade your EKS clusters. Additional information on updating an EKS cluster can be found in the EKS documentation. GitOps with ArgoCD . To indicate that you would like to manage EKS add-ons through an Argo CD GitOps tool, you should do the following: . | Enable the Argo CD add-on by setting the enable_argocd flag to true in the Terraform code. | Specify that ArgoCD should be responsible for deploying add-ons by setting the argocd_manage_add_ons flag to true. This will prevent the individual Terraform add-on modules from deploying through Helm charts. | Pass Application configuration for the add-ons repository through the argocd_applications property. | . Note that the add_on_application flag in an Application configuration must be set to true. enable_argocd = true argocd_manage_add_ons = true argocd_applications = { infra = { namespace = \"argocd\" path = \"&lt;path&gt;\" repo_url = \"&lt;repo_url&gt;\" values = {} add_on_application = true # Indicates the root add-on application. } } . An example of source code where similar changes can be made can be found here. GitOps Bridge . When managing add-ons through Argo CD, certain AWS resources may still need to be created through Terraform in order to support the add-on functionality (for example: IAM Roles and Services Accounts). Certain resource values will also need to be passed from Terraform to Argo CD through the Argo CD Application resource’s values map. We refer to this concept as GitOps Bridge. To ensure that the AWS resources needed for add-on functionality are created, you need to indicate a Terraform configuration where add-ons will be managed through Argo CD. To do so, enable selected add-ons through their boolean properties, as shown below: . enable_metrics_server = true # Deploys Metrics Server Addon enable_cluster_autoscaler = true # Deploys Cluster Autoscaler Addon enable_prometheus = true # Deploys Prometheus Addon . An example of source code for the above settings where these changes can be made for this Guidance can be found here. This will indicate to each add-on module that it should create the necessary AWS resources and pass the relevant values to the Argo CD Application resource through the Application’s values map. Argo CD Add-on . Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes Application definitions, configurations, and environments should be declarative and version controlled and stored in a Git compatible repository. Application deployment and lifecycle management should be automated, auditable, and easy to understand. Usage . Argo CD can be deployed by enabling the add-on through a flag: enable_argocd = true . Admin Password . Argo CD has a built-in admin user that has full access to the Argo CD server. By default, Argo CD will create a password for the admin user. See the Argo CD documentation for additional details on managing users. Customizing Argo CD deployment Helm Chart . You can customize the Helm chart that deploys Argo CD using the configuration below. An example of source code where similar changes can be made for this Guidance can be found here. argocd_helm_config = { name = \"argo-cd\" chart = \"argo-cd\" repository = \"https://argoproj.github.io/argo-helm\" version = \"&lt;chart_version&gt;\" namespace = \"argocd\" timeout = \"1200\" create_namespace = true values = [templatefile(\"${path.module}/argocd-values.yaml\", {})] } . Bootstrapping . The framework provides an approach to bootstrapping workloads and/or additional add-ons by leveraging the Argo CD App of Apps pattern. The code example below demonstrates how you can supply information for a repository in order to bootstrap multiple workloads in a new EKS cluster. The example leverages a sample App of Apps repository. An example of source code where these changes can be made for this Guidance can be found here. argocd_applications = { addons = { path = \"chart\" repo_url = \"https://github.com/aws-samples/eks-blueprints-add-ons.git\" add_on_application = true # Indicates the root add-on application. } } . Managing EKS Add-ons . A common operational pattern for EKS customers is to leverage Infrastructure as Code (IaC) to provision EKS clusters (in addition to other AWS resources), and Argo CD to manage cluster add-ons. This can present a challenge when add-ons managed by Argo CD depend on AWS resource values that are created through a Terraform execution, such as IAM Amazon Resource Names (ARN) for an add-on that leverages IAM roles for service accounts (IRSA). The framework provides an approach to bridge the gap between Terraform and Argo CD by leveraging the Argo CD App of Apps pattern. To indicate that Argo CD should manage cluster add-ons (applying add-on Helm charts to a cluster), you can set the argocd_manage_add_ons property to true. When this flag is set, the framework will still provision all AWS resources necessary to support add-on functionality, but it will not apply Helm charts directly through the Terraform Helm provider. Next, identify which Argo CD Application will serve as the add-on configuration repository by setting the add_on_application flag to true. When this flag is set, the framework will aggregate AWS resource values that are needed for each add-on into an object. It will then pass that object to Argo CD through the values map of the Application resource. See here for all the values objects that are passed to the Argo CD add-ons Application. Sample configuration is below: . enable_argocd = true argocd_manage_add_ons = true argocd_applications = { addons = { path = \"chart\" repo_url = \"https://github.com/aws-samples/eks-blueprints-add-ons.git\" # public repository add_on_application = true # Indicates the root add-on application. } } . The source code for the above examples where these changes can be made for this Guidance can be found here. Private Repositories . In order to leverage Argo CD with private Git repositories, please see related documentation on GitHub. Complete Example . A complete example that demonstrates the configuration of Argo CD for the management of cluster add-ons can be found in this documentation. Apache Spark Kubernetes Operator Add-on . The spark-on-k8s-operator allows Apache Spark applications to be defined in a declarative manner and supports one-time Apache Spark applications with SparkApplication and cron-scheduled applications with ScheduledSparkApplication. Apache Spark aims to make specifying and running Apache Spark applications as easy and idiomatic as running other workloads on Kubernetes. It uses Kubernetes custom resources for specifying, running, and surfacing the status of Apache Spark applications. For a complete reference of the custom resource definitions, please refer to the API Definition. For details on its design, please refer to the design documentation. It requires Apache Spark 2.3 and above that supports Kubernetes as a native scheduler backend. Usage . Apache Spark K8S Operator can be deployed by enabling the add-on with the following settings: . Basic example: . enable_spark_k8s_operator = true . Advanced example: . enable_spark_k8s_operator = true # Optional Map value # NOTE: This block requires passing the helm values.yaml spark_k8s_operator_helm_config = { name = \"spark-operator\" chart = \"spark-operator\" repository = \"https://googlecloudplatform.github.io/spark-on-k8s-operator\" version = \"1.1.19\" namespace = \"spark-k8s-operator\" timeout = \"1200\" create_namespace = true values = [templatefile(\"${path.module}/values.yaml\", {})] } . A Spark Operator example source code can be found here. Apache Spark History Server Add-on . Apache Spark Web UI can be enabled by the Apache Spark History Server Add-on. This add-on deploys Apache Spark History Server and fetches the Apache Spark Event logs stored in Amazon Simple Storage Service (Amazon S3). The Apache Spark Web UI can be exposed through Ingress and Load Balancer with values yaml. Alternatively, you can port-forward on spark-history-server service, for example: . kubectl port-forward services/spark-history-server 18085:80 -n spark-history-server . Usage . Apache Spark History Server can be deployed by enabling the add-on through the following example source codes: . Basic Example . enable_spark_history_server = true spark_history_server_s3a_path = \"s3a://&lt;ENTER_S3_BUCKET_NAME&gt;/&lt;PREFIX_FOR_SPARK_EVENT_LOGS&gt;/\" . Advanced Example . enable_spark_history_server = true # IAM policy used by IRSA role. It's recommended to create a dedicated IAM policy to access your s3 bucket spark_history_server_irsa_policies = [\"&lt;IRSA_POLICY_ARN&gt;\"] # NOTE: This block requires passing the helm values.yaml # spark_history_server_s3a_path won't be used when you pass custom `values.yaml`. s3a path is passed via `sparkHistoryOpts` in `values.yaml` spark_history_server_helm_config = { name = \"spark-history-server\" chart = \"spark-history-server\" repository = \"https://hyper-mesh.github.io/spark-history-server\" version = \"1.0.0\" namespace = \"spark-history-server\" timeout = \"300\" values = [ &lt;&lt;-EOT serviceAccount: create: false # Enter S3 bucket with Spark Event logs location. # Ensure IRSA roles has permissions to read the files for the given S3 bucket sparkHistoryOpts: \"-Dspark.history.fs.logDirectory=s3a://&lt;ENTER_S3_BUCKET_NAME&gt;/&lt;PREFIX_FOR_SPARK_EVENT_LOGS&gt;/\" # Update spark conf according to your needs sparkConf: |- spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.WebIdentityTokenCredentialsProvider spark.history.fs.eventLog.rolling.maxFilesToRetain=5 spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem spark.eventLog.enabled=true spark.history.ui.port=18080 resources: limits: cpu: 200m memory: 2G requests: cpu: 100m memory: 1G EOT ] } . An example of the source code using the configuration settings above for Apache Spark History server can be found here. ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html#updating-managed-add-ons",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html#updating-managed-add-ons"
  },"72": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "Running new VPC EKS Terraform Blueprint",
    "content": "Prerequisites . Ensure that you have installed the following tools in your Mac or Windows Laptop before you start working with this module; run Terraform Plan and select Apply: . | AWS CLI . | Kubectl . | Terraform . | . Minimum IAM Policy . The policy resource is set as * to allow all resources. This is not a recommended practice. Minimum IAM policy is documented above and can be found here – it is automatically bootstrapped to EKS clusters provisioned by blueprints. Deployment . To provision EKS Cluster with new VPC managed components and application workloads, you should follow these steps: . | Clone the repo using the command below: | . git clone https://github.com/aws-solutions-library-samples/guidance-for-automated-provisioning-of-amazon-elastic-kubernetes-service-using-terraform.git . | Initialize Terraform: | . cd examples/eks-cluster-with-new-vpc/ terraform init . | Verify resources to be created by this blueprint: | . terraform plan . | Apply staged resources to AWS environment: | . terraform apply . (Enter yes at the command prompt to apply or use terraform apply -auto-approve to bypass that question) . Validation . The following command will update the kubeconfig on the user’s machine and allow it to interact with the EKS Cluster using the kubectl client to validate the deployment. | Run update-kubeconfig command which will update the local K8s configuration file by adding a context corresponding to newly created cluster: | . aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt; . | List all the worked nodes by running the command below: | . kubectl get nodes . | List all the pods running in kube-system namespace: | . kubectl get pods -n kube-system . Cleanup . To clean up your environment, destroy the Terraform modules in reverse order of their deployment. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC: . terraform destroy -target=\"module.eks_blueprints_kubernetes_addons\" -auto-approve terraform destroy -target=\"module.eks_blueprints\" -auto-approve terraform destroy -target=\"module.vpc\" -auto-approve . Finally, destroy any additional resources that are not in the above modules: . terraform destroy -auto-approve . ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html#running-new-vpc-eks-terraform-blueprint",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html#running-new-vpc-eks-terraform-blueprint"
  },"73": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "Running Argo CD EKS Terraform Blueprint",
    "content": "Prerequisites . Ensure that the following tools are installed locally: . | aws cli . | kubectl . | terraform . | . Minimum IAM Policy . The policy resource is set as * to allow all resources, this is not a recommended practice. Minimum IAM policy is documented above and can be found in the project repository here – it is automatically bootstrapped to EKS clusters provisioned by blueprints . Deployment . To provision EKS Cluster with Argo CD managed components and application workloads, you should follow these steps: . | Clone EKS blueprints repository locally: | . git clone https://github.com/aws-solutions-library-samples/guidance-for-automated-provisioning-of-amazon-elastic-kubernetes-service-using-terraform.git . If any customization of parameters is needed, it can be performed by adjusting/adding parameter values in the main.tf source code file according to the documentation above. This includes basic AWS configuration: cluster name, deployment Region, VPC Classless Inter-Domain Routing (CIDR), and Availability Zones (AZs). Additionally, EKS Kubernetes parameters: K8s API version, managed node group instance types and ranges, tags, and add-ons can also be customized. Below is an example of such parameters with minor modifications highlighted in # comments: . locals { # Optional customization: add unique postfix to cluster_name to avoid duplication or 'global' # role names derived from a cluster name name1 = basename(path.cwd) name = \"${local.name1}-test1\" # Customization: us-west-2 region has sufficient resources region = \"us-west-2\" # Customization: VPC parameters vpc_cidr = \"10.0.0.0/16\" azs = slice(data.aws_availability_zones.available.names, 0, 3) tags = { Blueprint = local.name GithubRepo =\"github.com/aws-solutions-library-samples/guidance-for-automated-provisioning-of-amazon-elastic-kubernetes-service-using-terraform\" #use forked or upstream repo } } #--------------------------------------------------------------- # EKS Blueprints #--------------------------------------------------------------- module \"eks_blueprints\" { source = \"../../..\" # location of 'parent module' in the source code hierarchy cluster_name = local.name # Customization of K8s API version # K8s API version for data and compute planes cluster_version = \"1.24\" vpc_id = module.vpc.vpc_id private_subnet_ids = module.vpc.private_subnets managed_node_groups = { mg_5 = { node_group_name = \"managed-ondemand\" instance_types = [\"m5.large\"] subnet_ids = module.vpc.private_subnets desired_size = 3 max_size = 5 min_size = 2 } } tags = local.tags } # EKS Add-ons configuration module \"eks_blueprints_kubernetes_addons\" { # location of 'parent module' in the source code hierarchy source = \"../../../modules/kubernetes-addons\" eks_cluster_id = module.eks_blueprints.eks_cluster_id eks_cluster_endpoint = module.eks_blueprints.eks_cluster_endpoint eks_oidc_provider = module.eks_blueprints.oidc_provider eks_cluster_version = module.eks_blueprints.eks_cluster_version # Enable installation of argo-cd add-on via HELM enable_argocd = true # Set default ArgoCD Admin Password using SecretsManager with Helm Chart set_sensitive values. argocd_helm_config = { set_sensitive = [ { name = \"configs.secret.argocdServerAdminPassword\" value = bcrypt_hash.argo.id } ] } } . | Initialize Terraform from the directory where source code is located: | . cd examples/gitops/argocd terraform init . | Verify resources to be created by this blueprint: | . terraform plan . | Apply staged Terraform resources to AWS environment: | . terraform apply . (Enter yes at command prompt to apply or use terraform apply-auto-approve to bypass that question). Validation . The following command will update the kubeconfig Kubernetes context configuration file on user’s machine and allow to interact with the EKS Cluster using kubectl client to validate the deployment. | Run update-kubeconfig command which will update the local K8s configuration file by adding a context corresponding to newly created cluster: | . aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt; . | List out the pods running currently in the argocd and other K8s namespaces: | . (Names of pods in different K8s namespaces reflect corresponding deployed EKS add-ons) . | You can test access to ArgoCD Server UI by running the following commands: | . kubectl port-forward svc/argo-cd-argocd-server 8080:443 -n argocd . Then, open browser and navigate to http://localhost:8080/. The login Username should be admin. Also, permanent Argo CD admin UI service URL can be obtained by examining services running in the argocd namespace by running the following command: . and changing type of ‘argo-cd-argocd-server’ to LoadBalancer: . kubectl patch svc argo-cd-argocd-server -n argocd -p '{\"spec\":{\"type\": \"LoadBalancer\"}}' . which can be confirmed by running the following command: . kubectl get svc -n argocd | grep argo-cd-argocd-server argo-cd-argocd-server LoadBalancer 172.20.251.159 XXXXXXXXXXXXXXXXXXXXXXXXX.us-west-2.elb.amazonaws.com 80:31120/TCP,443:30200/TCP 12d . Argo CD endpoint URL is listed as an attribute of argo-cd-argocd-server and should appear like shown in the screenshot below: . Figure 1 Argo CD endpoint URL . The Argo CD admin password will be the generated password by the random_password resource, stored in AWS Secrets Manager. You can easily retrieve the password by running the following command: . aws secretsmanager get-secret-value --secret-id &lt;SECRET_NAME&gt; --region &lt;REGION&gt; . Replace &lt; SECRET_NAME &gt; with the name of the secret name (by default it should be 'argocd') and make sure to replace &lt; REGION &gt; with the Region where the EKS Blueprint is deployed. An example of retrieving the secret from the SecretString value is shown below: . Run this command: . aws secretsmanager get-secret-value --secret-id argocd --region us-west-2 . which should return an output like: . { \"ARN\": \"arn:aws:secretsmanager:us-west-2:XXXXXXXXX:secret:argocd-7Mz04Y\", \"Name\": \"argocd\", \"VersionId\": \"88E3BA7E-7A2E-4129-A87C-1794E1CAD627\", \"SecretString\": \"XXXXXXXXXXXXXXX\", \"VersionStages\": [ \"AWSCURRENT\" ], \"CreatedDate\": \"2022-12-19T12:05:12.458000-08:00\" } . Argo CD EKS blueprint comes with a number of pre-defined Argo CD applications configured and mapped for deployment into the local EKS cluster. Please refer to Argo CD documentation for details on how to work with applications using that GitOps platform. An example of such applications in Argo CD UI is shown below: . Figure 2 Displays how Argo CD applications are configured in an EKS cluster . Cleanup . To tear down and remove all the resources created in this example, you need to ensure that all Argo CD applications are properly deleted from the cluster. This can be achieved in multiple ways: . | Disabling the argocd_applications configuration in the code locally and running terraform apply command again . | Deleting the apps using argocd cli or UI . | . The example below shows how the Argo CD applications are configured and managed by that platform in an EKS cluster that can be deleted by selecting the Delete buttons: . Figure 3 Displays how Argo CD applications can be deleted from the EKS cluster . | Deleting the apps using kubectl following ArgoCD guidance | . After application-level cleanup, you can delete the Terraform provisioned resources as follows: . terraform destroy -target=module.eks_blueprints_kubernetes_addons -auto-approve terraform destroy -target=module.eks_blueprints -auto-approve # Finally destroy all additional resources not provisioned by above TF modules terraform destroy -auto-approve . ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html#running-argo-cd-eks-terraform-blueprint",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html#running-argo-cd-eks-terraform-blueprint"
  },"74": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "Running Apache Spark on EKS Terraform Blueprint",
    "content": "Prerequisites . Ensure that the following tools are installed locally: . | aws cli . | kubectl . | terraform . | . Minimum IAM Policy . The policy resource is set as * to allow all resources, this is not a recommended practice. Minimum IAM policy is documented above and can be found here – it is automatically bootstrapped to EKS clusters provisioned by blueprint . Deployment . To provision the EKS Cluster with Apache Spark and Apache Spark History server components and application workloads, follow these steps: . | Clone EKS blueprints data on EKS source code repository (where Apache Spark EKS add-on blueprint is hosted) locally: | . git clone https://github.com/aws-solutions-library-samples/guidance-for-automated-provisioning-of-data-apps-amazon-elastic-kubernetes-service-using-terraform.git . | Initialize Terraform: | . cd analytics/terraform/spark-k8s-operator terraform init . | Verify AWS and EKS K8s resources to be created by this blueprint | . terraform plan . | Apply staged resources to AWS environment | . terraform apply . (Enter yes at command prompt to apply) . Validation . The following command will update the kubeconfig client configuration file on the user’s machine and allow it to interact with the EKS Cluster using kubectl to validate the deployment. Run update-kubeconfig command: . aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt; . Running Test Apache Spark Job from a Driver pod . | First, you need to create a Kubernetes  Service account that will have a role that allows driver pods to create pods and services under the default RBAC policiesas explained in the Documentation | . An example of such commands for ‘default’ namespace is shown below: . kubectl create serviceaccount spark kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default . You can verify whether necessary permissions were granted to the created Service account by running the following command: . kubectl auth can-i create pods -n default --as system:serviceaccount:default:spark . which should produce an output:yes . | Now we need to determine a URL of Kubernetes master node to use later for Spark job submission. The command below returns a URL of master node: | . kubectl cluster-info . that should produce an output including entry like (partially masked for privacy): . Kubernetes control plane is running at https://XXXXXXXXXXXXXXXXXXXXXXXXX.gr7.us-east-2.eks.amazonaws.com . | With API endpoint determined and service accounts configured, we can run a test Spark job using an instance of the apache/spark image. The command below will create a pod instance from which we can launch test jobs. | . Creating a pod to deploy a cluster in client mode, Apache Spark applications is sometimes referred to as deploying a “jump,” “edge,” or “bastian” pod. It’s a variant of deploying a Bastion Host, where high-value or sensitive resources run in one environment and the bastion serves as a proxy. An example command below creates a jump pod using the Spark driver container based on the ‘apache/spark’ container image that is using the previously created ‘spark’ service account: . kubectl run spark-test-docker --overrides='{\"spec\" {\"serviceAccount\":\"spark\"}}' -it --image=apache/spark -- /bin/bash . The kubectl command creates a deployment and driver pod, and will drop into its bash shell when the pod becomes available. The remainder of the commands in this section will use this shell. | Apache’s Spark image distribution contains an example program that can be used to calculate the number Pi. Since it works without any input, it is useful for running tests. We can check that the Spark cluster is configured correctly by submitting this application to the cluster. Apache Spark commands are submitted using spark-submit utility. In the container images created above, spark-submit can be found in the /opt/spark/bin folder. | . spark-submit command options can be quite complicated, please see documentation for all option values . For that reason, let’s configure a set of environment variables with important runtime parameters. While we define these directly here, in Spark applications they can be injected from a ConfigMap or as part of the pod/deployment manifest: . # Define environment variables with K8s namespace, accounts and auth parameters export SPARK_NAMESPACE=default export SA=spark export K8S_CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt export K8S_TOKEN=/var/run/secrets/kubernetes.io/serviceaccount/token # Docker runtime image and driver pod name export DOCKER_IMAGE=apache/spark export SPARK_DRIVER_NAME=spark-test-pi . | The example command below submits Spark job to the cluster referenced by –master option. It will deploy in “cluster” mode and reference the spark-examples JAR included with the specified container image. We tell Apache Spark which program within the JAR to execute by defining the –class option. In this case, we wish to run the org.apache.spark.examples.SparkPi Java class: | . # spark-submit command /opt/spark/bin/spark-submit --name $SPARK_DRIVER_NAME \\ --master k8s://https://XXXXXXXXXXXXXXXXXXX.gr7.us-east-2.eks.amazonaws.com:443 \\ --deploy-mode cluster \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.kubernetes.driver.pod.name=$SPARK_DRIVER_NAME \\ --conf spark.kubernetes.authenticate.subdmission.caCertFile=$K8S_CACERT \\ --conf spark.kubernetes.authenticate.submission.oauthTokenFile=$K8S_TOKEN \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=$SA \\ --conf spark.kubernetes.namespace=$SPARK_NAMESPACE \\ --conf spark.executor.instances=2 \\ --conf spark.kubernetes.container.image=$DOCKER_IMAGE \\ --conf spark.kubernetes.container.image.pullPolicy=Always \\ local:///opt/spark/examples/jars/spark-examples_2.12-3.3.2.jar 100000 . NOTES: . | The Kubernetes API is available within the cluster within the default namespace and should be used in the master option. If Kubernetes DNS is available, API can be accessed via namespace URL (https://kubernetes.default:443 to be used the example above). The k8s://https://form of the URL - this is not a typo, the k8s:// prefix is how Spark determines the provider type. | The local:// path of the jar above references location of the file in the executor container image, not on the jump pod that we used to submit the job. Both the driver and executors rely on that path in order to find the program implementation and start Spark tasks. | . | If you watch the pod list while the job is running using kubectl get pods, you should see a “driver” pod initialized with the name provided in the SPARK_DRIVER_NAME option. It will launch executor pods that actually perform the work while staying in “Running” status and get deleted upon job completion . When the Spark Job finishes running, the driver pod changes into “Completed” status. You can review status of these pods and retrieve Spark job results from the pod logs using command like: | . # monitor driver and executor pods initialized by Spark job submission: kubectl get pods -n default -w . Which should give an output like: . You can also retrieve results of Spark Job execution running command like: . # Retrieve the results of the program from the cluster kubectl logs -f $SPARK_DRIVER_NAME . Toward the end of the “driver” pod log, you should see a result line similar to example shown below: . 23/02/21 02:05:31 INFO TaskSetManager: Finished task 99998.0 in stage 0.0 (TID 99998) in 27 ms on 10.1.142.42 (executor 2) (99999/100000) 23/02/21 02:05:31 INFO TaskSetManager: Finished task 99999.0 in stage 0.0 (TID 99999) in 25 ms on 10.1.46.210 (executor 1) (100000/100000) 23/02/21 02:05:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 23/02/21 02:05:31 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 153.114 s 23/02/21 02:05:31 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job 23/02/21 02:05:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished 23/02/21 02:05:31 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 153.531012 s **Pi is roughly 3.1416237160019795** . Pre-requisites for running Kubernetes Spark Application examples . There are examples for Apache Spark Applications configurations that can be launched directly as K8s objects located here. Every yaml file in the examples folder has pre-requisite requirements mentioned at the top of the file in the commented section. See an example for this configuration here. This example requires the following prerequisites before executing the jobs: . | Ensure spark-team-a name space exists . | replace &lt; ENTER_YOUR_BUCKET &gt; with your bucket name . | Ensure you run \"analytics/spark-k8s-operator/spark-samples/tpcds-benchmark-data-generation-1t.yaml\" to generate the INPUT data in the Amazon S3 bucket and update INPUT argument (\"s3a://&lt;ENTER_YOUR_BUCKET&gt;/TPCDS-TEST-1T/catalog_sales/\") path in the below yaml . | . Please follow the instructions (for example: create an Amazon S3 bucket with necessary permissions and specify its name in the section of the application configuration file) in order to run the examples using kubectl apply -f &lt;descriptor&gt;.yaml syntax . Cleanup . To clean up your environment, first delete all Apache Spark applications and then destroy the Terraform modules in reverse order of their deployment. Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC as follows: . terraform destroy -target=\"module.eks_blueprints_kubernetes_addons\" -auto-approve terraform destroy -target=\"module.eks_blueprints\" -auto-approve terraform destroy -target=\"module.vpc\" -auto-approve . Finally, destroy any additional resources that are not in the above modules: . terraform destroy -auto-approve . ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html#running-apache-spark-on-eks-terraform-blueprint",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html#running-apache-spark-on-eks-terraform-blueprint"
  },"75": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "Support and Troubleshooting",
    "content": "Support &amp; Feedback . EKS Terraform Blueprints is an Open-Source project maintained by AWS Solution Architects. It is not part of an AWS Service and support is provided with best-effort by AWS Solution Architects and the EKS Blueprints community. To post feedback, submit feature ideas or report bugs, you can use the Issues section of the project GitHub repository (and the link on the Guidance). If you are interested in contributing to EKS Blueprints, you can follow the Contribution guide. Version Requirements . This version of EKS Blueprint requires the following version of core tools . | Name | Version | . | terraform | &gt;= 1.0.0 | . | aws | &gt;= 3.72 | . | helm | &gt;= 2.4.1 | . | http | 2.4.1 | . | kubectl | &gt;= 1.14 | . | kubernetes | &gt;= 2.10 | . | local | &gt;= 2.1 | . and Service providers: . | Name | Version | . | aws | &gt;= 3.72 | . | http | 2.4.1 | . | kubernetes | &gt;= 2.10 | . Customization . EKS Terraform Blueprints are highly customizable. Versions of core components from Kubernetes API version, to AWS services parameters such as: target AWS region, VPC CIDR range, name, AMI instance type, and range of node pool for compute plane can all be customized. This Guidance has been successfully tested with values of those parameters used in the sample code in the GitHub repository for “New VPC” and “Argo CD Add-on” and this repository for Spark Add-on use cases, with an exception of local.region parameter that was set to the AWS Region with the most available resources. You may specify different values for customization parameters. Be aware of your AWS environment topology, resources, and use values that make sense for that environment. A full list of Terraform input and output parameters that are used for Blueprint customization can be found in the Project repository. Troubleshooting . Terraform is a command-line tool that generates extensive logs when commands are executed. If Blueprint deployment fails for some reason (usually during execution of terraform apply command, see above for details) you can find detailed error messages in the log displayed on your console such as shown below: . Error: failed creating IAM Role (eks-cluster-with-new-vpc-aws-node-irsa): EntityAlreadyExists: Role with name eks-cluster-with-new-vpc-aws-node-irsa already exists. │ status code: 409, request id: c29d95fb-b206-4121-bdda-953be12209ef │ with module.eks_blueprints_kubernetes_addons.module.aws_vpc_cni[0].module.irsa_addon[0].aws_iam_role.irsa[0], │ on ../../modules/irsa/main.tf line 35, in resource \"aws_iam_role\" \"irsa\": │ 35: resource \"aws_iam_role\" \"irsa\" { │ Error: unexpected EKS Add-On (eks-cluster-with-new-vpc:coredns) state returned during creation: timeout while waiting for state to become 'ACTIVE' (last state: 'DEGRADED', timeout: 20m0s) │ [WARNING] Running terraform apply again will remove the kubernetes add-on and attempt to create it again effectively purging previous add-on configuration │ with module.eks_blueprints_kubernetes_addons.module.aws_coredns[0].aws_eks_addon.coredns[0], │ on ../../modules/kubernetes-addons/aws-coredns/main.tf line 12, in resource \"aws_eks_addon\" \"coredns\": │ 12: resource \"aws_eks_addon\" \"coredns\" { │Error: error creating IAM Policy eks-cluster-with-new-vpc-aws-ebs-csi-driver-irsa: EntityAlreadyExists: A policy called eks-cluster-with-new-vpc-aws-ebs-csi-driver-irsa already exists. Duplicate names are not || allowed. │ status code: 409, request id: a1d15c7f-24fe-4e1f-966a-c0b97191d5a3 │ with module.eks_blueprints_kubernetes_addons.module.aws_ebs_csi_driver[0].aws_iam_policy.aws_ebs_csi_driver[0], │ on ../../modules/kubernetes-addons/aws-ebs-csi-driver/main.tf line 91, in resource \"aws_iam_policy\" \"aws_ebs_csi_driver\": │ 91: resource \"aws_iam_policy\" \"aws_ebs_csi_driver\" { . Log files usually point to a Terraform module name and line of its script where error occurred, allowing you to examine the specified parameters and make necessary changes, then try deployment again. In the example above, an EKS cluster named eks-cluster-with-new-vpc was deployed to another Region, with generated IAM roles and policies objects based on that cluster name that are “globally” scoped. That caused the errors above. The errors were fixed by changing the value assigned to the parameter cluster_name = \"${local.cluster_name1}-test1\" in the main.tf locals module. This ensured that the cluster, the related role, and the policy names would be unique. ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html#support-and-troubleshooting",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html#support-and-troubleshooting"
  },"76": {
    "doc": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "title": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide",
    "content": ". ",
    "url": "/automated-provisioning-of-amazon-eks-using-terraform.html",
    
    "relUrl": "/automated-provisioning-of-amazon-eks-using-terraform.html"
  },"77": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Overview",
    "content": "This Guidance demonstrates ingesting Google Analytics 4 data into AWS and activating marketing channels with customized audience profiles for marketing analytics. It explores each stage of building this solution and covers data ingestion, transformation, data cataloging, and analysis to prepare it for consumption using AWS native services. ",
    "url": "/connecting-data-from-google-analytics.html#overview",
    
    "relUrl": "/connecting-data-from-google-analytics.html#overview"
  },"78": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Cost and licenses",
    "content": "No licenses are required to deploy this solution. There is no cost to use this solution, but you will be billed for any AWS services or resources that this solution deploys. You’ll need the no-cost Google BigQuery Connector for AWS Glue from AWS Marketplace to connect AWS Glue jobs to BigQuery. ",
    "url": "/connecting-data-from-google-analytics.html#cost-and-licenses",
    
    "relUrl": "/connecting-data-from-google-analytics.html#cost-and-licenses"
  },"79": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Architecture Overview",
    "content": "This sample solution will help customers securely ingest and analyze first-party and third-party data. Marketers can further use this information to activate marketing channels with customized segments. Figure 1 – Architecture overview of a sample solution . | Amazon EventBridge scheduled rule starts and runs the AWS Step Functions workflow. | Google Cloud Big Query access credentials are securely stored in AWS Secrets Manager and encrypted with AWS Key Management Service (AWS KMS). | AWS Glue job will ingest data using the AWS Marketplace Google BigQuery Connector for AWS Glue. The connector simplifies the process of connecting AWS Glue jobs to extract data from BigQuery. This AWS Glue job will encrypt, normalize, and hash the data. | The output of the AWS Glue job will be written to the target Amazon Simple Storage Service (Amazon S3) bucket:prefix location in parquet format. The output file will be partitioned by date and encrypted with AWS KMS. | AWS Glue Crawler job is initiated to “refresh” the table definition and its associated meta-data in the AWS Glue Data Catalog. | The Data Consumer queries the data output with Amazon Athena. | . ",
    "url": "/connecting-data-from-google-analytics.html#architecture-overview",
    
    "relUrl": "/connecting-data-from-google-analytics.html#architecture-overview"
  },"80": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Security",
    "content": "When you build systems on AWS infrastructure, security responsibilities are shared between you and AWS. This shared model can reduce your operational burden as AWS operates, manages, and controls the components from the host operating system and virtualization layer down to the physical security of the facilities where the services operate. For more information about security on AWS, refer to AWS Cloud Security. Amazon S3 . Infrastructure components in the Guidance where user data flows through are encrypted using Server-Side Encryption (SSE). Multiple Amazon S3 buckets are created for this solution and they are encrypted using S3-SSE AES-256 to secure user data. AWS KMS . This AWS service is used to encrypt the data stored in Amazon Kendra and Amazon OpenSearch Service. In addition, AWS KMS is used to encrypt the data in transit through Amazon Simple Notification Service (Amazon SNS) and Amazon Simple Queue Service (Amazon SQS). Secrets Manager . Secrets Manager will encrypt secrets at rest using encryption keys you own and store in AWS KMS. ",
    "url": "/connecting-data-from-google-analytics.html#security",
    
    "relUrl": "/connecting-data-from-google-analytics.html#security"
  },"81": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Implementation steps",
    "content": "Follow the step-by-step instructions in this section to configure the solution into your account. Deploying the Project with AWS Cloud Development Kits . The project code uses the Python version of the AWS Cloud Development Kit (AWS CDK). To start the project code, please ensure that you have fulfilled the AWS CDK Prerequisites for Python. The project code requires that the AWS account is bootstrapped in order to allow the deployment of the AWS CDK stack. AWS CDK Deployment . navigate to project directory: . cd aws-glue-connector-ingestion-ga4-analytics . install and activate a Python Virtual Environment: . python3 -m venv .venv . source .venv/bin/activate . install dependent libraries: . python -m pip install -r requirements.txt . AWS CDK Context Parameters Configuration . Update the cdk.context.json . { \"job_name\": \"bigquery-analytics\", \"data_bucket_name\": \"your-s3-bucket-name\", \"dataset_id\": \"Big Query Dataset ID\", \"parent_project\": \"GCP Project ID\", \"connection_name\": \"bigquery\", \"filter\": \"\", \"job_script\": \"job-analytics-relationalize-flat.py\", \"schedule_daily_hour\": \"3\", \"glue_database\": \"gcp_analytics\", \"glue_table\": \"ga4_events\", \"timedelta_days\": \"1\" } . Context Parameter Summary . | job_name: name of the AWS Glue job . | data_bucket_name: bucket name for the data and AWS Glue job script . | dataset_id: BigQuery Dataset ID for the Google Analytics 4 export . | parent_project: GCP Project ID . | connection_name: AWS Glue Connection name . | filter: not currently used, however, used for query filtering . | job_script - job-analytics-relationalize-flat.py: this included AWS Glue script will pull yesterday’s data from BigQuery . | schedule_daily_hour - - default 3 AM: daily schedule hour of the job runs to get yesterday’s analytics data . | glue_database: AWS Glue database name . | glue_table: AWS Glue table name . | timedelta_days: Number of days back to pull events. 0 = today, 1 = yesterday . | . Bootstrap the account to set up AWS CDK deployments in the region . cdk bootstrap . Upon successful completion of cdk bootstrap, the project is ready to be deployed. cdk deploy . Exporting Data from Google Analytics 4 Properties to BigQuery . Follow the setup from Exporting Data from Google Analytics 4 Properties to BigQuery . Subscribe to the Google BigQuery Connector for AWS Glue . Follow the below steps for subscribing to the Google BigQuery Connector for AWS Glue in the AWS Marketplace . | Choose to Continue to Subscribe. | Review the terms and conditions, pricing, and other details. | Choose to Continue to Configuration. | For the Fulfillment option, choose the AWS Glue Version (3.0). | For Software Version, choose your software version . | Choose to Continue to Launch . | Under Usage instructions, review the documentation, then choose to Activate the Glue connector from AWS Glue Studio. | You’re redirected to AWS Glue Studio to create a Connection. | For Name, enter a name for your connection (for example, bigquery). | For AWS Secret, choose bigquery_credentials. | Choose to Create a connection and activate the connector. | A message appears that the connection was successfully created, and the connection is now visible on the AWS Glue Studio console. | . Setup Google Cloud Platform (GCP) Access Credentials . | Create and Download the service account credentials JSON file from Google Cloud. Create credentials for a GCP service account . | base64 encode the JSON access credentials. For Linux and Mac, you can use base64 «service_account_json_file» to output the file contents as a base64-encoded string . | . Add GCP Credentials in Secrets Manager . In Secret Manager, paste the credentials into the secret bigquery_credentials credentials value with the base64 encode access credentials: . Figure 2 –Secrets Manager . Figure 3 – Location to paste the credentials . ",
    "url": "/connecting-data-from-google-analytics.html#implementation-steps",
    
    "relUrl": "/connecting-data-from-google-analytics.html#implementation-steps"
  },"82": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Testing",
    "content": ". | Manually initiate the AWS Step Functions named gcp-connector-glue . | After the Step Functions is completed, go to Athena . | Select Data source: AwsDataCatalog . | Select the Database: ga4_analytics . | Query SELECT * FROM your ga4_events . | . | . Figure 4 – Athena query editor . ",
    "url": "/connecting-data-from-google-analytics.html#testing",
    
    "relUrl": "/connecting-data-from-google-analytics.html#testing"
  },"83": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Sample Queries",
    "content": "Sample queries are available in the GitHub repository . ",
    "url": "/connecting-data-from-google-analytics.html#sample-queries",
    
    "relUrl": "/connecting-data-from-google-analytics.html#sample-queries"
  },"84": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Cleanup",
    "content": "When you are finished experimenting with this solution, clean up your resources by running the command: . cdk destroy . This command deletes resources deploying through the solution. The Secrets Manager secret containing the manually added GCP Secret and Amazon CloudWatch log groups are retained after the stack is deleted. ",
    "url": "/connecting-data-from-google-analytics.html#cleanup",
    
    "relUrl": "/connecting-data-from-google-analytics.html#cleanup"
  },"85": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Resources",
    "content": ". | Google BigQuery Connector for AWS Glue . | Exporting Data from Google Analytics 4 Properties to BigQuery . | Migrating data from Google BigQuery to Amazon S3 using AWS Glue custom connectors . | Create credentials for a GCP service account . | . ",
    "url": "/connecting-data-from-google-analytics.html#resources",
    
    "relUrl": "/connecting-data-from-google-analytics.html#resources"
  },"86": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Source Code",
    "content": "You can visit the GitHub repository to download the templates and scripts for this solution . ",
    "url": "/connecting-data-from-google-analytics.html#source-code",
    
    "relUrl": "/connecting-data-from-google-analytics.html#source-code"
  },"87": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Revisions",
    "content": "November 2022 Initial Release . ",
    "url": "/connecting-data-from-google-analytics.html#revisions",
    
    "relUrl": "/connecting-data-from-google-analytics.html#revisions"
  },"88": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Contributors",
    "content": "The following individuals contributed to this document: . | Brian Maguire . | Anurag Singh . | . ",
    "url": "/connecting-data-from-google-analytics.html#contributors",
    
    "relUrl": "/connecting-data-from-google-analytics.html#contributors"
  },"89": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "AWS Glossary",
    "content": "For the latest AWS terminology, see the AWS glossary in the AWS General Reference guide. ",
    "url": "/connecting-data-from-google-analytics.html#aws-glossary",
    
    "relUrl": "/connecting-data-from-google-analytics.html#aws-glossary"
  },"90": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Notices",
    "content": "Customers are responsible for making their own independent assessment of the information in this document. This document: (a) is for informational purposes only, (b) represents AWS current product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from AWS and its affiliates, suppliers or licensors. AWS products or services are provided “as is” without warranties, representations, or conditions of any kind, whether express or implied. AWS responsibilities and liabilities to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. Google Analytics is a trademark of Google LLC . ",
    "url": "/connecting-data-from-google-analytics.html#notices",
    
    "relUrl": "/connecting-data-from-google-analytics.html#notices"
  },"91": {
    "doc": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "title": "Guidance for Connecting Data from Google Analytics to AWS Clean Rooms",
    "content": ". ",
    "url": "/connecting-data-from-google-analytics.html",
    
    "relUrl": "/connecting-data-from-google-analytics.html"
  },"92": {
    "doc": "Implementation Guides",
    "title": "Implementation Guides",
    "content": "Guidance for Automated Provisioning of Amazon Elastic Kubernetes Service (EKS) using Terraform Implementation Guide . Learn More Guidance for Building Queries in AWS Clean Rooms . Learn More Guidance for Connecting Audiences to Amazon Marketing Cloud Uploader . Learn More Guidance for Connecting Data from Adobe Experience Platform to AWS Clean Rooms . Learn More Guidance for Connecting Data from Google Analytics to AWS Clean Rooms . Learn More Guidance for Connecting Data to AWS Clean Rooms . Learn More Guidance for Developing Applications Using Generative AI with Amazon CodeWhisperer . Learn More Guidance for Launching a Simple E-commerce Website with WordPress on AWS . Learn More Guidance for Meter Data Analytics on AWS . Learn More Guidance for Uploading Audiences created in AWS Clean Rooms to Meta Business Marketing Platform . Learn More Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads . Learn More ",
    "url": "/index.html",
    
    "relUrl": "/index.html"
  },"93": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to Meta Business Marketing Platform",
    "title": "Overview",
    "content": "Today, marketers have plenty of options when it comes to audience targeting and intelligence solutions in the form of Software as a Service (SaaS). Increasingly, first party data in an enterprise are being siloed because of on-premise, public cloud, and SaaS implementation models for business applications. With these enterprise data siloes and the need for multiple data providers get the right marketing intelligence, marketers are faced with the difficulty in bringing data together in a privacy compliant way. AWS Clean Rooms service is a data collaboration tool that helps marketers bring data together from various data providers and publishers and match it against their first party data in a fast, privacy compliant fashion. AWS Clean Rooms allows this to be done with minimum data movement.  . One of the common outcomes for an advertiser from the data collaboration is a list of customers who need to be targeted on leading publishing platforms like Meta, Amazon Ads, Snap, and TikTok. This document serves as a solution guidance for marketers and data engineers to activate their audiences on Meta Business Marketing platform.  . Meta has multiple properties under their brand for advertisers to use. This solution focuses and on Facebook Marketing API. The strategies mentioned here could be used for other meta properties like Instagram Ads API. ",
    "url": "/uploading-audiences-to-meta-business-marketing-platform.html#overview",
    
    "relUrl": "/uploading-audiences-to-meta-business-marketing-platform.html#overview"
  },"94": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to Meta Business Marketing Platform",
    "title": "Considerations for this guidance",
    "content": ". | Intended audience is existing AWS customers with an active account and have already engaged in a data collaboration using AWS Clean Rooms service. | AWS Clean Rooms service is used by a marketing analyst or data engineer to create an audience list using the SQL query and export capability which comes out of the box in the AWS Clean Rooms service . | User personas who are implementing this solution guidance are data engineers in marketing or a central data team.  . | Each run of the data pipeline uploads a new list of audience data. Updates to existing audience list and de-duplication is expected to be handled by the Publisher (Meta) API . | The solution accelerates deployment of the services needed to create an activation data pipeline. Example data transformations will also be provisioned.  . | Expectation is that customers can customize the services and transformation logic provisioned in their account by the solution to meet their specific needs. | . ",
    "url": "/uploading-audiences-to-meta-business-marketing-platform.html#considerations-for-this-guidance",
    
    "relUrl": "/uploading-audiences-to-meta-business-marketing-platform.html#considerations-for-this-guidance"
  },"95": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to Meta Business Marketing Platform",
    "title": "Solution Architecture for AWS Clean Rooms Activation flow",
    "content": "Figure 1 - Diagram for AWS Clean Rooms activation flow . ",
    "url": "/uploading-audiences-to-meta-business-marketing-platform.html#solution-architecture-for-aws-clean-rooms-activation-flow",
    
    "relUrl": "/uploading-audiences-to-meta-business-marketing-platform.html#solution-architecture-for-aws-clean-rooms-activation-flow"
  },"96": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to Meta Business Marketing Platform",
    "title": "Detailed steps for Meta activation after data collaboration",
    "content": "The proposed solution for activation on Facebook Marketing API uses Amazon S3, AWS Glue, Amazon EventBridge, AWS Secrets Manager, and AWS Lambda services. These services are put together in the solution for collecting the output of AWS Clean Rooms collaboration, normalizing, and delivering data to the Meta ads platform based on their API contracts.  . For demonstration purposes, this solution is using the Conversion API within the Meta ads platform. Please refer to Meta documentation for data normalization requirements. There are several ways to setup the Conversions API. The Direct integration using code approach is used here. The solution also uses Facebook’s Business Software Development Kit (SDK) in Python for building the integration. Use of SDK abstracts the hashing/pseudo-anonymizing Personally Identifiable Information (PII). Prerequisites . For setting up this solution, you will need access to the following beforehand: . | A Facebook Developer Account . | A registered Facebook App with Basic settings configured . | A Page Access Token: . | . Figure 2 - Image of the settings tab where you’ll find the token . | AWS Secrets Manager secret (you can create the secret in following steps) | . Solution Setup instructions . High Level Steps . The solution setup consists of these high-level steps: . | AWS Identity and Access Management (IAM) setup using AWS IAM service . | AWS Key Management Store (AWS KMS) setup for encryption keys using AWS KMS service . | App secret and configuration setup using AWS Secrets Manager/AWS System Manager Parameter store service . | Data Storage setup in Amazon S3 service . | Data Transformation job setup using AWS Glue service . | Event handling setup using Amazon EventBridge and AWS Simple Queue Service (SQS) service . | Facebook data publish setup using AWS Lambda service . | . These steps are in more detail below: . Detailed instructions . IAM Setup . Step 1: Create a new IAM role that would be used for the build of this solution. This role is assumed by the AWS services that need access to other AWS services. | Navigate to Access Management → Roles → Create role . | Keep Trusted entity type as AWS Service . | Select Lambda as the Use case and select next. We are starting with Lambda but could add more as needed . | In the Add permissions page, search and use the below permissions: . | AWSLambdaBasicExecutionRole . | AWSGlueServiceRole . | Create an inline policy with the policy statement below to give the role read access to Amazon S3, KMS, System Manager services and resources under them. Substitute bucket name, region, account, key id, and parameter values with real ones: . | . | . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CleanRoomActivationS3Access\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:GetObject\", \"s3:ListBucket\", \"s3:DeleteObject\", \"s3:GetBucketLocation\", \"s3:ListMultipartUploadParts\" ], \"Resource\": [ \"arn:aws:s3:::&lt;bucket name&gt;/*\", \"arn:aws:s3:::&lt;bucket name&gt;\" ] }, { \"Sid\": \"CleanRoomActivationKMSAccess\", \"Effect\": \"Allow\", \"Action\": [ \"kms:Describe*\", \"kms:Get*\", \"kms:List*\", \"kms:Decrypt\", \"kms:Encrypt\", \"kms:GenerateDataKey\" ], \"Resource\": \"arn:aws:kms:&lt;region&gt;:&lt;account&gt;:key/&lt;key id&gt;\" }, { \"Sid\": \"CleanRoomActivationSSMAccess\", \"Effect\": \"Allow\", \"Action\": [ \"ssm:GetParameter*\" ], \"Resource\": \"arn:aws:ssm:&lt;region&gt;:&lt;account id&gt;:parameter/&lt;parameter path&gt;*\" } ] } . | Enter name, description and tags and create role. | Open the newly created role and navigate to the Trust relationships tab.  . | Edit the trust policy to include the below trust relationships to let multiple services assume this role: . | . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"lambda.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" }, { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"glue.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] } . | Keep the ARN of the role handy for future use | . AWS Key Management Service (AWS KMS) Setup . Step 2: Create a new AWS KMS Key . | Navigate to the AWS KMS console and select on Create Key . | Keep Symmetric as Key Type and Encrypt and decrypt as key usage and select next . | Enter name, description and tags, and click next . | Keep all other settings as default and create the key. Keep the Key id and ARN handy for future use . | . Secrets and configuration storage . Step 3: Store access token from Meta Pixel setup in either AWS Secrets manager or AWS System Manager Parameter Store . If using AWS Secrets manager for storing secrets, follow below steps: . | On the Secrets Manager console, choose Store a new secret. | For Secret type, select Other . | Enter your key as credentials and the value as the base64-encoded string. | Leave the rest of the options at their default. | Choose Next. | Give a name to the secret following a URI format to make it easier to find it amongst multiple secrets in the/dev/cleanroom-activations/meta/conversions/access_token. | Follow through the rest of the steps to store the secret. | . If using AWS System Manager Parameter Store for storing secrets and other configurations, follow the steps below: . | On the System Manager console, navigate to Application Management → Parameter Store and select Create parameter . | Enter parameter name in a URI format. For example, /dev/cleanroom-activations/meta/conversions . | Enter a description, keep Tier as Standard and choose SecureString as Type . | Select an AWS KMS key to encrypt the secret. Use Customer Managed Key as a best practice. Create a new one for the project if needed as it can be used for encryption of data throughout the data pipeline . | Keep datatype as text and into the value input enter the json string below. Fill in the access_token and pixel_id values: . | . { \"access_token\": \"\", \"pixel_id\": \"\" } . | Enter tags and click on create parameter | . Data storage setup . Step 4: Create two Amazon S3 buckets, one for storing output of AWS Clean Rooms collaboration export query and another for storing output of the AWS Glue data transformation job . | Navigate to Amazon S3 console . | Choose Create bucket . | Provide a globally unique bucket name . | Choose the appropriate region . | Block public access . | Enable Bucket Versioning . | Enable SSE-S3 based bucket encryption . | Provide appropriate tag(s) . | Create bucket and repeat the process for the second one . | . Data transformation setup . Step 5: Create an AWS Glue job that reads from export bucket and generates activation output files. Create multiple files: . | Navigate to AWS Glue studio console → Data Integration and ETL → AWS Glue Studio → Jobs . | On the create job page keep Visual with source and target option and keep source and target as Amazon S3 and select create . | On the AWS Glue studio canvas go to Job Details . | Add a name, select the new IAM role created earlier . | Select Server-side encryption in Advanced options and keep all other configurations default . | Click save on the top right corner of console . | Go back to the canvas and click on the Source s3 node. a. Select the s3 bucket and folder where data collaboration output will be stored . b. It’s assumed at this point that the output of the AWS Clean Rooms export may have multiple files having the same structure and the AWS Glue job is expected to read all of them when run . c. In this example, it’s assumed that output file is a CSV with “,” as delimiter . d. Choose escape, Quote and column header options accordingly . | . tips . | Use data preview option to ensure data is being read correctly | AWS Glue requires input file to be encoded in utf8. If input file is not utf8 encoded, refer this page for potential solutions for converting | . | Remove the ApplyMapping node . | Add a new SQL query to transform node and select source s3 bucket as parent. We will come back to this transform node to finish the setup . | Select target s3 node and change the node parent to the SQL transform node . | Go to Data target properties tab and select the Amazon S3 target location. Keep all other configurations as default and save the job using the button on the top right corner of the console . | . Once all the steps are validated, AWS Glue gives you an option to preview data. At this point doing that validation, it is a good idea to visualize data output in each step: . Figure 3 - Visualization of data output in each step . Event handling setup . | Create a new dummy lambda function. This step is just to have a reference point available to update the EventBridge rule. Once the integration is done and tested on a high level, you will be coming back and updating this function to include more logic . | Create an Amazon Simple Queue Service (Amazon SQS) queue that would act as a dead letter queue for s3 upload events . a. Navigate to Amazon SQS console and select Create queue . b. Keep Type as Standard and give a name for the queue. Suffix DLQ to notate that this queue acts as a dead letter queue . c. Keep the configuration settings as is . d. In the Access policy card, keep the Basic selection on.  . e. Give permission to the IAM role created earlier to send and receive messages from this queue by entering the ARN of the role in the input field . f. Keep all other configurations as default. Encryption at rest is optional because this queue will not be storing sensitive data. It will be storing only the metadata about objects created in the s3 bucket . g. Add tags as needed and create the queue. Keep the ARN of the queue handy for future reference . | . | Create an Amazon EventBridge rule on the default event bus | . Create a new rule: .             a. From the Amazon EventBridge Console navigate to Events → Rules.             b. Enter Name, Description and keep default selected in Event bus option .             c. Keep Rule with an event pattern selected and click next .             d. Select AWS Events as Event Source .             e. Skip Sample event card and scroll down to Event Pattern .             f. Select Custom patterns that allows you to enter the pattern as a json .             g. Edit json below to update the bucket.name and object.key.prefix key value pairs and paste in the json editor . { \"source\": [\"aws.s3\"], \"detail-type\": [\"Object Created\"], \"detail\": { \"bucket\": { \"name\": [\"cleanroom-activations\"] }, \"object\": { \"key\": [{ \"prefix\": \"meta/\" }] } } } .             h. Ensure that json is valid and select next .             i. Select AWS service as target and select the newly created lambda function .             j. Keep all the other settings as defaults except for retry attempts and dead-letter queue option .             k. For retry attempts change the value to 3 .             l. Select the newly created Amazon SQS queue from the drop down and select next .             m. Add tags as required and select next .             n. Review the configurations and select create button to complete rule creation. | Create an event bus archive to persist events specific to the file upload and for replay capability | . Facebook data publish setup . | Update the Lambda function and include logic to read the files and send it to Meta API . | Clone sample code from repo and change it to meet your needs. Paste the code in to the Lambda code editor. | Add two dependency layers to the Lambda function following referenced documentations AWS Data Wrangler and Facebook Business . | . | Use Python 3.9 where ever Python 3.8 is referenced in the above instruction . | Use facebook_business as the module name in pip install . | Give FacebookBusiness-Python39 as the layer name . a. While using AWS Data Wrangler package use “chunksize” option reading s3 files . b. Use Meta Python Business SDK to build the payload and send data to the platform . c. Upload a sample file to the s3 bucket created and test the lambda function using a sample event payload. Replace bucket and object name with real bucket and object name so that the function can access the file to read data: . | . { \"version\": \"0\", \"id\": \"2d4eba74-fd51-3966-4bfa-b013c9da8ff1\", \"detail-type\": \"Object Created\", \"source\": \"aws.s3\", \"account\": \"123456789012\", \"time\": \"2021-11-13T00:00:59Z\", \"region\": \"us-east-1\", \"resources\": [ \"arn:aws:s3:::&lt;bucket name&gt;\" ], \"detail\": { \"version\": \"0\", \"bucket\": { \"name\": \"&lt;bucket name&gt;\" }, \"object\": { \"key\": \"&lt;object name&gt;\", \"size\": 99797, \"etag\": \"7a72374e1238761aca7778318b363232\", \"version-id\": \"a7diKodKIlW3mHIvhGvVphz5N_ZcL3RG\", \"sequencer\": \"00618F003B7286F496\" }, \"request-id\": \"4Z2S00BKW2P1AQK8\", \"requester\": \"348414629041\", \"source-ip-address\": \"72.21.198.68\", \"reason\": \"PutObject\" } } . Successful execution should show results similar to below: . Figure 4 - Successful execution should show results similar to image above . d. Test the end-to-end flow starting from file upload. e. Validate data in Meta platform using the Events Manager portal . Figure 5 - Image of Events Manager portal . Scheduling and other operational procedures . Here are some considerations for ongoing maintenance and support of the solution . | AWS Glue comes with a scheduler functionality that can be used to schedule jobs at a regular cadence. Use this for recurring jobs.  . | Use Amazon CloudWatch service for viewing execution logs and visualizations for troubleshooting.  . | Since the source of data for this integration is generated in a batch mode, re-running of failed batches could end up sending duplicate data into the activation channels and it’s assumed that the activation channel API’s are able to handle it.  . | To avoid duplications, e-running the data collaboration query by restricting the records exported is recommended. | Amazon EventBridge event replay feature could be used to re-run batches without the need for recreating the files,  . | Implement Amazon S3 bucket lifecycle policies to keep data secure . | . Performance Considerations . Proposed solution implements below for performance optimization techniques . | Data output from AWS Clean Rooms Clean Rooms data collaboration and AWS Glue transform job stage should be stored in multiple files. | Read data in chunks of predefined size from each file in the AWS Lambda function . | . These measures provide the following benefits: . | Publisher API request can be batched up with multiple rows, reducing the risk of hitting Publisher API rate limits. | Memory utilization of Lambda can be minimized . | . Customer List Custom Audiences API . For activation purposes, customers should be using the Customer List Custom Audiences API. Setting up your Meta account to receive custom customer lists should be done in consultation with your Meta Business Marketing point of contact(s).  . Once the Meta app and Ad account setup is done, follow the steps below to repoint your pipeline to hit the Custom Audiences API . | Create a new the AWS Lambda function that connects to the Custom Audiences API.  . | Update EventBridge rule to call the new Lambda function . | . Data Transformation – Alternatives . The AWS glue job that does the transformation/normalization of data can be substituted with an AWS Glue DataBrew job. DataBrew service is a no code solution to build your data transformation logic. The data transformation code is stored as a re-usable recipe. A published recipe is then used in a DataBrew job to generate output. ",
    "url": "/uploading-audiences-to-meta-business-marketing-platform.html#detailed-steps-for-meta-activation-after-data-collaboration",
    
    "relUrl": "/uploading-audiences-to-meta-business-marketing-platform.html#detailed-steps-for-meta-activation-after-data-collaboration"
  },"97": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to Meta Business Marketing Platform",
    "title": "Conclusion",
    "content": "The reference architecture provided here addresses the need for activation of audiences on multiple publisher platforms. The detailed guidance provides the steps needed to activate on Meta which can be used as a template for adding new publishing channels. The main difference in the activation flow for other platforms like Snapchat and TikTok would be on the Lambda code that connects to the publisher API as well as their unique data transformation needs. ",
    "url": "/uploading-audiences-to-meta-business-marketing-platform.html#conclusion",
    
    "relUrl": "/uploading-audiences-to-meta-business-marketing-platform.html#conclusion"
  },"98": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to Meta Business Marketing Platform",
    "title": "Notices",
    "content": "Meta and Facebook are trademarks of Meta Platforms, Inc. or its affiliates. ",
    "url": "/uploading-audiences-to-meta-business-marketing-platform.html#notices",
    
    "relUrl": "/uploading-audiences-to-meta-business-marketing-platform.html#notices"
  },"99": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to Meta Business Marketing Platform",
    "title": "Guidance for Uploading Audiences created in AWS Clean Rooms to Meta Business Marketing Platform",
    "content": ". ",
    "url": "/uploading-audiences-to-meta-business-marketing-platform.html",
    
    "relUrl": "/uploading-audiences-to-meta-business-marketing-platform.html"
  },"100": {
    "doc": "Guidance for Meter Data Analytics on AWS",
    "title": "Overview",
    "content": "This guide covers the information you need to deploy “Guidance for Meter Data Analytics on AWS” in your AWS environment. ",
    "url": "/meter-data-analytics.html#overview",
    
    "relUrl": "/meter-data-analytics.html#overview"
  },"101": {
    "doc": "Guidance for Meter Data Analytics on AWS",
    "title": "Costs and licenses",
    "content": "This implementation guide is offered to users at no cost. However, if you deploy “Guidance for Meter Data Analytics to AWS” in your environment, you will be billed for any AWS services or resources you use in the deployment. ",
    "url": "/meter-data-analytics.html#costs-and-licenses",
    
    "relUrl": "/meter-data-analytics.html#costs-and-licenses"
  },"102": {
    "doc": "Guidance for Meter Data Analytics on AWS",
    "title": "Architecture",
    "content": "Deploying this Guidance with default parameters builds the Meter Data Analytics (MDA) environment in the AWS Cloud, as shown in Figure 1. Figure 1. Solution architecture for Meter Data Analytics on AWS. This Guidance sets up the following services: . | Amazon Simple Storage Service (Amazon S3) buckets to store: . | Weather and topology data from external databases. | Raw meter data. | Partitioned data. | Processed data from the AWS Step Functions model training workflow. | . | AWS Lambda functions to: . | Load and transform topology data from an external database. | Process late-arriving meter data. | Obtain API query results of partitioned business data and Amazon SageMaker inferences. | . | An AWS Glue crawler to transform meter data from the meter data management (MDM) system and headend system (HES) system into partitioned data. | Amazon EventBridge to process and store late-arriving meter data in the correct partition. | AWS Glue Data Catalog for a central catalog of weather, topology, and meter data. | Amazon Athena to provide query results of partitioned data. | Two Step Functions workflows: . | Model training to build a machine learning (ML) model using partitioned business data. | Batch processing of partitioned business data from the Data Catalog and ML model data for use in forecasting. | . | SageMaker to generate energy usage inferences using the ML model. | Amazon API Gateway to manage API queries. | . ",
    "url": "/meter-data-analytics.html#architecture",
    
    "relUrl": "/meter-data-analytics.html#architecture"
  },"103": {
    "doc": "Guidance for Meter Data Analytics on AWS",
    "title": "Deployment options",
    "content": "This Guidance provides one deployment option: . | CloudFormation (CFN) template – This uses assets already deployed in public S3 buckets. To use the latest updates, you have to use the ‘Manual deployment’ process. | .           o Download the template from https://github.com/aws-solutions-library-samples/guidance-for-meter-data-analytics-on-aws/blob/main/templates/work-load.template.yaml . | Manual deployment – Build and deploy in your AWS account | .          o Steps to build and deploy are in https://github.com/aws-solutions-library-samples/guidance-for-meter-data-analytics-on-aws . This Guidance lets you configure instance types and MDA settings. Predeployment steps . Meter data generator and HES simulator . The input adapter regularly pulls data from the HES simulator and prepares the data for further processing. During deployment, you can choose to deploy the meter data generator and HES simulator stacks by choosing ENABLED for the MeterDataGenerator parameter. You also configure the number of meters and the interval between meter reads with the TotalDevices and GenerationInterval parameters, respectively. The default values for these parameters generate 10,000 reads every 15 minutes. ",
    "url": "/meter-data-analytics.html#deployment-options",
    
    "relUrl": "/meter-data-analytics.html#deployment-options"
  },"104": {
    "doc": "Guidance for Meter Data Analytics on AWS",
    "title": "Deployment steps - using CFN",
    "content": ". | Sign in to your AWS account, and download the CloudFormation template for this Guidance, as described under Deployment options. | Navigate to CloudFormations. | Select Create stack. | Select Upload a template file. | Select Choose file and select the downloaded file. | On the Create stack page, choose Next. | On the Specify stack details page, change the stack name if needed. Review the parameters for the template. Provide values for the parameters that require input. For all other parameters, review the default settings and customize them as necessary. When you finish reviewing and customizing the parameters, choose Next. | . Unless you’re customizing the Guidance templates or are instructed otherwise in this guide’s Predeployment section, don’t change the default settings for the following parameters: QSS3BucketName, QSS3BucketRegion, and QSS3KeyPrefix. Changing the values of these parameters will modify code references that point to the Amazon S3 bucket name and key prefix. | On the Configure stack options page, you can specify tags (key-value pairs) for resources in your stack and set advanced options. When you finish, choose Next. | On the Review page, review and confirm the template settings. Under Capabilities, select all of the check boxes to acknowledge that the template creates AWS Identity and Access Management (IAM) resources that might require the ability to automatically expand macros. | Choose Create stack. The stack takes about 30 minutes to deploy. | Monitor the stack’s status, and when the status is CREATE_COMPLETE, the MDA deployment is ready. | To view the created resources, choose the Outputs tab. | . Postdeployment steps . Set up Amazon Managed Grafana dashboards . This Guidance includes a set of Amazon Managed Grafana dashboards. These dashboards use Athena to query data stored in a data lake and depend on several of the optional datasets for full functionality. Amazon Managed Grafana supports Security Assertion Markup Language (SAML) and IAM Identity Center for authentication. If your organization does not have SAML or IAM Identity Center set up, consult your administrator. Create a Grafana workspace . | In the Amazon Managed Grafanaconsole, choose Create workspace. | Enter a Workspace Name (for example, mda_solution). If desired, enter an optional Workspace Description. | Choose Next. | On the Configure Settings page, for Authentication access, select Security Assertion Markup Language (SAML). | For Permission type, choose Service managed. | Choose Next. | On the Service managed permission settings page, for IAM permission access settings, choose Current account. | For Data sources and notification channels, select Amazon Athena. | Choose Next. | Choose Create workspace. | . Select/Create SSO users and groups . After creating a Grafana workspace, select the single sign-on (SSO) users and groups with whom you want to share access. For more information, refer to Managing user and group access to Amazon Managed Grafana. If you don’t already have users and groups setup in IAM Identity Center, follow these steps to create a user to be used with Grafana: . | In a new tab or browser window, navigate to the IAM Identity Center in AWS Console. | Select Users in the left panel. | Select Add user button on the right panel. | Add a user you would like to use for testing, as shown in Figure 2 below. | . Figure 2. Screenshot showing how to add a user for testing. | Choose Next at the bottom of the screen. | Choose Next again in the group creation screen. | Choose Add user button to create the user. | Choose Copy to copy the URL and login details for later use. | . Assign SSO user to access Grafana . To give an SSO user access to Grafana, follow these steps: . | In the Grafana tab, choose the workspace mda_solution that was created. | Confirm you are in the Authentication tab. | Choose Assign new user or group button. | Select the checkbox next to the user that was created in the IAM Identity Center. | Select the button Assign users and groups. | Select the checkbox next to the user, and then select the Action dropdown. | Select Make admin. | . To access the workspace, follow these steps: . | Navigate to the URL that was saved when the user was created in IAM Identity Center. | Log in using the saved username and password. | You will be prompted to update the password the first time you log in. | Select the Amazon Grafana panel to open Grafana, as shown in Figure 3. | . Figure 3. Screenshot showing Grafana in the Amazon Grafana panel. Set up Amazon Athena as a data source in Grafana . Follow these steps to confirm Amazon Athena is added as a data source in Amazon Managed Grafana: . | Select Configuration (cog icon), and then select Data sources. | Validate that you can see the data sources for Athena shown in Figure 4. | . Figure 4. Screenshot showing data sources for Athena. If you run into issues, refer to Amazon Athena and Data Source Management. Import dashboards into Amazon Managed Grafana . Upload or paste the contents of dashboard JSON files from the /scripts/assets/grafana folder of the solution’s repository to Amazon Managed Grafana. Follow these steps to upload the dashboard JSON: . | Select the + icon, and then select Import. | Select Upload JSON file to upload each JSON file. | . For more information, refer to Importing a dashboard. Follow these steps to view the imported dashboards: . | Select the Dashboards icon, and then select Browse. | You should see all the imported dashboards. Select each dashboard to view. | . After importing, you may need to verify that the names of dashboard panel data sources match AWS Glue database names. If you receive errors, check panel data sources and variables in dashboard settings. For more information, refer to Adding or editing a panel and Dashboards. Input adapter . As shown in Figure 5, the Guidance’s input adapter loads meter reads from an external source (such as HES or FTP) and prepares them for processing. Figure 5. Input adapter . A Step Functions workflow orchestrates the generation and download of the meter-reads file as a compressed file from the external database to the inbound bucket. Another process extracts the file from the inbound bucket and stores it in the uncompressed folder. The inbound bucket stores the compressed and uncompressed files. The Guidance deletes uncompressed files to save on storage and costs. After the file is extracted, the Guidance generates an event that invokes a Lambda function for further processing. The file-range-extractor Lambda function extracts range information from the uncompressed file based on the file size and number of chunks (configurable). A range is a group of lines that you want to process together. Extracted range information is sent to Amazon Simple Queue Service (Amazon SQS). Each worker takes a range from the Amazon SQS queue and processes the respective meter reads (parse and transform) before sending each element to Amazon Kinesis. This process ensures that the content input file can be processed in parallel. The worker transforms the CSV line into JSON and creates a separate object for each reading type. The Amazon Kinesis data stream ingests the data into the staging area. This stream scales on demand. Dataflows . You can set up connections to other data sources by configuring additional dataflows. As shown in Figure 6, a dataflow connects to the external database, loads data from it, and stores data in a purpose-built database that can be accessed from the Guidance’s central data catalog. Figure 6. Dataflow . The Guidance comes with two sample dataflows: weather and topology. To add a new dataflow, create a data pipeline that loads data from the source, prepares the data, and stores results in an appropriate data store. Then, add the data store you’ve configured to the Guidance’s data catalog. Data partitioning . The weather data is partitioned in the S3 bucket s3://gen-disabled-aws-utility-met-integrateddatabucket-\\*/weather/date=\\&lt;year\\&gt;-\\&lt;month\\&gt;-\\&lt;day\\&gt;/\\&lt;data-file-in-parquet-format\\&gt; . The topology data is stored in the S3 bucket s3://gen-disabled-aws-utility-met-integrateddatabucket-\\*/topology/ . The curated data in the integration stage S3 bucket is partitioned by reading type, year, month, day, and hour, as follows: . s3://\\*-integrateddatabucket-\\*/readings/parquet/reading_type=\\&lt;reading_type_value\\&gt;/year=\\&lt;year\\&gt;/month=\\&lt;month\\&gt;/day=\\&lt;day\\&gt;/hour=\\&lt;hour\\&gt;/\\&lt;meter-data-file-in-parquet-format\\&gt; . You can find all meter reads for the hour of a day on the lowest level of the partition tree. To optimize query performance, the data is stored in a column-based file format (Parquet). Late-arriving data . The data lake handles late-arriving meter reads. Late-arriving meter reads are detected as soon as the data reaches the staging phase. If a late read is detected, an event is sent to Amazon EventBridge. The extract, transform, load (ETL) pipeline moves the late read to the correct partition and ensures that data is stored in an optimized way. Data formats . Table 1 shows inbound schema. | Field | Type | Format | Description | . | time | timestamp | yyyy-MM-dd HH:mm:ss.SSSSSSS | Timestamp when the meter read reaches the source system. | . | arrival_time | timestamp | yyyy-MM-dd HH:mm:ss.SSSSSSS | Timestamp of the actual meter read. | . | device_id | string | 7a044be7-2f1e-3bf1-aa86-b8b1b9064f19 | uuid | . | measure_name | string |   |   | . | load | double | 0.000 | Load, unit: A | . | crrnt | double | 0.000 | Current, unit: A | . | pf | double | 0.000 | Power factor, between 0 and 1 | . | vkva | double | 0.000 | Volt ampere, unit: VA | . | kw | double | 0.000 | Kilowatt, unit: kW | . | vltg | double | 0.000 | Voltage, unit: V | . Table 1. Inbound schema . Inbound format . The inbound meter-data format is variable and can be adjusted. The following shows the sample inbound data format of the meter data generator. Integrated format . Data are stored in the format shown in Table 2 in the integration stage. | Field | Type | Format | . | meter_id | String |   | . | reading_value | Double | 0.000 | . | reading_date_time | Timestamp | yyyy-MM-dd HH:mm:ss.SSS | . | unit | String |   | . | obis_code | String |   | . | phase | String |   | . | reading_source | String |   | . | reading_type | String (Partitioned) | load, crrnt, pf, kva, kw, vltg | . | year | String (Partitioned) |   | . | month | String (Partitioned) |   | . | day | String (Partitioned) |   | . | hour | String (Partitioned) |   | . Table 2. Integration schema . ",
    "url": "/meter-data-analytics.html#deployment-steps---using-cfn",
    
    "relUrl": "/meter-data-analytics.html#deployment-steps---using-cfn"
  },"105": {
    "doc": "Guidance for Meter Data Analytics on AWS",
    "title": "Troubleshooting",
    "content": "For common troubleshooting issues, visit Troubleshooting CloudFormation. ",
    "url": "/meter-data-analytics.html#troubleshooting",
    
    "relUrl": "/meter-data-analytics.html#troubleshooting"
  },"106": {
    "doc": "Guidance for Meter Data Analytics on AWS",
    "title": "Customer responsibility",
    "content": "After you deploy a Guidance, confirm that your resources and services are updated and configured—including any required patches—to meet your security and other needs. For more information, refer to the Shared Responsibility Model. ",
    "url": "/meter-data-analytics.html#customer-responsibility",
    
    "relUrl": "/meter-data-analytics.html#customer-responsibility"
  },"107": {
    "doc": "Guidance for Meter Data Analytics on AWS",
    "title": "Feedback",
    "content": "To submit feature ideas and report bugs, use the Issues section of the GitHub repository for this Guidance. ",
    "url": "/meter-data-analytics.html#feedback",
    
    "relUrl": "/meter-data-analytics.html#feedback"
  },"108": {
    "doc": "Guidance for Meter Data Analytics on AWS",
    "title": "Notices",
    "content": "This document is provided for informational purposes only. It represents current AWS product offerings and practices as of the date of issue of this document, which are subject to change without notice. Customers are responsible for making their own independent assessment of the information in this document and any use of AWS products or services, each of which is provided \"as is\" without warranty of any kind, whether expressed or implied. This document does not create any warranties, representations, contractual commitments, conditions, or assurances from AWS, its affiliates, suppliers, or licensors. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. The software included with this paper is licensed under the Apache License, version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the accompanying \"license\" file. This code is distributed on an\"as is\" basis, without warranties or conditions of any kind, either expressed or implied. Refer to the License for specific language governing permissions and limitations. Privacy| Site terms| © 2023, Amazon Web Services, Inc. or its affiliates and Amazon Web Services. All rights reserved. ",
    "url": "/meter-data-analytics.html#notices",
    
    "relUrl": "/meter-data-analytics.html#notices"
  },"109": {
    "doc": "Guidance for Meter Data Analytics on AWS",
    "title": "Guidance for Meter Data Analytics on AWS",
    "content": ". Refer to the GitHub repository to view source files, report bugs, submit feature ideas, and post feedback about this Implementation Guide. To comment on the documentation, refer to Feedback. This implementation guide provides an overview of the \"Guidance for Meter Data Analytics on AWS,\" its reference architecture and components, considerations for planning the deployment, and configuration steps for deploying to Amazon Web Services (AWS). This guide is intended for solution architects, business decision makers, DevOps engineers, data scientists, and cloud professionals who want to implement \"Guidance for Meter Data Analytics on AWS\" in their environment. ",
    "url": "/meter-data-analytics.html",
    
    "relUrl": "/meter-data-analytics.html"
  },"110": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Overview",
    "content": "This Guidance assists AWS customers with automating the uploader of TikTok Ads with custom audience data for TikTok Advertiser. It explores the stages of uploading custom audience segment data created in AWS to deliver personalized ads in TikTok. The uploader for TikTok Ads allows you to use enriched data in AWS to create targeted custom audiences in TikTok. With this connector, you can leverage user profile data to create custom audiences in TikTok in a custom file upload. ",
    "url": "/uploading-audiences-to-tiktok-ads.html#overview",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#overview"
  },"111": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Cost and licenses",
    "content": "No licenses are required to deploy this solution. There is no cost to use this solution, but you will be billed for any AWS services or resources that this solution deploys. ",
    "url": "/uploading-audiences-to-tiktok-ads.html#cost-and-licenses",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#cost-and-licenses"
  },"112": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Architecture overview",
    "content": "With the Uploader for TikTok Ads, using an event-based serverless connector solution, you can securely ingest first party data along with third party data to create custom audiences in TikTok. Figure 1 - Diagram for uploading TikTok Ads marketing campaign using AWS . | TikTok access token and advertiser_id is securely updated in AWS Secrets Manager . | Custom audience data is uploaded in the Amazon Simpler Storage Service (Amazon S3) bucket’s designated prefix (&lt;S3 Bucket&gt;/tiktok/&lt;audiencename&gt;/&lt;format&gt;/custom_auidences.csv ) in any of the Tiktok SHA256 supported formats shown below. The Amazon S3 bucket is encrypted using AWS Key Management Service (AWS KMS): . | EMAIL_SHA256 . | PHONE_SHA256 . | IDFA_SHA256 . | GAID_SHA256 . | FIRST_SHA256 . | . | Amazon EvenBridge routes the Amazon S3 object event to Amazon Simple Queue Service (Amazon SQS), enabling support for API retry, replay, and throttling. | Amazon Simple Queue Service (Amazon SQS) queue event triggers TikTok Audience Uploader and AWS Lambda function. | The Audience Activation AWS Lambda function retrieves the access token and advertiser_id from AWS Secrets Manager and uploads the target custom audience to TikTok Ads. If uploaded audience is already present, activated Lambda function appends the audiences to current audience. | TikTok Ads advertisers, agencies, or companies leverage this custom audience data as first party audience targeting. | . ",
    "url": "/uploading-audiences-to-tiktok-ads.html#architecture-overview",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#architecture-overview"
  },"113": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Security",
    "content": "When you build systems on an AWS infrastructure, security responsibilities are shared between you and AWS. This shared model can reduce your operational burden as AWS operates, manages, and controls the components from the host operating system and virtualization layer down to the physical security of the facilities in which the services operate. For more information about security on AWS, refer to AWS Cloud Security. Amazon S3 . Infrastructure components where user data flows through are encrypted using Server-Side Encryption (SSE). Multiple Amazon S3 buckets are created for this solution, and they are encrypted using S3-SSE AES-256 encryption to secure user data. AWS KMS . This AWS service is used to encrypt the data stored in Amazon S3 and Secrets Manager. In addition, AWS KMS is used to encrypt the data in transit through Amazon EventBridge and Amazon SQS. AWS Secrets Manager . Secrets Manager helps encrypts secrets at rest using encryption keys that you own and store in AWS KMS. ",
    "url": "/uploading-audiences-to-tiktok-ads.html#security",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#security"
  },"114": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Implementation steps",
    "content": "Manual prerequisites . | Setup TikTok API for business developers by following documentation here. | You will need a long-term access token (with Audience Management Permission of scope) and advertiser_id by following the TikTok Authentication API documentation here. | . Deploying the project with AWS Cloud Development Kits . The project code uses the Python version of the AWS Cloud Development Kit (AWS CDK). To execute the project code, please ensure that you have fulfilled the AWS CDK Prerequisites for Python. The project code requires that the AWS account is bootstrapped in order to allow the deployment of the AWS CDK stack. AWS CDK deployment . | Navigate to project directory: cd [activation-connector-tiktok-ads](https://github.com/aws-samples/activation-connector-tiktok-ads) . | Install and activate a Python Virtual Environment: python3 -m venv .venv source .venv/bin/activate . | Install dependent libraries: python -m pip install -r requirements.txt . | . TikTok credentials . | Update the TikTok credentials in Secrets Manager. Secrets Manager tiktok_activation_credentials are created as part of AWS CDK deployment. Go to the Secrets Manager Console and select tiktok_activation_credentials: | . Figure 2 - Image TikTok Activation Credentials in Secrets Manager Console . | Select Retrieve secret value: | . Figure 3- Image of Retrieve secret value prompt in UI . | Add ACCESS_TOKEN and ADVERTISER_ID keys and corresponding Secret value retrieved from TikTok Authentication API: | . Figure 4- Image of Secret value fields . AWS CDK context . Update the cdk.context.json with the bucket name for TikTok custom segment data: . { \"tiktok_data_bucket_name\": \"rajeabh-connector-data-tiktok-001\" } . Bootstrap the account to setup CDK deployments in the Region . cdk bootstrap . Upon successful completion of cdk bootstrap, the project is ready to be deployed: . cdk deploy . Data Bucket Structure . Targeted custom audience segment data needs to be normalized and hashed in the SHA256 format and uploaded in an Amazon S3 bucket. The Amazon S3 bucket and Prefix should be in this format S3bucket/tiktok/&lt;audience-segment-name&gt;/&lt;format-type&gt;/custom_audiences.csv . Include these parameters: . | audience-segment-name matches with the name of the audience in TikTok Ads Manager . | format-type matches with any of the following TikTok SHA256 supported format: . | email_sha256 . | phone_sha256 . | idfa_sha256 . | gaid_sha256 . | first_sha256 . | . | . Format-type is NOT case sensitive. For example, you can give prefix name “email_sha256” or “EMAIL_SHA256” for uploading Custom Audiences segment emails encrypted with the SHA256 format. Figure 5- Image of Amazon S3 bucket structure . Protect your user data! Do not store it in client code or share it with users. TikTok Data File Schema . TikTok API for Business supports custom audience uploads in the following SHA256 encrypted formats. Refer to TikTok API for Business for all supported types for Custom File Upload: . | EMAIL_SHA256 . | PHONE_SHA256 . | IDFA_SHA256 . | GAID_SHA256 . | FIRST_SHA256 . | . PHONE_SHA256 Example: Phone based in SHA256 format: 3d562b4ba5680ddba530ca888ec699e921b74fcbf5b89e34868d2c9afcd82fb9 . EMAIL_SHA256 Example: Email based in SHA256 format: fd911bd8cac2e603a80efafca2210b7a917c97410f0c29d9f2bfb99867e5a589 . ",
    "url": "/uploading-audiences-to-tiktok-ads.html#implementation-steps",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#implementation-steps"
  },"115": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Testing",
    "content": ". | Copy test custom audiences file from GitHub: /test/tiktok/test_foodies_phone_sha256_audience.csv . to: S3Bucket/tiktok/ foodies-custom-audience/phone_sha256/ test_foodies_phone_sha256_audience.csv . | . | Verify Custom audience foodies-custom-audience is created in TikTok Ads Manager | . | Verify Custom file is uploaded for audience foodies-custom-audience in TikTok Ads Manager. | . If you upload audience with existing custom audience name, audience data will be appended to the existing custom audience. Figure 6 - TikTok Ads Manager showing audience date appended to existing custom audience . ",
    "url": "/uploading-audiences-to-tiktok-ads.html#testing",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#testing"
  },"116": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Cleanup",
    "content": "When you’re finished experimenting with this solution, clean up your resources by running the command: cdk destroy . This command deletes resources deployed through the solution. The Secrets Manager secret containing the manually added tiktok_activation_credentials and CloudWatch log groups are retained after the stack is deleted. ",
    "url": "/uploading-audiences-to-tiktok-ads.html#cleanup",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#cleanup"
  },"117": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Resources",
    "content": ". | TikTok for Business Account | TikTok API for Business | TikTok Audiences | Upload Audiences | TikTok Custom Audience Customer File | . ",
    "url": "/uploading-audiences-to-tiktok-ads.html#resources",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#resources"
  },"118": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Source Code",
    "content": "You can visit our GitHub repository to download the templates and scripts for this solution, and to share your customizations with others. ",
    "url": "/uploading-audiences-to-tiktok-ads.html#source-code",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#source-code"
  },"119": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Document Revisions",
    "content": "January 2023 - Initial Release . ",
    "url": "/uploading-audiences-to-tiktok-ads.html#document-revisions",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#document-revisions"
  },"120": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Contributors",
    "content": "Author: Abhijit Rajeshirke . ",
    "url": "/uploading-audiences-to-tiktok-ads.html#contributors",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#contributors"
  },"121": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Notices",
    "content": "Customers are responsible for making their own independent assessment of the information in this document. This document: (a) is for informational purposes only, (b) represents AWS current product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from AWS and its affiliates, suppliers or licensors. AWS products or services are provided “as is” without warranties, representations, or conditions of any kind, whether express or implied. AWS responsibilities and liabilities to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. Any customer list output from your Amazon environment that you are seeking to activate through TikTok’s Custom Audiences will still need to adhere to TikTok’s Custom Audience terms, including verifying that data you share with TikTok does not include information about children, sensitive health or financial information, other categories of sensitive information. For full details on TikTok’s Custom Audience terms please review: https://ads.tiktok.com/i18n/official/policy/custom-audience-terms. ",
    "url": "/uploading-audiences-to-tiktok-ads.html#notices",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#notices"
  },"122": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "AWS glossary",
    "content": "For the latest AWS terminology, see the AWS glossary in the AWS General Reference. ",
    "url": "/uploading-audiences-to-tiktok-ads.html#aws-glossary",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html#aws-glossary"
  },"123": {
    "doc": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "title": "Guidance for Uploading Audiences created in AWS Clean Rooms to TikTok Ads",
    "content": ". ",
    "url": "/uploading-audiences-to-tiktok-ads.html",
    
    "relUrl": "/uploading-audiences-to-tiktok-ads.html"
  },"124": {
    "doc": "Guidance for Launching a Simple E-commerce Website with WordPress on AWS",
    "title": "Benefits",
    "content": "By using this Guidance, you will gain the following benefits: . | Quickly launch e-commerce websites: Launch e-commerce websites quickly, and manage them using intuitive plug-ins like WordPress and WooCommerce. | Auto setup and configuration: The setup of the website instance, security, login, and plug-ins are all handled automatically, without any manual technical work. | Predicable monthly pricing: This solution is deployed on Lightsail and provides low, monthly, predictable pricing. | . Architecture diagram Figure 1 shows the diagram for this Guidance. Figure 1: Diagram for Launching a Simple E-Commerce Website with WordPress on AWS . ",
    "url": "/launching-simple-ecommerce-website-with-wordpress-on-aws.html#benefits",
    
    "relUrl": "/launching-simple-ecommerce-website-with-wordpress-on-aws.html#benefits"
  },"125": {
    "doc": "Guidance for Launching a Simple E-commerce Website with WordPress on AWS",
    "title": "Architecture workflow",
    "content": ". | Access the AWS CloudFormation template here and deploy it in your AWS Console here. The template will handle all of the setup and access and provide you with a URL to access your new website. The CloudFormation was tested in US-East-1 AWS Region, and the guidance can be used only in US-East-1 based on Lightsail distribution availability. For more details, see AWS::Lightsail::Distribution in the AWS CloudFormation User Guide. | Before launching the CloudFormation template, create your Admin user name and Admin password and store it in AWS Secrets Manager here. The name of the secret should be WordpressAdminCredentials. The two keys for username and password should be name AdminUsername and AdminPassword, respectively. | The website is created utilizing LightSail’s instance and is initialized with standard configurations such as Linux OS, the latest version of WordPress, and other plug-ins to enable e-commerce and analytics. a. Your website instance on AWS is priced at a standard $5 USD/month, with 1GB of RAM, 1 virtual CPU, 40 GB of storage, and 2 TB of data transfer. These costs do not account for any additional WordPress plug-ins you may install. b. Once the instance is created, you are responsible for managing the installed plug-ins like WordPress, WooCommerce, Elementor, and WP Statistics. | An Amazon CloudFront distribution is automatically created within Lightsail, which accelerates delivery of your website content and assets globally and provides distributed-denial-of-service (DDoS) protection. | Once the CloudFormation Template has created the stack, you can find the URL of your website in the Outputs section of the CloudFormation console. a. In the Output tab in the CloudFormation Stack, open the DistributionDetails URL in a new tab. See Figure 2 for an example screenshot of where to find your URL. b. Click the Default domain link shown in the distribution. Example link: https://d1gb5spb######.cloudfront.net/ . c. Append wp-login.php to the URL of the Default domain link: Example link: https://d1gb5spb######.cloudfront.net/wp-login.php . d. Log in using the username and password from Secrets Manager. | . Figure 2: Screenshot of Outputs Tab Hosting the URL .        e. Your website will have the following plug-ins enabled so you can customize your website: .           I. WordPress plug-in to create your website using various themes .           II. WooCommerce plug-in to create your online e-commerce store .           III. Elementor plug-in to provide you more control over customizing your website layout and formatting .           IV. WP Statistics plug-in that provides insights on website analytics, so you can track visits to your new website . | Your website instance is now created, and you can log in to the website with your username and password, that you provided in Step 2. a. Log in to your website using the WordPress Login URL. For example, if your website is www.example.com, you can log in at www.example.com/wp-login.php. You simply need to append “/wp-login.php” to the end of your URL or domain name. | You can also register a new domain and transfer it to AWS in order to access your website through a domain name. | .               a. To use your own domain name, register your domain name and ensure the domain is active.                     I. Log in to the LightSail console, and access the Domains and DNS section from the main Lightsail page.                     II. Create a new Domain Name System (DNS) Zone, as shown in the screenshot in Figure 3. Figure 3: Screenshot of “Create DNS zone” function. | Enter the domain you have registered, and select Create DNS Zone. | . Figure 4: Screenshot of Domain Registration . | Go to the Hosting provider where you registered the domain (if outside of AWS), and create CNAME records provided after you select “Create DNS Zone”. | . Figure 5: Screenshot of CNAME Records .        a) For example, if you are using an external hosting provider, you can create CNAME records as shown in Figure 6, to ensure the domain is redirected to your existing website. Figure 6: Screenshot Showing How to Create CNAME Records .        b) For more information, refer to the webpage: Creating a DNS zone to manage your domain’s DNS records in Lightsail. | Check and verify that website traffic is routing correctly to your Lightsail instance. | . ",
    "url": "/launching-simple-ecommerce-website-with-wordpress-on-aws.html#architecture-workflow",
    
    "relUrl": "/launching-simple-ecommerce-website-with-wordpress-on-aws.html#architecture-workflow"
  },"126": {
    "doc": "Guidance for Launching a Simple E-commerce Website with WordPress on AWS",
    "title": "Guidance for Launching a Simple E-commerce Website with WordPress on AWS",
    "content": ". Launch a simple website on AWS in minutes with e-commerce and analytics capabilities, without having to delve into complex coding or configuration. Small- and medium-sized businesses (SMBs) can use this Guidance Implementation Guide to quickly deploy a website with WordPress for website content management, WooCommerce for e-commerce capabilities, and WordPress Statistics (WP Statistics) to monitor your site analytics. With this Guidance, you can launch a secure website and manage your design and content. The website is deployed on Amazon Lightsail, which gives you standard, predictable pricing per month. ",
    "url": "/launching-simple-ecommerce-website-with-wordpress-on-aws.html",
    
    "relUrl": "/launching-simple-ecommerce-website-with-wordpress-on-aws.html"
  }
}
